{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af98568-db11-4b00-9546-6cd0c9e4a154",
   "metadata": {},
   "source": [
    "# Step 1.4: Splitting Utterance Content\n",
    "\n",
    "This code will perform a number of tasks. First, it will remove lines in the gold standard CSV (created in Step 1.3) which do not contain any instances of the AAL morphosyntactic feature in question. Next, it will produce a CSV spreadsheet with the content of each utterance split into columns by word. It will then duplicate lines that have more than one instance of the AAL morphosyntactic feature in question, in order for each instance within the line to be analyzed in its own line rather than all instances within an utterance being analyzed in the same line. It will then write the results into a CSV file and export it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27e6b-6798-4684-9d4b-b505d5bbc52f",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code:\n",
    "os, re, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5823998-c3e0-46aa-b41c-ad2ab279b85d",
   "metadata": {},
   "source": [
    "## Define the Dataframe Creating Function\n",
    "\n",
    "This function takes the following arguments:\n",
    "\n",
    "<ol>\n",
    "<li>The filepath to the gold standard CSV file, created in Step 1.3 and manually annotated</li>\n",
    "<li>The word being searched for</li>\n",
    "</ol>\n",
    "\n",
    "It will do the following:\n",
    "\n",
    "<ol>\n",
    "<li>Removes lines which do not contain the morphosyntactic feature in question (i.e., lines whose FeatureCountPerLine == 0)</li>\n",
    "<li>Splits the content of the utterance into separate columns per word (Punctuation marks will remain with words they co-occur with)</li>\n",
    "<li>Duplicates lines which have more than one instance of the morphosyntactic feature in question in order for each instance to be examined in its own separate line rather than all instances being examined in the same line</li>\n",
    "<li>Writes the results to a CSV file</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ee4f3-ed83-4359-8a6c-c0a6853d0357",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "The *justify* function is the work of StackOverflow user [Divakar](https://stackoverflow.com/users/3293881/divakar) and is taken from:\n",
    "<ul>\n",
    "<li>https://stackoverflow.com/questions/44558215/python-justifying-numpy-array/44559180#44559180</li>\n",
    "<li>https://stackoverflow.com/questions/51304610/pandas-shifting-columns-depending-on-if-nan-or-not)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af0b45-f075-4f21-bbd3-25a9ce60cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_content_dataframe(csv_input_path, search_word_string):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes a gold standard csv produced by previous steps in the pipeline.\n",
    "    Performs the following tasks:\n",
    "    (1) Removes lines which do not contain the morphosyntactic feature in\n",
    "    question (i.e., lines whose FeatureCountPerLine == 0).\n",
    "    (2) Splits the Content of the utterance into separate columns per word.\n",
    "    Punctuation marks will remain with words they co-occur with.\n",
    "    (3) Duplicates lines which have more than one instance of the\n",
    "    morphosyntactic feature in question in order for each instance to be\n",
    "    examined in its own separate line rather than all instances being\n",
    "    examined in the same line.\n",
    "    (4) Writes the results to a .csv file\n",
    "\n",
    "    One issue remains with this function. If there are multiple instances\n",
    "    of a search word in an utterance, and there is a difference between\n",
    "    the InstancesCountPerLine and the FeatureCountPerLine, then the code\n",
    "    which replicates lines will replicate as many lines as there are\n",
    "    instances rather than actual feature counts. This shouldn't be a\n",
    "    large issue since occurences like this are rare. However, extra\n",
    "    care should be taken in the manually annotation of the .csv files\n",
    "    that result from this code. If the contents of ContentFeature\n",
    "    are not a feature, then the row can be deleted.\n",
    "    \"\"\"\n",
    "\n",
    "################################################################################\n",
    "################## SECTION 1: PRELIMINARY ACTIONS ##############################\n",
    "################################################################################\n",
    "\n",
    "    import re\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    #the following function will align the dataframes of Content before the\n",
    "    # feature to the right so that it will be correctly aligned in the eventual\n",
    "    # csv file to simulate concordance lines correctly. This function was not my\n",
    "    # own development and proper citations are given below.\n",
    "\n",
    "    def justify(a, invalid_val=0, axis=1, side='left'): \n",
    "        \n",
    "        \"\"\"\n",
    "        Taken from Divakar (https://stackoverflow.com/users/3293881/divakar)\n",
    "        ---> https://stackoverflow.com/questions/44558215/python-justifying-numpy-array/44559180#44559180\n",
    "        ---> https://stackoverflow.com/questions/51304610/pandas-shifting-columns-depending-on-if-nan-or-not\n",
    "\n",
    "        Justifies a 2D array\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray\n",
    "          Input array to be justified\n",
    "        axis : int\n",
    "          Axis along which justification is to be made\n",
    "        side : str\n",
    "          Direction of justification. It could be 'left', 'right', 'up', 'down'\n",
    "          It should be 'left' or 'right' for axis=1 and 'up' or 'down' for axis=0.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if invalid_val is np.nan:\n",
    "            \n",
    "          mask = pd.notnull(a)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "          mask = a!=invalid_val\n",
    "        \n",
    "        justified_mask = np.sort(mask,axis=axis)\n",
    "        \n",
    "        if (side=='up') | (side=='left'):\n",
    "            \n",
    "          justified_mask = np.flip(justified_mask,axis=axis)\n",
    "        \n",
    "        out = np.full(a.shape, invalid_val, dtype=object) \n",
    "        \n",
    "        if axis==1:\n",
    "            \n",
    "          out[justified_mask] = a[mask]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "          out.T[justified_mask.T] = a.T[mask.T]\n",
    "        \n",
    "        return out\n",
    "\n",
    "################################################################################\n",
    "###################### SECTION 2: EXECUTION OF CODE ############################\n",
    "################################################################################\n",
    "\n",
    "    # reads in the gold standard csv and creates a pandas dataframe\n",
    "    gs_df = pd.read_csv(csv_input_path)\n",
    "    \n",
    "    # returns an empty dataframe for an empty dataframe\n",
    "    if len(gs_df) == 0:\n",
    "        \n",
    "        #creates an empty dataframe since there are no instances\n",
    "        # of the morphosyntactic feature\n",
    "        gs_empty_df = pd.DataFrame(columns=[\n",
    "            'File', 'Line', 'Speaker', 'UttStartTime',\n",
    "            'UttEndTime', 'UttLength', 'InstancesCountPerLine', \n",
    "            'FeatureCountPerLine', 'Content', 'SubjectWordToken',\n",
    "            'PredicateWordToken', 'WordPattern', 'SubjectPOS',\n",
    "            'PredicatePOS', 'POSPattern'])\n",
    "        \n",
    "        # returns the final dataframe\n",
    "        return gs_empty_df\n",
    "        \n",
    "    \n",
    "    # checks if there are no instances of the morphosyntactic feature,\n",
    "    #  if so, returns an empty dataframe\n",
    "    elif max(gs_df['FeatureCountPerLine']) == 0:\n",
    "        \n",
    "        #creates an empty dataframe since there are no instances\n",
    "        # of the morphosyntactic feature\n",
    "        gs_empty_df = pd.DataFrame(columns=[\n",
    "            'File', 'Line', 'Speaker', 'UttStartTime',\n",
    "            'UttEndTime', 'UttLength', 'InstancesCountPerLine', \n",
    "            'FeatureCountPerLine', 'Content', 'SubjectWordToken',\n",
    "            'PredicateWordToken', 'WordPattern', 'SubjectPOS',\n",
    "            'PredicatePOS', 'POSPattern'])\n",
    "        \n",
    "        # returns the final dataframe\n",
    "        return gs_empty_df\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # adds empty columns for subject word token, subject part of speech, predicate\n",
    "        # word token, predicate part of speech, subject/predicate word pattern\n",
    "        # surrounding the feature, and POS pattern surrounding the feature to be \n",
    "        # filled in manually\n",
    "        gs_df[\"SubjectWordToken\"] = np.nan\n",
    "        gs_df[\"PredicateWordToken\"] = np.nan\n",
    "        gs_df[\"WordPattern\"] = np.nan\n",
    "        gs_df[\"SubjectPOS\"] = np.nan\n",
    "        gs_df[\"PredicatePOS\"] = np.nan\n",
    "        gs_df[\"POSPattern\"] = np.nan\n",
    "\n",
    "        # takes the search word input and transforms it into a regular expression \n",
    "        # that will search for only whole words if the sequence of strings submitted \n",
    "        # is contained within a larger word, this will filter those instances out\n",
    "        # and leave only whole matches\n",
    "        search_word_regex = f\"\\\\b[{search_word_string[0].upper()}|{search_word_string[0].lower()}]{search_word_string[1:]}\\\\b\"\n",
    "\n",
    "        # the following will create columns to be changed later\n",
    "        # IterationNumber will be used to help duplicate lines that contain more\n",
    "        #  than one instance of the feature in question\n",
    "        gs_df['IterationNumber'] = 1\n",
    "\n",
    "        # creates an empty column to store the Content that occurs before the feature\n",
    "        gs_df['ContentBeforeFeature'] = np.nan\n",
    "\n",
    "        # creates an empty column to store the feature text\n",
    "        gs_df['ContentFeature'] = np.nan\n",
    "\n",
    "        # creates an empty column to store the Content that occurs after the feature\n",
    "        gs_df['ContentAfterFeature'] = np.nan\n",
    "\n",
    "        # creates a list of columns from the gold standard dataframe\n",
    "        gs_df_column_names = list(gs_df)\n",
    "\n",
    "        # creates an empty list for replicated lines to be appended to. A line will \n",
    "        # be replicated if it contains more than one instance of a feature. The\n",
    "        # result will be as many total occurences of the line as there are number\n",
    "        # of features in the line. This will allow each instance of the feature in \n",
    "        # the line to be examined separately, rather than needing to examine each\n",
    "        # instance within the same line.\n",
    "        replicated_lines = []\n",
    "\n",
    "        #iterates through the rows of the gold standard dataframe\n",
    "        for row in gs_df.itertuples():\n",
    "\n",
    "            #if there is only one instance of the feature in the line, this will \n",
    "            # separate the Content into columns based on what comes before and after\n",
    "            # the feature in the text and a separate column for the feature. Doing\n",
    "            # this will replicate concordance lines which can center the feature\n",
    "            # in question.\n",
    "            #re.findall is used here to find all the instances of the search word \n",
    "            # (i.e., feature). There are other ways to search strings with re but this\n",
    "            # one will find them all.\n",
    "            if row.FeatureCountPerLine == 1:\n",
    "\n",
    "              #here, re.finditer is used because finditer will iterate through the\n",
    "              # matches of the search word and return a match object. match objects\n",
    "              # provide more information than just the match. for example, .start()\n",
    "              # returns the start position of the match within the text.\n",
    "              # Check here for more info: \n",
    "              #  https://docs.python.org/3/library/re.html#match-objects\n",
    "              #Loops through the matches of the search word in the row's Content\n",
    "                for match in re.finditer(search_word_regex, str(row.Content)):\n",
    "\n",
    "                    # replaces the \"ContentBeforeFeature\" cell with all text that occurs\n",
    "                    #  before the search word (i.e., feature in question)\n",
    "                    gs_df.loc[row.Index, \"ContentBeforeFeature\"] = str(row.Content)[:match.start()]\n",
    "\n",
    "                    # replaces the \"ContentFeature\" with the text of the search word\n",
    "                    #  (i.e., feature in question)\n",
    "                    gs_df.loc[row.Index, \"ContentFeature\"] = str(row.Content)[match.start():match.end()+1]\n",
    "\n",
    "                    # replaces the \"ContentAfterFeature\" cell with all text that occurs\n",
    "                    #  after the search word (i.e., feature in question)\n",
    "                    gs_df.loc[row.Index, \"ContentAfterFeature\"] = str(row.Content)[match.end()+1:]\n",
    "\n",
    "            elif row.FeatureCountPerLine > 1:\n",
    "\n",
    "                # sets the iteration number to 0. This number will be changed within\n",
    "                # the loops below. It will be used to determine which instance of the\n",
    "                # feature should be centered on that line. It will also be used later\n",
    "                # to sort lines along with File and Line.\n",
    "                iteration_number = 0\n",
    "\n",
    "                #here, re.finditer is used because finditer will iterate through the\n",
    "                # matches of the search word and return a match object. match objects\n",
    "                # provide more information than just the match. for example, .start()\n",
    "                # returns the start position of the match within the text.\n",
    "                # Check here for more info: \n",
    "                #  https://docs.python.org/3/library/re.html#match-objects\n",
    "                #Loops through the matches of the search word in the row's Content\n",
    "                for match in re.finditer(search_word_regex, str(row.Content)):\n",
    "\n",
    "                    # creates a variable with all text that occurs\n",
    "                    #  before the search word (i.e., feature in question)\n",
    "                    current_before = str(row.Content)[:match.start()]\n",
    "\n",
    "                    # creates a variable with the text of the search word\n",
    "                    #  (i.e., feature in question)\n",
    "                    current_feature = str(row.Content)[match.start():match.end()+1]\n",
    "\n",
    "                    # creates a variable with all text that occurs\n",
    "                    #  after the search word (i.e., feature in question)\n",
    "                    current_after = str(row.Content)[match.end()+1:]\n",
    "                    \n",
    "                    try:\n",
    "                   \n",
    "                        # creates a list of the values in cells of the row. Importantly,\n",
    "                        # if the variable begins with \"row.\" that means, it's simply\n",
    "                        # taking what's already there. But the last four variables are taken\n",
    "                        # from the variables created in the immediately previous steps.\n",
    "                        # iteration number is added to by 1 to reflect which iteration of \n",
    "                        # the feature matches it is.\n",
    "                        cell_values = [row.File, row.Line, row.Speaker, row.UttStartTime,\n",
    "                                       row.UttEndTime, row.UttLength, row.Content,\n",
    "                                       row.InstancesCountPerLine, row.FeatureCountPerLine,\n",
    "                                       row.SubjectWordToken, row.PredicateWordToken, row.WordPattern,\n",
    "                                       row.SubjectPOS, row.PredicatePOS, row.POSPattern,\n",
    "                                       iteration_number+1, current_before, current_feature,\n",
    "                                       current_after]\n",
    "\n",
    "                        # zips together the column names with the cell values from the\n",
    "                        # current row. Zipping creates tuples\n",
    "                        zipped = zip(gs_df_column_names, cell_values)\n",
    "\n",
    "                        # creates a python dictionary based on the zipped variable just\n",
    "                        #  created. This is necessary to be able to append the row\n",
    "                        #  back into the larger dataframe later.\n",
    "                        line_dict = dict(zipped)\n",
    "\n",
    "                        # appends the dictionary to the replicated lines empty list above\n",
    "                        #  to be used for appending to the dataframe later\n",
    "                        replicated_lines.append(line_dict)\n",
    "\n",
    "                        # increases the iteration number by one so the next iteration number\n",
    "                        #  will be correct\n",
    "                        iteration_number += 1\n",
    "                        \n",
    "                    except:\n",
    "                   \n",
    "                        # creates a list of the values in cells of the row. Importantly,\n",
    "                        # if the variable begins with \"row.\" that means, it's simply\n",
    "                        # taking what's already there. But the last four variables are taken\n",
    "                        # from the variables created in the immediately previous steps.\n",
    "                        # iteration number is added to by 1 to reflect which iteration of \n",
    "                        # the feature matches it is.\n",
    "                        cell_values = [row.File, row.Line, row.Content, \n",
    "                                       row.InstancesCountPerLine, row.FeatureCountPerLine,\n",
    "                                       row.SubjectWordToken, row.PredicateWordToken, row.WordPattern,\n",
    "                                       row.SubjectPOS, row.PredicatePOS, row.POSPattern,\n",
    "                                       iteration_number+1, current_before, current_feature,\n",
    "                                       current_after]\n",
    "\n",
    "                        # zips together the column names with the cell values from the\n",
    "                        # current row. Zipping creates tuples\n",
    "                        zipped = zip(gs_df_column_names, cell_values)\n",
    "\n",
    "                        # creates a python dictionary based on the zipped variable just\n",
    "                        #  created. This is necessary to be able to append the row\n",
    "                        #  back into the larger dataframe later.\n",
    "                        line_dict = dict(zipped)\n",
    "\n",
    "                        # appends the dictionary to the replicated lines empty list above\n",
    "                        #  to be used for appending to the dataframe later\n",
    "                        replicated_lines.append(line_dict)\n",
    "\n",
    "                        # increases the iteration number by one so the next iteration number\n",
    "                        #  will be correct\n",
    "                        iteration_number += 1\n",
    "\n",
    "        # appends replicated lines to the gold standard dataframe              \n",
    "        gs_replicated_df = gs_df.append(replicated_lines)\n",
    "\n",
    "        #############################################################\n",
    "        # This next section will take the Content before and after\n",
    "        # the feature and split it into columns, one word per column.\n",
    "        #############################################################\n",
    "\n",
    "        ##########################\n",
    "        # Content before\n",
    "        ##########################\n",
    "\n",
    "        # creates a separate dataframe with columns of words within the Content \n",
    "        #  before the feature, one word per column\n",
    "        content_before_df = gs_replicated_df[\"ContentBeforeFeature\"].str.split(expand=True)\n",
    "\n",
    "        # aligns the words in the before_df dataframe to the right. This will \n",
    "        #  correctly align row Content in the eventual csv to appear as traditional\n",
    "        #  concordance lines. This utilizes the justify function defined above.\n",
    "        #  Please see citations there for authorship and for more information.\n",
    "        content_before_alignedR_df = pd.DataFrame(\n",
    "            justify(content_before_df.values, invalid_val=np.nan, side=\"right\"),\n",
    "            index=content_before_df.index, columns=content_before_df.columns) \n",
    "\n",
    "        # creates a list of the column names in the aligned dataframe just created.\n",
    "        #  this list will just be numbers. These column names are automatically\n",
    "        #  produced by str.split.(expand=True) two steps ago.\n",
    "        content_before_column_names = list(content_before_alignedR_df)\n",
    "\n",
    "        # creates a reversed list of column names in order to correctly count down \n",
    "        #  from left to right (i.e., \"L3, L2, L1, Feature\")\n",
    "        content_before_column_names_rev = content_before_column_names[::-1]\n",
    "\n",
    "        # creates a list of renamed column names to reflect tradition corpus\n",
    "        #  linguistics annotation (i.e., \"L3, L2, L1\")\n",
    "        content_before_column_names_renamed = [f\"L{column_name+1}\" for column_name in content_before_column_names_rev]\n",
    "\n",
    "        # resets the column names of the aligned dataframe to the new column names\n",
    "        content_before_alignedR_df.columns = content_before_column_names_renamed\n",
    "\n",
    "        ##########################\n",
    "        # Content feature\n",
    "        ##########################\n",
    "\n",
    "        # creates a dataframe with only the feature text\n",
    "        content_feature_df = gs_replicated_df[['ContentFeature']]\n",
    "\n",
    "        ##########################\n",
    "        # Content after\n",
    "        ##########################\n",
    "\n",
    "        # creates a separate dataframe with columns of words within the Content \n",
    "        #  after the feature, one word per column\n",
    "        content_after_df = gs_replicated_df[\"ContentAfterFeature\"].str.split(expand = True)\n",
    "\n",
    "        #NOTE: There is no need for aligning the columns here as the after content\n",
    "        #         columns automatically align left as they should.\n",
    "\n",
    "        # creates a list of the column names in the dataframe just created.\n",
    "        #  this list will just be numbers. These column names are automatically\n",
    "        #  produced by str.split.(expand=True) in the previous step.\n",
    "        content_after_column_names = list(content_after_df)\n",
    "\n",
    "        #NOTE: There is no need for reversing the columns here as the after content\n",
    "        #         columns automatically increase left to right as they should\n",
    "\n",
    "        # creates a list of renamed column names to reflect tradition corpus\n",
    "        #  linguistics annotation (i.e., \"R1, R2, R3\")\n",
    "        content_after_column_names_renamed = [f\"R{column_name+1}\" for column_name in content_after_column_names]\n",
    "\n",
    "        # resets the column names of the aligned dataframe to the new column names\n",
    "        content_after_df.columns = content_after_column_names_renamed\n",
    "\n",
    "        #############################################################\n",
    "        # This next section stitches everything together\n",
    "        #############################################################\n",
    "\n",
    "        # drops the columns ContentBeforeFeature, ContentFeature, ContentAfterFeature\n",
    "        #  because they are no longer needed\n",
    "        gs_dropped_df = gs_replicated_df.drop(columns = ['ContentBeforeFeature',\n",
    "                                                          'ContentFeature',\n",
    "                                                          'ContentAfterFeature'])\n",
    "\n",
    "        # concatenates the dataframes created in previous steps\n",
    "        gs_split_content_df = pd.concat([gs_dropped_df, content_before_alignedR_df,\n",
    "                                         content_feature_df, content_after_df], axis=1)\n",
    "\n",
    "        # removes original rows which have more than one instance of the feature, \n",
    "        #  but now are copies with None type entries in their cells\n",
    "        gs_split_content_droppedNa_df = gs_split_content_df[gs_split_content_df['ContentFeature'].notna()]\n",
    "\n",
    "        # gets the indexNames of all the lines which do not\n",
    "        # contain the AAL morphosyntactic feature in question\n",
    "        index_names = gs_split_content_droppedNa_df[gs_split_content_droppedNa_df['FeatureCountPerLine'] == 0 ].index\n",
    "\n",
    "        # drops all lines which do not contain the AAL morphosyntactic\n",
    "        # feature in question\n",
    "        gs_split_content_droppedNa_df.drop(index_names, inplace=True)\n",
    "\n",
    "        #if all rows are dropped becase no instances of the morphosyntactic feature\n",
    "        # occur in any lines, then this will return an empty dataframe. for example,\n",
    "        # TIMIT had no occurences of habitual 'be'. if there are no rows and the code\n",
    "        # progresses beyond this point, an error will be raised and the code will \n",
    "        # stopped. \n",
    "        if len(gs_split_content_droppedNa_df) == 0:\n",
    "\n",
    "            # removes the IterationNumber column because it is no longer needed\n",
    "            #  and resets the index\n",
    "            gs_final_split_df = gs_split_content_droppedNa_df.drop(\n",
    "                columns = ['IterationNumber']).reset_index(drop=True) \n",
    "\n",
    "            #returns the final dataframe\n",
    "            return gs_final_split_df\n",
    "\n",
    "        else:\n",
    "            # resets the index for the dataframe to count lines from the beginning and\n",
    "            #  sorts the dataframe according to File, Line, then Iteration Number for\n",
    "            #  those lines which have more than one instance\n",
    "            gs_split_content_droppedNa_df = gs_split_content_droppedNa_df.sort_values(\n",
    "                ['File', 'Line', 'IterationNumber']).reset_index(drop=True)\n",
    "\n",
    "            # removes the IterationNumber column because it is no longer needed\n",
    "            #  and resets the index\n",
    "            gs_final_split_df = gs_split_content_droppedNa_df.drop(\n",
    "                columns = ['IterationNumber']).reset_index(drop=True) \n",
    "\n",
    "            #returns dataframe\n",
    "            return gs_final_split_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5124e-a4c0-4650-98d7-eb26f54c3874",
   "metadata": {},
   "source": [
    "# Creating Split Utterance Content Dataframes\n",
    "\n",
    "This will execute the code and create the dataframes and then export them as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620dc96c-1008-4f4a-a94f-74525f1e0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the input path where the gold standard CSVs are stored\n",
    "csv_input_path = \"path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0ef09-55d8-43ce-aea3-ce3dd2f1bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2cb8d-17eb-4ab8-955a-832fc0add2d5",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7180120-d551-4dd4-b9a8-28d4f7452ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"ain\\'t\"\n",
    "\n",
    "# Get filenames for gold standard CSVs\n",
    "#  Ensures:\n",
    "#  (1) Only CSV files will be collected (no folders, no .DS_Store)\n",
    "#  (2) Only filepaths from CSV files connected to the search word will be collected\n",
    "#  (3) The gold standard info CSV will be skipped\n",
    "#\n",
    "#  Note:\n",
    "#    Make sure the file.startswith(___) portion of the code matches the beginning\n",
    "#    of the filenames. Usually, the search_word_string variable is used because\n",
    "#    it is both the search term and should be the beginning of the filename. \n",
    "#    However, for ain't, the search term and beginning of file will be different\n",
    "#    because of the apostrophe.\n",
    "\n",
    "csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                 if file.endswith(\".csv\") and\n",
    "                 file.startswith(\"aint\") and \"info\" not in file]\n",
    "\n",
    "#loop through the filepath and corpus name tuples and creates split content dataframe\n",
    "for filename in csv_filenames:\n",
    "    \n",
    "    #gets just the first two items in the filename sequence (e.g., \"be_coraal\")\n",
    "    stripped_filename = \"_\".join(filename.split(\"_\")[:2])\n",
    "    \n",
    "    #creates the name for the dataframe by adding \"_splitContent_df\" to the stripped filename\n",
    "    dataframe_name = f\"{stripped_filename}_splitContent_df\"\n",
    "    \n",
    "    globals()[dataframe_name] = create_split_content_dataframe(f\"{csv_input_path}/{filename}\", search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da4126-233a-4e09-8443-7d20d8c552a7",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8debe-56d4-49f4-ae5f-4b64d81d6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"be\"\n",
    "\n",
    "# Get filenames for gold standard CSVs\n",
    "#  Ensures:\n",
    "#  (1) Only CSV files will be collected (no folders, no .DS_Store)\n",
    "#  (2) Only filepaths from CSV files connected to the search word will be collected\n",
    "#  (3) The gold standard info CSV will be skipped\n",
    "#\n",
    "#  Note:\n",
    "#    Make sure the file.startswith(___) portion of the code matches the beginning\n",
    "#    of the filenames. Usually, the search_word_string variable is used because\n",
    "#    it is both the search term and should be the beginning of the filename. \n",
    "#    However, for ain't, the search term and beginning of file will be different\n",
    "#    because of the apostrophe.\n",
    "\n",
    "csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                 if file.endswith(\".csv\") and\n",
    "                 file.startswith(search_word_string) and \"info\" not in file]\n",
    "\n",
    "#loop through the filepath and corpus name tuples and creates split content dataframe\n",
    "for filename in csv_filenames:\n",
    "    \n",
    "    #gets just the first two items in the filename sequence (e.g., \"be_coraal\")\n",
    "    stripped_filename = \"_\".join(filename.split(\"_\")[:2])\n",
    "    \n",
    "    #creates the name for the dataframe by adding \"_splitContent_df\" to the stripped filename\n",
    "    dataframe_name = f\"{stripped_filename}_splitContent_df\"\n",
    "    \n",
    "    globals()[dataframe_name] = create_split_content_dataframe(f\"{csv_input_path}/{filename}\", search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172aceb-3912-4721-88a8-7de24175a3f2",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ef5cb-09c9-4abd-b923-746cbc3278c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"done\"\n",
    "\n",
    "# Get filenames for gold standard CSVs\n",
    "#  Ensures:\n",
    "#  (1) Only CSV files will be collected (no folders, no .DS_Store)\n",
    "#  (2) Only filepaths from CSV files connected to the search word will be collected\n",
    "#  (3) The gold standard info CSV will be skipped\n",
    "#\n",
    "#  Note:\n",
    "#    Make sure the file.startswith(___) portion of the code matches the beginning\n",
    "#    of the filenames. Usually, the search_word_string variable is used because\n",
    "#    it is both the search term and should be the beginning of the filename. \n",
    "#    However, for ain't, the search term and beginning of file will be different\n",
    "#    because of the apostrophe.\n",
    "\n",
    "csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                 if file.endswith(\".csv\") and\n",
    "                 file.startswith(search_word_string) and \"info\" not in file]\n",
    "\n",
    "#loop through the filepath and corpus name tuples and creates split content dataframe\n",
    "for filename in csv_filenames:\n",
    "    \n",
    "    #gets just the first two items in the filename sequence (e.g., \"be_coraal\")\n",
    "    stripped_filename = \"_\".join(filename.split(\"_\")[:2])\n",
    "    \n",
    "    #creates the name for the dataframe by adding \"_splitContent_df\" to the stripped filename\n",
    "    dataframe_name = f\"{stripped_filename}_splitContent_df\"\n",
    "    \n",
    "    globals()[dataframe_name] = create_split_content_dataframe(f\"{csv_input_path}/{filename}\", search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c303e-b650-4e9d-bc79-e279751611fe",
   "metadata": {},
   "source": [
    "## Sorting the Dataframes by File and Line\n",
    "\n",
    "This will sort the dataframes first by filename and then by line number. Doing this each step will ensure consistency across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57205-3bfd-4933-bdbe-5f8e6e75ea9d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f32913-4e1f-44ea-bd82-f0fdbfd167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_coraal_splitContent_df = aint_coraal_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "aint_fisher_splitContent_df = aint_fisher_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "aint_librispeech_splitContent_df = aint_librispeech_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "aint_switchboardHub5_splitContent_df = aint_switchboardHub5_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "aint_timit_splitContent_df = aint_timit_splitContent_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a08157-c6a8-4cbb-8978-e6af4b931a7e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12356d-7ffb-4af6-9c58-21e00fae9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_coraal_splitContent_df = be_coraal_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "be_fisher_splitContent_df = be_fisher_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "be_librispeech_splitContent_df = be_librispeech_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "be_switchboardHub5_splitContent_df = be_switchboardHub5_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "be_timit_splitContent_df = be_timit_splitContent_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cd837-4bb3-4129-b4db-583aeb11ea90",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc53ff-73c4-484b-ad31-2025fba38270",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_coraal_splitContent_df = done_coraal_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "done_fisher_splitContent_df = done_fisher_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "done_librispeech_splitContent_df = done_librispeech_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "done_switchboardHub5_splitContent_df = done_switchboardHub5_splitContent_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "done_timit_splitContent_df = done_timit_splitContent_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983788-c9a5-4b73-8a2e-1d18a6ae24b9",
   "metadata": {},
   "source": [
    "## Exporting Dataframes to CSV Files\n",
    "\n",
    "This will export the dataframes to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff3dee-e5ab-47f3-a75d-14de88610b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the output path where the gold standard CSVs are stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331a54-e11e-4b58-813a-717796e37e73",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e3561-7148-4ace-aa8f-2e2ca60a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_coraal_splitContent_df.to_csv(f\"{csv_output_path}aint_coraal_splitContent.csv\", index=False)\n",
    "\n",
    "aint_fisher_splitContent_df.to_csv(f\"{csv_output_path}aint_fisher_splitContent.csv\", index=False)\n",
    "\n",
    "aint_librispeech_splitContent_df.to_csv(f\"{csv_output_path}aint_librispeech_splitContent.csv\", index=False)\n",
    "\n",
    "aint_switchboardHub5_splitContent_df.to_csv(f\"{csv_output_path}aint_switchboardHub5_splitContent.csv\", index=False)\n",
    "\n",
    "aint_timit_splitContent_df.to_csv(f\"{csv_output_path}aint_timit_splitContent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf982d6-41cb-46d8-9917-8d4a6a5bdd4d",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6042b-fa7e-4017-aac3-7cfca0b8fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_coraal_splitContent_df.to_csv(f\"{csv_output_path}be_coraal_splitContent.csv\", index=False)\n",
    "\n",
    "be_fisher_splitContent_df.to_csv(f\"{csv_output_path}be_fisher_splitContent.csv\", index=False)\n",
    "\n",
    "be_librispeech_splitContent_df.to_csv(f\"{csv_output_path}be_librispeech_splitContent.csv\", index=False)\n",
    "\n",
    "be_switchboardHub5_splitContent_df.to_csv(f\"{csv_output_path}be_switchboardHub5_splitContent.csv\", index=False)\n",
    "\n",
    "be_timit_splitContent_df.to_csv(f\"{csv_output_path}be_timit_splitContent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101802dd-7fd7-4676-a52b-8b9d76e1c990",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cab56-e32a-467e-aa8e-12a6640b5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_coraal_splitContent_df.to_csv(f\"{csv_output_path}done_coraal_splitContent.csv\", index=False)\n",
    "\n",
    "done_fisher_splitContent_df.to_csv(f\"{csv_output_path}done_fisher_splitContent.csv\", index=False)\n",
    "\n",
    "done_librispeech_splitContent_df.to_csv(f\"{csv_output_path}done_librispeech_splitContent.csv\", index=False)\n",
    "\n",
    "done_switchboardHub5_splitContent_df.to_csv(f\"{csv_output_path}done_switchboardHub5_splitContent.csv\", index=False)\n",
    "\n",
    "done_timit_splitContent_df.to_csv(f\"{csv_output_path}done_timit_splitContent.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
