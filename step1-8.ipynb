{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf78ff63-ae1b-4a6c-a959-6a71d2d83a76",
   "metadata": {},
   "source": [
    "# Step 1.8: Getting n-gram Dataframes\n",
    "\n",
    "This code will produce CSV files which have n-grams (of up to 3 words) to the left and right of the feature.It will create four sets of columns:\n",
    "\n",
    "<ol>\n",
    "<li>A column of pre-cleaned lists of the trigrams</li>\n",
    "<li>A column of pre-cleaned tuples of the trigrams</li>\n",
    "<li>A column of cleaned lists of the trigrams</li>\n",
    "<li>A column of cleaned tuples of the trigrams</li>\n",
    "</ol>\n",
    "\n",
    "Tuples will be included because they are easier to examine as a unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e01ee8-e091-4f3a-9c45-9c7766929bce",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code:\n",
    "string, os, re, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30c5cd-2dd2-4813-b263-e98e6bf04fb6",
   "metadata": {},
   "source": [
    "## Define the Dataframe Creating Function\n",
    "\n",
    "This function takes the following arguments:\n",
    "\n",
    "<ol>\n",
    "<li>The filepath to the split content CSV produced in Step 1.5</li>\n",
    "<li>The filepath to the folder where the newly created CSVs will be stored</li>\n",
    "<li>The word being searched for</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb12a59-b087-49bb-9d30-70beb7973246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trigram_df(csv_input_path, csv_output_path, search_word_string):\n",
    "\n",
    "    \"\"\"Creates a csv file with trigrams on either side of the feature.\"\"\"\n",
    "\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from string import punctuation\n",
    "    \n",
    "    \n",
    "    \n",
    "    if search_word_string == \"ain\\'t\":\n",
    "        \n",
    "        #creates a list of the split content .csv's which should be stored together in \n",
    "        # the same folder (csv_input_path)\n",
    "        csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                         if file.endswith(\".csv\") and\n",
    "                         file.startswith(\"aint\") and \"info\" not in file]\n",
    "        \n",
    "        # filename for the all_corpora_info csv from Step1-3\n",
    "        #  creates a list of the one filename and then uses [0] to get the filename\n",
    "        #  out of the list format\n",
    "        all_corpora_info_csv_path = [f\"{csv_input_path}{filename}\" \n",
    "                                     for filename in os.listdir(csv_input_path) \n",
    "                                     if \"all_corpora_info\" in filename \n",
    "                                     and filename.startswith(\"aint\")][0]\n",
    "    \n",
    "    else:\n",
    "                                     \n",
    "        #creates a list of the split content .csv's which should be stored together in \n",
    "        # the same folder (csv_input_path)\n",
    "        csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                         if file.endswith(\".csv\") and\n",
    "                         file.startswith(search_word_string) and \"info\" not in file]\n",
    "\n",
    "        # filename for the all_corpora_info csv from Step1-3\n",
    "        #  creates a list of the one filename and then uses [0] to get the filename\n",
    "        #  out of the list format\n",
    "        all_corpora_info_csv_path = [f\"{csv_input_path}{filename}\" \n",
    "                                     for filename in os.listdir(csv_input_path) \n",
    "                                     if \"all_corpora_info\" in filename \n",
    "                                     and filename.startswith(search_word_string)][0]\n",
    "\n",
    "    #creates a list of tuples with the csv input full paths and the corpus name\n",
    "    filePath_corpusName = [(f\"{csv_input_path}{filename}\",\n",
    "                            re.search(r\"_(.*?)_\", filename).group(1).lower())\n",
    "                           for filename in csv_filenames if \"info\" not in filename]\n",
    "    \n",
    "     ####################\n",
    "    \n",
    "    \n",
    "    # creates a dataframe from the all_corpora_info_df csv\n",
    "    all_corpora_info_df = pd.read_csv(f\"{all_corpora_info_csv_path}\", index_col=0)\n",
    "    \n",
    "    # lowercases the column names of this dataframe, which are corpus names\n",
    "    #  this is to ensure the next immediate lines of code will fuction correctly\n",
    "    all_corpora_info_df.columns = map(str.lower, all_corpora_info_df.columns)\n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    # loops through the filepath and corpus name tuples list\n",
    "    for file_path, corpus_name in filePath_corpusName:\n",
    "    \n",
    "        # creates a variable of the corpus' total word count\n",
    "        corpus_word_count_total = all_corpora_info_df.loc['TotalCorpusWordCount', corpus_name]\n",
    "    \n",
    "    \n",
    "        # creates a string of punctuation markers to be used for cleaning later\n",
    "        #  that does not include a single apostrophe or dash\n",
    "        #  the dash is because when words are cut off in the transcript\n",
    "        #  a dash will be used to represent that and that can be important\n",
    "        #  in the eventual data analysis\n",
    "        punctuation_no_apostrophe_no_dash = punctuation.replace(\"'\", \"\").replace(\"-\",\"\")\n",
    "        \n",
    "        # creates a string of punctuation markers to be used for cleaning later\n",
    "        #  that does not include a single apostrophe or dash\n",
    "        punctuation_no_apostrophe = punctuation.replace(\"'\", \"\")\n",
    "\n",
    "        # reads in the split content csv\n",
    "        patterns_df = pd.read_csv(file_path)\n",
    "\n",
    "        # filters the data frame to only include needed columns\n",
    "        trigram_df = patterns_df[['File', 'Line', 'InstancesCountPerLine', 'FeatureCountPerLine', 'Content']]\n",
    "\n",
    "        # if the split content csv is empty because it had no\n",
    "        # instances of the feature, this will return an empty\n",
    "        # csv\n",
    "        if len(trigram_df) == 0:\n",
    "            \n",
    "            #creates an empty dataframe since there are no instances\n",
    "            # of the morphosyntactic feature\n",
    "            trigram_empty_df = pd.DataFrame(columns=[\n",
    "                'File', 'Line', 'InstancesCountPerLine', \n",
    "                'FeatureCountPerLine', 'Content'])\n",
    "\n",
    "             #exports dataframe to csv\n",
    "            trigram_empty_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_trigrams.csv\", index=False)\n",
    "\n",
    "        # otherwise, continue on with the code\n",
    "        else:\n",
    "\n",
    "        #################\n",
    "\n",
    "            # adds a column with a list of tokens from L3 to L1 (3 words to the\n",
    "            # left to 1 word to the left of the feature) for each row\n",
    "            trigram_df['L3_L1_List'] = patterns_df[['L3', 'L2', 'L1']].values.tolist()\n",
    "\n",
    "            # adds a column with tuples of the L3_L1 lists\n",
    "            trigram_df['L3_L1_Tuple'] = [tuple(sublist) for sublist \n",
    "                                        in list(trigram_df['L3_L1_List'])] \n",
    "\n",
    "            # creates a list of the lists in the column\n",
    "            L3_L1_trigrams_list = list(trigram_df['L3_L1_List'])\n",
    "\n",
    "            # creates an empty list to be appended to\n",
    "            cleaned_L3_L1_trigrams_list = []\n",
    "\n",
    "            # loops through the lists in the L3_L1 trigrams list\n",
    "            for sublist in L3_L1_trigrams_list:\n",
    "                #creates an empty list to be appended to\n",
    "                # this list will be emptied on every loop\n",
    "                cleaned_sublist = []\n",
    "\n",
    "                # loops through the word in each sub-list\n",
    "                for word in sublist:\n",
    "\n",
    "                    # if the token is a string, appends the word to the cleaned_sublist\n",
    "                    if type(word) != float:\n",
    "                        cleaned_sublist.append(word.lower().strip().translate(\n",
    "                            str.maketrans(\"\", \"\", punctuation_no_apostrophe_no_dash)))\n",
    "\n",
    "                    # skips the token if it is NaN\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                # appends the cleaned list to the larger list        \n",
    "                cleaned_L3_L1_trigrams_list.append(cleaned_sublist)\n",
    "\n",
    "            # creates a column with the cleaned lists    \n",
    "            trigram_df['CleanedL3_L1_List'] = cleaned_L3_L1_trigrams_list\n",
    "\n",
    "            # creates a column with cleaned tuples  \n",
    "            trigram_df['CleanedL3_L1_Tuple'] = [tuple(sublist) for sublist in cleaned_L3_L1_trigrams_list] \n",
    "\n",
    "            # creates an empty column for tuple counts to be inserted\n",
    "            trigram_df['Count_CleanedL3_L1_Tuple'] = np.nan\n",
    "\n",
    "            # count number of occurences of the trigram tuples in the corpus\n",
    "            for row in trigram_df.itertuples():\n",
    "\n",
    "                # inserts the count of the trigram tuple into the empty column\n",
    "                trigram_df.loc[row.Index, 'Count_CleanedL3_L1_Tuple'] = list(trigram_df['CleanedL3_L1_Tuple']).count(row.CleanedL3_L1_Tuple)\n",
    "\n",
    "            # creates column with normalized count of tuple (number of instances divided by\n",
    "            #  total word count in the corpus multiplied by 100,000)\n",
    "            trigram_df['NormCount_CleanedL3_L1_Tuple'] = trigram_df['Count_CleanedL3_L1_Tuple']/corpus_word_count_total*100000\n",
    "                        \n",
    "            #################\n",
    "\n",
    "            # adds a column with a list of tokens from R1 to R3 (1 word to the\n",
    "            # right to 3 words to the right of the feature) for each row\n",
    "            trigram_df['R1_R3_List'] = patterns_df[['R1', 'R2', 'R3']].values.tolist()\n",
    "\n",
    "            # adds a column with tuples of the R1_R3 lists\n",
    "            trigram_df['R1_R3_Tuple'] = [tuple(sublist) for sublist \n",
    "                                        in list(trigram_df['R1_R3_List'])]\n",
    "\n",
    "            # creates a list of the lists in the column                            \n",
    "            R1_R3_trigrams_list = list(trigram_df['R1_R3_List'])\n",
    "\n",
    "            # creates an empty list to be appended to\n",
    "            cleaned_R1_R3_trigrams_list = []\n",
    "\n",
    "            # loops through the lists in the L3_L1 trigrams list\n",
    "            for sublist in R1_R3_trigrams_list:\n",
    "                #creates an empty list to be appended to\n",
    "                # this list will be emptied on every loop        \n",
    "                cleaned_sublist = []\n",
    "\n",
    "                # loops through the word in each sub-list\n",
    "                for word in sublist:\n",
    "\n",
    "                    # if the token is a string, appends the word to the cleaned_sublist\n",
    "                    if type(word) != float:\n",
    "                        cleaned_sublist.append(word.lower().strip().translate(\n",
    "                            str.maketrans(\"\", \"\", punctuation_no_apostrophe_no_dash)))\n",
    "\n",
    "                    # skips the token if it is NaN\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                # appends the cleaned list to the larger list              \n",
    "                cleaned_R1_R3_trigrams_list.append(cleaned_sublist)\n",
    "\n",
    "            # creates a column with the cleaned lists            \n",
    "            trigram_df['CleanedR1_R3_List'] = cleaned_R1_R3_trigrams_list\n",
    "\n",
    "            # creates a column with cleaned tuples       \n",
    "            trigram_df['CleanedR1_R3_Tuple'] = [tuple(sublist) for sublist in cleaned_R1_R3_trigrams_list] \n",
    "\n",
    "            # creates an empty column for tuple counts to be inserted\n",
    "            trigram_df['Count_CleanedR1_R3_Tuple'] = np.nan\n",
    "\n",
    "            # count number of occurences of the trigram tuples in the corpus\n",
    "            for row in trigram_df.itertuples():\n",
    "\n",
    "                # inserts the count of the trigram tuple into the empty column\n",
    "                trigram_df.loc[row.Index, 'Count_CleanedR1_R3_Tuple'] = list(trigram_df['CleanedR1_R3_Tuple']).count(row.CleanedR1_R3_Tuple)\n",
    "\n",
    "            # creates column with normalized count of tuple (number of instances divided by\n",
    "            #  total word count in the corpus multiplied by 100,000)\n",
    "            trigram_df['NormCount_CleanedR1_R3_Tuple'] = trigram_df['Count_CleanedR1_R3_Tuple']/corpus_word_count_total*100000\n",
    "\n",
    "            \n",
    "            #################\n",
    "            \n",
    "            \n",
    "            def get_individual_L3_R3(headers_list):\n",
    "\n",
    "                \"\"\" \n",
    "                Takes in a list of column headers which match the L3-R3 positions\n",
    "                from the previous split content csvs. Creates new columns in the\n",
    "                dataframe which contain:\n",
    "\n",
    "                (1) The raw token of that position\n",
    "                (2) The cleaned token of that position\n",
    "                (3) The raw frequency count of that type in that position\n",
    "                (4) The normalized count of that type in that position\n",
    "\n",
    "                \"\"\"\n",
    "            \n",
    "                #loops through the headers list\n",
    "                for header in headers_list:\n",
    "                    \n",
    "                    # adds a column with a list of cleaned tokens\n",
    "                    trigram_df[f\"{header}\"] = patterns_df[f\"{header}\"]\n",
    "\n",
    "                    # adds a column with a list of cleaned tokens\n",
    "                    trigram_df[f\"Cleaned_{header}\"] = patterns_df[f\"{header}\"].apply(lambda x : x.lower().strip().translate(\n",
    "                                    str.maketrans(\"\", \"\", punctuation_no_apostrophe)) if type(x) != float else x)\n",
    "\n",
    "                    #creates an empty column to enter raw frequency counts into\n",
    "                    trigram_df[f\"{header}_Count\"] = np.nan\n",
    "\n",
    "                    # count number of occurences of the trigram tuples in the corpus\n",
    "                    for row in trigram_df.itertuples():\n",
    "\n",
    "                        # inserts the count of the trigram tuple into the empty column\n",
    "                        trigram_df.loc[row.Index, f\"{header}_Count\"] = list(trigram_df[f\"Cleaned_{header}\"]).count(trigram_df.loc[row.Index, f\"Cleaned_{header}\"])\n",
    "\n",
    "                    # creates column with normalized count of the token (number of instances divided by\n",
    "                    #  total word count in the corpus multiplied by 100,000)\n",
    "                    trigram_df[f\"{header}_Norm_Count\"] = trigram_df[f\"{header}_Count\"]/corpus_word_count_total*100000\n",
    "                    \n",
    "                #returns the dataframe\n",
    "                return trigram_df\n",
    "            \n",
    "            get_individual_L3_R3(['L3', 'L2', 'L1', 'R1', 'R2', 'R3'])\n",
    "            \n",
    "            \n",
    "            if search_word_string == \"ain\\'t\":\n",
    "                \n",
    "                #exports dataframe to csv\n",
    "                trigram_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_trigrams.csv\", index=False)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                #exports dataframe to csv\n",
    "                trigram_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_{search_word_string}_trigrams.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbb138-16c5-4bad-a6ad-f0ec2e055273",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating Quantitative  Dataframes and Exporting Dataframes to CSV Files\n",
    "\n",
    "This will execute the code and create the dataframes and then export them as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e2ae5-d92e-4e79-82fb-7dc85cf7653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the input path where the gold standard CSVs are stored\n",
    "csv_input_path = \"path\"\n",
    "\n",
    "# Designate the output path where the gold standard CSVs are stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aced28f-f203-4c58-811e-c855e23faddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33576a-bae7-4657-8742-eb0257a60356",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e6259-1812-4288-8d1e-054568c3945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"ain\\'t\"\n",
    "\n",
    "# execute code\n",
    "create_trigram_df(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1162a8-d9de-492d-9a12-022716aca883",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596c701-bbe7-4f9e-8ca3-b7d912d74d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"be\"\n",
    "\n",
    "# execute code\n",
    "create_trigram_df(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280668e0-81e0-42a9-986d-b2f35fc0f044",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e660b0d-27a7-4364-9d93-37acfc4335f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"done\"\n",
    "\n",
    "# execute code\n",
    "create_trigram_df(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b7c27-06f4-4143-8789-438214d1608b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
