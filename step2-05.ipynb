{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af98568-db11-4b00-9546-6cd0c9e4a154",
   "metadata": {},
   "source": [
    "# Step 2.5: Cleaning Utterance Content\n",
    "\n",
    "This code will clean the utterance content of the original CORAAL content along with the ASR transcriptions. This code is specifically attuned to address the idiosyncracies of the ASR outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27e6b-6798-4684-9d4b-b505d5bbc52f",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code: string, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/), [nltk](https://pypi.org/project/nltk/), [num2words](https://pypi.org/project/num2words/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f58c-3a5c-4109-91ae-bf6083dbf0da",
   "metadata": {},
   "source": [
    "## Intitial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9910a3-9f2c-450d-b5e8-70c68efaef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9bcf7-7295-44d6-8ef7-dabff40708fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath for the csv produced in Step 2.4\n",
    "aint_file_path = \"path\"\n",
    "\n",
    "be_file_path = \"path\"\n",
    "\n",
    "done_file_path = \"path\"\n",
    "\n",
    "#reads in the gold standard dataframe    \n",
    "aint_gs_df = pd.read_csv(aint_file_path)\n",
    "\n",
    "be_gs_df = pd.read_csv(be_file_path)\n",
    "\n",
    "done_gs_df = pd.read_csv(done_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575a508-d9e1-45d9-8664-a098566d539a",
   "metadata": {},
   "source": [
    "## Defining the Cleaning Function\n",
    "\n",
    "This function takes one argument:\n",
    "1. The utterance content as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e5c5f-ed40-46f8-9e5c-18e1582fa685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_utterance_content(utterance_content):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans the utterance content of both the original CORAAL utterances\n",
    "    and also the ASR outputs. This code is specifically attuned to address\n",
    "    the idiosyncracies of the ASR outputs.\n",
    "    \n",
    "    Takes the utterance content (a string) as it's first argument\n",
    "    \"\"\"\n",
    "\n",
    "    #imports required libraries\n",
    "    from num2words import num2words\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import numpy\n",
    "    import string\n",
    "    \n",
    "    \n",
    "    #if the entered utterance_content is not a string, returns a NaN\n",
    "    if type(utterance_content) != str:\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    #else, execute the code\n",
    "    else:\n",
    "\n",
    "        #a list of censored words. CORAAL includes these without censoring\n",
    "        #  however, the ASR services censor them. I believe it is possible to turn\n",
    "        #  off the censoring filter. However, given that most everyday users of an ASR like Siri would\n",
    "        #  likely not take this step, I felt it more natural to include the censorship\n",
    "        censor_words = [\"shit\", \"shits\", \"fuck\", \"fucks\", \"fucking\", \"fucked\",\n",
    "                       \"fucker\", \"fuckers\", \"motherfucker\", \"motherfuckers\",\n",
    "                       \"damn\", \"bitch\", \"bitches\", \"bastard\", \"bastards\",\n",
    "                       \"ass\", \"asses\", \"goddamn\", \"nigga\", \"niggas\"]\n",
    "\n",
    "        #a dictionary of reductions to be converted\n",
    "        reduction_dict = {\n",
    "            \"musta\": [\"must\", \"have\"],\n",
    "            \"woulda\": [\"would\", \"have\"],\n",
    "            \"shoulda\": [\"should\", \"have\"],\n",
    "            \"coulda\": [\"could\", \"have\"],\n",
    "            \"mighta\": [\"might\", \"have\"],\n",
    "            \"gonna\": [\"going\", \"to\"],\n",
    "            \"hafta\": [\"have\", \"to\"],\n",
    "            \"tryna\": [\"trying\", \"to\"],\n",
    "            \"sposta\": [\"supposed\", \"to\"],\n",
    "            \"finna\": [\"fixing\", \"to\"],\n",
    "            \"gotta\": [\"got\", \"to\"],\n",
    "            \"wanna\": [\"want\", \"to\"],\n",
    "            \"oughta\": [\"ought\", \"to\"],\n",
    "            \"cause\": [\"because\"],\n",
    "            \"til\": [\"until\"],\n",
    "            \"'em\": [\"them\"],\n",
    "            \"lemme\": [\"let\", \"me\"],\n",
    "            \"whatchu\": [\"what\", \"are\", \"you\"],\n",
    "            \"gotcha\": [\"got\", \"you\"],\n",
    "        }\n",
    "\n",
    "        \n",
    "        #replaces a few reductions because the nltk tokenizer will split them here\n",
    "        # and the reduction cleaner later in the code won't catch them. this should fix that\n",
    "        utterance_content = utterance_content.replace(\"gotta\", \"got to\").replace(\"gonna\", \"going to\").replace(\"wanna\", \"want to\").replace(\"lemme\", \"let me\").replace(\"%HESITATION\", \"\").replace(\"#\", \"number\").replace(\"&\", \"and\").replace(\"%\", \"percent\").replace(\"+\", \"plus\").replace(\"***\", \"****\").replace(\"*****\", \"****\").strip()\n",
    "        \n",
    "        \n",
    "        #strips the words in the utterance of hyphens and underscores\n",
    "        #  this is because if a conjunction has one of these at the end\n",
    "        #  nltk will not tokenize it correctly. instead of tokenizing\n",
    "        #  \"ain't-\" as [\"ai\", \"n't\", \"-\"] it will do [\"ain't=\"]. This\n",
    "        #  will take care of that ahead of the tokenizing\n",
    "        utterance_content = \" \".join([word.strip(string.punctuation) for word in utterance_content.split()])\n",
    "        \n",
    "        \n",
    "        #tokenizes the utterance content by word\n",
    "        # importantly: this tokenizer will separate punctuation on the edge\n",
    "        #  of words out into separate tokens. it will also separate conjunctions\n",
    "        #  into separate words as well as informally contracted words like \"gonna\"\n",
    "        tokenized_content = word_tokenize(utterance_content)\n",
    "\n",
    "        #loops through the tokens\n",
    "        for content_word in tokenized_content:\n",
    "\n",
    "            #gets the index of the token\n",
    "            index = tokenized_content.index(content_word)\n",
    "\n",
    "           \n",
    "            #replaces censored words with the correct amount of stars\n",
    "            #  depending on the service. if the service is deepspeech\n",
    "            #  (the else condition), the word is not replaced\n",
    "            if content_word in censor_words:\n",
    "\n",
    "                tokenized_content[index] = \"****\"\n",
    "\n",
    "\n",
    "            #replaces reductions with complete, separated words\n",
    "            elif content_word in reduction_dict:\n",
    "\n",
    "                if len(reduction_dict[content_word]) == 1:\n",
    "\n",
    "                    tokenized_content[index] = reduction_dict[content_word][0]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    tokenized_content = tokenized_content[:index] + reduction_dict[content_word] + tokenized_content[index+1:]\n",
    "\n",
    "\n",
    "\n",
    "            #looks for strings which contain the :\n",
    "            # which will only be times in the ASR transcriptions\n",
    "            #  this is because CORAAL generally transcribes times\n",
    "            #   into words rather than numbers\n",
    "            elif \":\" in content_word.strip(\":\"):\n",
    "\n",
    "                #separates the hour\n",
    "                hour = content_word.split(\":\")[0]\n",
    "\n",
    "                #separates the minute\n",
    "                minute = content_word.split(\":\")[1]\n",
    "\n",
    "                #converts the hour number to a word\n",
    "                hour = num2words(hour)\n",
    "\n",
    "                #converts the minute number to words\n",
    "                # if the minute is 00, converts to o'clock\n",
    "                if minute == \"00\":\n",
    "                    \n",
    "                    minute = \"o'clock\"\n",
    "\n",
    "                # any other minute, converts to number\n",
    "                else: \n",
    "                    minute = num2words(minute)\n",
    "\n",
    "                #breaks the original sentence into two by the index of the original time\n",
    "                #  adds the hour and minute at the index\n",
    "                tokenized_content = tokenized_content[:index] + [hour, minute] + tokenized_content[index+1:]\n",
    "\n",
    "\n",
    "            #converts a dollar or cent amount from numbers to words\n",
    "            elif tokenized_content[index-1] == \"$\":\n",
    "\n",
    "                #converts a cent amount\n",
    "                if content_word.startswith(\"0.\"):\n",
    "\n",
    "                    #splits at the decimal and takes only the cent amount number\n",
    "                    cent_amount = tokenized_content[index].split(\".\")[1]\n",
    "\n",
    "                    #converts words to numbers\n",
    "                    cent_amount = num2words(cent_amount)\n",
    "\n",
    "                    #replaces the dollar sign with the cent amount in the list\n",
    "                    tokenized_content[index-1] = cent_amount\n",
    "\n",
    "                    #replaces the number cent amount with the word 'cents' in the list\n",
    "                    tokenized_content[index] = \"cents\"\n",
    "\n",
    "\n",
    "                #converts a cent amount \n",
    "                else:\n",
    "\n",
    "                    #removes any potential comma in the number and converts to words\n",
    "                    dollar_amount = num2words(content_word.replace(\",\",\"\"))\n",
    "\n",
    "                    #creates a split list from the dollar_amount\n",
    "                    split_dollar_amount = dollar_amount.split()\n",
    "\n",
    "                    #if the dollar amount is a single word, which would either be\n",
    "                    #  a single digit or a two digit number like fifty-six\n",
    "                    #  then just replaces the number with the word\n",
    "                    if len(split_dollar_amount) == 1:\n",
    "\n",
    "                        #replaces the dollar sign with the dollar amount in the list\n",
    "                        tokenized_content[index-1] = dollar_amount\n",
    "\n",
    "                        #replaces the dollar amount with either the word \"dollar\" or \"dollars\" in the list\n",
    "                        if dollar_amount == \"one\":\n",
    "\n",
    "                            tokenized_content[index] = \"dollar\"\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            tokenized_content[index] = \"dollars\"\n",
    "\n",
    "                    #if the dollar amount is multiple words long, replaces the number\n",
    "                    #  with the words separated out\n",
    "                    else:\n",
    "\n",
    "                        #loops through the words in the split dollar amount\n",
    "                        for local_index in range(len(split_dollar_amount)):\n",
    "\n",
    "                            #inserts the words into the larger list one by one\n",
    "                            tokenized_content.insert(local_index + index, split_dollar_amount[local_index])\n",
    "\n",
    "                        #replaces the number amount with the word \"dollars\"\n",
    "                        tokenized_content[index + len(split_dollar_amount)] = \"dollars\"\n",
    "\n",
    "                        #removes the dollar sign from the list\n",
    "                        tokenized_content.pop(index-1)\n",
    "\n",
    "\n",
    "            #converts numbers to words\n",
    "            elif content_word.replace(\",\", \"\").isnumeric():\n",
    "\n",
    "                #removes commas because num2words won't accept it otherwise\n",
    "                number_words = num2words(content_word.replace(\",\", \"\"))\n",
    "\n",
    "                #creates a split list from number_words\n",
    "                split_number_words = number_words.split()\n",
    "\n",
    "                #if the number is a single digit or a hyphenated two digit like fifty-six\n",
    "                #  then the number is replaced by the word in the original list\n",
    "                if len(split_number_words) == 1:\n",
    "\n",
    "                    tokenized_content[index] = number_words\n",
    "\n",
    "                #if the number is multiple words long, replaces the number\n",
    "                #  with the words in the original list\n",
    "                else:\n",
    "\n",
    "                    #loops through the words in the split number words\n",
    "                    for local_index in range(len(split_number_words)):\n",
    "\n",
    "                        #inserts the words into the larger list one by one\n",
    "                        tokenized_content.insert(local_index + index, split_number_words[local_index])\n",
    "\n",
    "                    #removes the original number\n",
    "                    tokenized_content.pop(index + len(split_number_words))\n",
    "\n",
    "\n",
    "\n",
    "            # removes tokens which are only punctuation\n",
    "            #  this list of punctuation is specific and not the same as \n",
    "            #  the python string library which has string.punctuation\n",
    "            elif content_word in '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~':\n",
    "\n",
    "                tokenized_content.remove(content_word)\n",
    "\n",
    "\n",
    "        #strips hyphens from the ends of words\n",
    "        #  lowercases all letters\n",
    "        #  removes any redactions in CORAAL because anytime a redaction \n",
    "        #  occurs in the audio, it is covered by a beep which the ASR\n",
    "        #  will not be able to transcribe\n",
    "        tokenized_content = [token.strip(\"-\").lower() for token in tokenized_content if \"RD-NAME\" not in token]     \n",
    "        \n",
    "        tokenized_content = (\" \").join(tokenized_content)\n",
    "        \n",
    "        tokenized_content = tokenized_content.replace(\"o'clock o'clock\", \"o'clock\")\n",
    "        \n",
    "        return tokenized_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21f853-1ba1-4f40-8a25-51d17bbfe75d",
   "metadata": {},
   "source": [
    "## Executing the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603357e-6c44-4187-9f44-fd0e00840f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of column names to be appended next to\n",
    "column_names = [\"Content\", \"amazon_transcription\", \"deepspeech_transcription\", \n",
    "                \"google_transcription\", \"IBMWatson_transcription\", \"microsoft_transcription\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63203f6b-2d23-492b-a8c3-6bfd15f0125d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fe7a4-fbb7-454c-8320-1853dfa98a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through column names\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = aint_gs_df.columns.get_loc(column_name)\n",
    "\n",
    "    aint_gs_df.insert(col_index+1, f\"{column_name}_cleaned\", np.nan)\n",
    "\n",
    "# loops through the rows, cleans each utterance content and writes the result\n",
    "for file_row in aint_gs_df.itertuples():\n",
    "\n",
    "    aint_gs_df.loc[file_row.Index, \"Content_cleaned\"] = clean_utterance_content(file_row.Content)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned\"] = clean_utterance_content(file_row.amazon_transcription)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned\"] = clean_utterance_content(file_row.deepspeech_transcription)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"google_transcription_cleaned\"] = clean_utterance_content(file_row.google_transcription)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned\"] = clean_utterance_content(file_row.IBMWatson_transcription)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned\"] = clean_utterance_content(file_row.microsoft_transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f839-307e-4e80-a648-1842e9f01eb5",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa61f4-9499-4a75-afcd-b44d3509ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through column names\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = be_gs_df.columns.get_loc(column_name)\n",
    "\n",
    "    be_gs_df.insert(col_index+1, f\"{column_name}_cleaned\", np.nan)\n",
    "\n",
    "# loops through the rows, cleans each utterance content and writes the result\n",
    "for file_row in be_gs_df.itertuples():\n",
    "\n",
    "    be_gs_df.loc[file_row.Index, \"Content_cleaned\"] = clean_utterance_content(file_row.Content)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned\"] = clean_utterance_content(file_row.amazon_transcription)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned\"] = clean_utterance_content(file_row.deepspeech_transcription)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"google_transcription_cleaned\"] = clean_utterance_content(file_row.google_transcription)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned\"] = clean_utterance_content(file_row.IBMWatson_transcription)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned\"] = clean_utterance_content(file_row.microsoft_transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1093-19ed-4e4b-be56-0aec20f9002c",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb105bf-9f1c-470e-ba6c-9db49b30ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through column names\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = done_gs_df.columns.get_loc(column_name)\n",
    "\n",
    "    done_gs_df.insert(col_index+1, f\"{column_name}_cleaned\", np.nan)\n",
    "\n",
    "# loops through the rows, cleans each utterance content and writes the result\n",
    "for file_row in done_gs_df.itertuples():\n",
    "\n",
    "    done_gs_df.loc[file_row.Index, \"Content_cleaned\"] = clean_utterance_content(file_row.Content)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned\"] = clean_utterance_content(file_row.amazon_transcription)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned\"] = clean_utterance_content(file_row.deepspeech_transcription)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"google_transcription_cleaned\"] = clean_utterance_content(file_row.google_transcription)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned\"] = clean_utterance_content(file_row.IBMWatson_transcription)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned\"] = clean_utterance_content(file_row.microsoft_transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c303e-b650-4e9d-bc79-e279751611fe",
   "metadata": {},
   "source": [
    "## Sorting the Dataframes by File and Line\n",
    "\n",
    "This will sort the dataframes first by filename and then by line number. Doing this each step will ensure consistency across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57205-3bfd-4933-bdbe-5f8e6e75ea9d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f32913-4e1f-44ea-bd82-f0fdbfd167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = aint_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a08157-c6a8-4cbb-8978-e6af4b931a7e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12356d-7ffb-4af6-9c58-21e00fae9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df = be_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cd837-4bb3-4129-b4db-583aeb11ea90",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc53ff-73c4-484b-ad31-2025fba38270",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df = done_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983788-c9a5-4b73-8a2e-1d18a6ae24b9",
   "metadata": {},
   "source": [
    "## Exporting Dataframes to CSV Files\n",
    "\n",
    "This will export the dataframes to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff3dee-e5ab-47f3-a75d-14de88610b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the output path where the CSVs will be stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331a54-e11e-4b58-813a-717796e37e73",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e3561-7148-4ace-aa8f-2e2ca60a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df.to_csv(f\"{csv_output_path}aint_variations_cleanedUtterances.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf982d6-41cb-46d8-9917-8d4a6a5bdd4d",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6042b-fa7e-4017-aac3-7cfca0b8fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df.to_csv(f\"{csv_output_path}be_cleanedUtterances.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101802dd-7fd7-4676-a52b-8b9d76e1c990",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cab56-e32a-467e-aa8e-12a6640b5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df.to_csv(f\"{csv_output_path}done_cleanedUtterances.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df4499-4d1b-4953-939e-b1179bebf0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
