{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf78ff63-ae1b-4a6c-a959-6a71d2d83a76",
   "metadata": {},
   "source": [
    "# Step 1.6: Getting Quantitative Information about Feature Usage\n",
    "\n",
    "This code will produce a number of CSV files, which will provide various information on the corpora. These CSVs will include:\n",
    "\n",
    "<ol>\n",
    "<li>Information on subject tokens which precede the feature</li>\n",
    "<li>Information on predicate tokens which follow the feature</li>\n",
    "<li>Information on subject parts of speech which precede the feature</li>\n",
    "<li>Information on predicate parts of speech which follow the feature</li>\n",
    "<li>Information on word patterns which surround/co-occur with the feature</li>\n",
    "<li>Information on part of speech patterns which surround/co-occur with the feature</li>\n",
    "<li>A complete all corpora info CSV with the following new information added:\n",
    "    <ul>\n",
    "        <li>Total feature count for the corpus</li>\n",
    "        <li>Total feature count normalized for the corpus</li>\n",
    "        <li>List of unique filenames (filename types) where the feature occurs</li>\n",
    "        <li>Total file count where the feature occurs</li>\n",
    "        <li>Percent of total corpus files of where the feature occurs</li>\n",
    "        <li>Total possible part of speech patterns count</li>\n",
    "        <li>Total occuring part of speech patterns count</li>\n",
    "        <li>Percent of total part of speech patterns which occur</li>\n",
    "    </ul>\n",
    "    </li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e01ee8-e091-4f3a-9c45-9c7766929bce",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code:\n",
    "string, os, re, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30c5cd-2dd2-4813-b263-e98e6bf04fb6",
   "metadata": {},
   "source": [
    "## Define the Dataframe Creating Function\n",
    "\n",
    "This function takes the following arguments:\n",
    "\n",
    "<ol>\n",
    "<li>The filepath to the split content CSV produced in Step 1.5</li>\n",
    "<li>The filepath to the folder where the newly created CSVs will be stored</li>\n",
    "<li>The word being searched for</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909529e-7a68-4e98-a5d8-291a835eb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpora_stats_csvs(csv_input_path, cvs_output_path, search_word_string):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Reads in split content csvs produced in step 1-5 and creates the following csvs:\n",
    "    (1) A complete all corpora info csv with the following new information added:\n",
    "        (a) Total feature count for the corpus\n",
    "        (b) Total feature count normalized for the corpus\n",
    "        (c) List of unique filenames (filename types) where the feature occurs\n",
    "        (d) Total file count where the feature occurs\n",
    "        (e) Percent of total corpus files of where the feature occurs\n",
    "        (f) Total possible part of speech patterns count\n",
    "        (g) Total occuring part of speech patterns count\n",
    "        (h) Percent of total part of speech patterns which occur\n",
    "    (2) Information on subject tokens which precede the feature\n",
    "    (3) Information on predicate tokens which follow the feature\n",
    "    (4) Information on subject parts of speech which precede the feature\n",
    "    (5) Information on predicate parts of speech which follow the feature\n",
    "    (6) Information on word patterns which surround/co-occur with the feature\n",
    "    (7) Information on part of speech patterns which surround/co-occur with the feature\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from string import punctuation\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    if search_word_string == \"ain\\'t\":\n",
    "        \n",
    "        #creates a list of the split content .csv's which should be stored together in \n",
    "        # the same folder (csv_input_path)\n",
    "        csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                         if file.endswith(\".csv\") and\n",
    "                         file.startswith(\"aint\") and \"info\" not in file]\n",
    "        \n",
    "        # filename for the all_corpora_info csv from Step1-3\n",
    "        #  creates a list of the one filename and then uses [0] to get the filename\n",
    "        #  out of the list format\n",
    "        all_corpora_info_csv_path = [f\"{csv_input_path}{filename}\" \n",
    "                                     for filename in os.listdir(csv_input_path) \n",
    "                                     if \"all_corpora_info\" in filename \n",
    "                                     and filename.startswith(\"aint\")][0]\n",
    "    \n",
    "    else:\n",
    "                                     \n",
    "        #creates a list of the split content .csv's which should be stored together in \n",
    "        # the same folder (csv_input_path)\n",
    "        csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                         if file.endswith(\".csv\") and\n",
    "                         file.startswith(search_word_string) and \"info\" not in file]\n",
    "\n",
    "        # filename for the all_corpora_info csv from Step1-3\n",
    "        #  creates a list of the one filename and then uses [0] to get the filename\n",
    "        #  out of the list format\n",
    "        all_corpora_info_csv_path = [f\"{csv_input_path}{filename}\" \n",
    "                                     for filename in os.listdir(csv_input_path) \n",
    "                                     if \"all_corpora_info\" in filename \n",
    "                                     and filename.startswith(search_word_string)][0]\n",
    "\n",
    "    #creates a list of tuples with the csv input full paths and the corpus name\n",
    "    filePath_corpusName = [(f\"{csv_input_path}{filename}\",\n",
    "                            re.search(r\"_(.*?)_\", filename).group(1).lower())\n",
    "                           for filename in csv_filenames if \"info\" not in filename]\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    # creates a dataframe from the all_corpora_info_df csv\n",
    "    all_corpora_info_df = pd.read_csv(f\"{all_corpora_info_csv_path}\", index_col=0)\n",
    "    \n",
    "    # lowercases the column names of this dataframe, which are corpus names\n",
    "    #  this is to ensure the next immediate lines of code will fuction correctly\n",
    "    all_corpora_info_df.columns = map(str.lower, all_corpora_info_df.columns)\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    # creates an empty dataframe for feature count, normalized feature count,\n",
    "    # file types, file types count, file types percent total, possible POS\n",
    "    # patterns count, actually occuring POS patterns count, percent of total\n",
    "    # possible POS patterns that actually occur\n",
    "    # this dataframe will be filled in and then appended to the all_corpora_info\n",
    "    # dataframe later\n",
    "    \n",
    "    all_corpora_to_append_df = pd.DataFrame(columns=['coraal',\n",
    "                                                     'fisher',\n",
    "                                                     'librispeech',\n",
    "                                                     'switchboardhub5', \n",
    "                                                     'timit'],\n",
    "                                            index=['FeatureCount', \n",
    "                                                   'FeatureCountNormalized',\n",
    "                                                   'FilesWithFeatureCount', \n",
    "                                                   'PercentTotalFilesWithFeature',\n",
    "                                                   'FilesWithFeatureList', \n",
    "                                                   'SubjectWordTypeCount',\n",
    "                                                   'PredicateWordTypeCount',\n",
    "                                                   'PossiblePOSPatternCount',\n",
    "                                                   'OccuringPOSPatternCount',\n",
    "                                                   'PercentTotalPOSPatternOccuringCount',\n",
    "                                                   'PossiblePOSPatternList',\n",
    "                                                   'OccurringPOSPatternList'])\n",
    "    \n",
    "    # creates an empty list for possible part of speech patterns\n",
    "    #  this will be used later to calculate perent of total possible patterns\n",
    "    #  that actually occur in each corpus\n",
    "    total_possible_POS_patterns = []\n",
    "    \n",
    "    ###################\n",
    "    \n",
    "    \n",
    "    # loops through the filepath and corpus name tuples list\n",
    "    for file_path, corpus_name in filePath_corpusName:\n",
    "    \n",
    "        # creates a variable of the corpus' total word count\n",
    "        corpus_word_count_total = all_corpora_info_df.loc['TotalCorpusWordCount', corpus_name]\n",
    "        \n",
    "        # creates a variable of the corpus' total file count\n",
    "        corpus_file_count_total = all_corpora_info_df.loc['TotalFileCount', corpus_name]\n",
    "        \n",
    "        # creates a string of punctuation markers to be used to filter out punctuation\n",
    "        #  in cleaning stages later\n",
    "        punctuation_modified = punctuation.replace(\"'\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "        \n",
    "        ###################\n",
    "        \n",
    "        # creates a dataframe from the split content csv for the corpus\n",
    "        patterns_df = pd.read_csv(file_path)\n",
    "\n",
    "        # if the feature count in a corpus is zero, skips the corpus\n",
    "        if len(patterns_df) == 0:\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            ####################\n",
    "\n",
    "\n",
    "            # calculates the number of features in the corpora by counting\n",
    "            #  the number of lines in the split content dataframe\n",
    "            #  adds the total to the dataframe\n",
    "            all_corpora_to_append_df.loc['FeatureCount', corpus_name] = len(patterns_df)\n",
    "\n",
    "            # normalizes the number of features in the corpora by taking the feature count\n",
    "            #  dividing by the total number of words in the corpus and multiplying by 100,000\n",
    "            #  this gives a measure of feature occurence per 100,000 words\n",
    "            #  adds the normalized count to the dataframe\n",
    "            all_corpora_to_append_df.loc['FeatureCountNormalized', corpus_name] = float(f\"{(len(patterns_df)/corpus_word_count_total)*100000:.3f}\")\n",
    "\n",
    "            # calculates the total number of unique files in which the feature occurs at least once\n",
    "            #  adds the total to the dataframe\n",
    "            all_corpora_to_append_df.loc['FilesWithFeatureCount', corpus_name] = len(sorted(set(patterns_df['File'])))\n",
    "\n",
    "            # calculates the percent of total files in which the feature occurs\n",
    "            #  adds the percentage to the dataframe\n",
    "            all_corpora_to_append_df.loc['PercentTotalFilesWithFeature', corpus_name] = float(f\"{(len(sorted(set(patterns_df['File'])))/corpus_file_count_total)*100:.2f}\")\n",
    "\n",
    "            # creates a list of files in which the feature occurs at least once\n",
    "            #  adds the list to the dataframe\n",
    "            all_corpora_to_append_df.loc['FilesWithFeatureList', corpus_name] = sorted(set(patterns_df['File']))\n",
    "\n",
    "\n",
    "            ####################\n",
    "\n",
    "\n",
    "            # creates a list of all subject tokens, lowercased, stripped of white space on right and left, and punctuation\n",
    "            #  other than ' and _ removed. \n",
    "            subject_tokens = sorted([token.lower().strip().translate(str.maketrans(\"\", \"\", punctuation_modified)) \n",
    "                                     for token in list(patterns_df['SubjectWordToken'])])\n",
    "\n",
    "            # creates a list of unique subject word types\n",
    "            subject_types = sorted(set(subject_tokens))\n",
    "\n",
    "            # calculates the total number of subject tokens\n",
    "            subject_tokens_count = len(subject_tokens)\n",
    "\n",
    "            # calculates the total number of subject word types                                                                         \n",
    "            subject_types_count = len(subject_types)\n",
    "\n",
    "            # adds the total number of subject word types to the all_corpora dataframe                                                                       \n",
    "            all_corpora_to_append_df.loc['SubjectWordTypeCount', corpus_name] = subject_types_count\n",
    "\n",
    "\n",
    "\n",
    "            # creates a dataframe for subject word tokens\n",
    "            #  uses subject word types as row index names\n",
    "            subject_tokens_df = pd.DataFrame(index=subject_types)\n",
    "\n",
    "            # counts the number of token occurences of a subject word type\n",
    "            #  adds the total as a new column to the subject_tokens dataframe\n",
    "            subject_tokens_df['SubjectRawCount'] = [subject_tokens.count(subject_type) for subject_type in subject_types]\n",
    "\n",
    "            # calculates the normalized count of subject word occurences in the corpus per 100,000 words\n",
    "            #  adds the normalized count to the subject_tokens dataframe\n",
    "            subject_tokens_df['SubjectNormalizedCount'] = [float(f\"{(subject_tokens.count(subject_type)/corpus_word_count_total)*100000:.3f}\") \n",
    "                                                           for subject_type in subject_types]\n",
    "\n",
    "            # calculates the percent of total subject token count the subject type occurs\n",
    "            #  adds the percentage to the subject_tokens dataframe\n",
    "            subject_tokens_df['SubjectPercentTotal'] = [float(f\"{(subject_tokens.count(subject_type) / subject_tokens_count)*100:.2f}\") \n",
    "                                                        for subject_type in subject_types]\n",
    "\n",
    "\n",
    "\n",
    "            # creates an empty list to be appended to\n",
    "            subject_token_files = []\n",
    "\n",
    "            # loops through the rows in the patterns_df\n",
    "            for row in patterns_df.itertuples():\n",
    "\n",
    "                #creates a tuple of the cleaned subject word token and the file it occurs within\n",
    "                # appends the tuple to the empty subject_token_files list\n",
    "                subject_token_files.append((row.SubjectWordToken.lower().strip().translate(str.maketrans(\"\", \"\", punctuation_modified)), row.File))\n",
    "\n",
    "            # creates a list of unique subject token + file occurences \n",
    "            # in other words, if a subject occurs more than once in a file, that subject-file combo only counts once\n",
    "            subject_type_files = sorted(set(subject_token_files))\n",
    "\n",
    "            # creates an empty dictionary to be appended to\n",
    "            subject_type_files_dict = {}\n",
    "\n",
    "            # loops through the list of subject_type_files tuples\n",
    "            for type_file in subject_type_files:\n",
    "\n",
    "                # if the subject word is in the dictionary as a key already,\n",
    "                #  the filename is added to the list of filenames stored in the value for that key\n",
    "                if type_file[0] in subject_type_files_dict:\n",
    "                    subject_type_files_dict[type_file[0]].append(type_file[1])\n",
    "\n",
    "                # if the subject word is not in the dictionary yet,\n",
    "                #  creates a key of that subject word and creates a list with the filename to be the value\n",
    "                else:\n",
    "                    subject_type_files_dict[type_file[0]] = [type_file[1]]\n",
    "\n",
    "\n",
    "            # loops through the key, value pairs in the subject_type_files_dict\n",
    "            #  and calculates the length of the list to get the number of unique files in which the subject word occurs\n",
    "            #  adds the resulting list as a column to subject_tokens_df\n",
    "            subject_tokens_df['SubjectFileCount'] = [len(filename_list) for subject_type, filename_list in subject_type_files_dict.items()]\n",
    "\n",
    "            # loops through the key, value pairs in the subject_type_files_dict\n",
    "            #  and calculates the percent of the total number of files in which each subject word occurs\n",
    "            #  adds the resulting list as a column to subject_tokens_df\n",
    "            subject_tokens_df['SubjectFilePercentTotal'] = [float(f\"{(len(filename_list)/corpus_file_count_total)*100:.2f}\") \n",
    "                                                           for subject_type, filename_list in subject_type_files_dict.items()]\n",
    "\n",
    "            # loops through the key, value pairs in the subject_type_files_dict\n",
    "            #  and stores the list of filenames for each subject word\n",
    "            #  adds the resulting list as a column to subject_tokens_df\n",
    "            subject_tokens_df['SubjectFileList'] = [filename_list for subject_type, filename_list in subject_type_files_dict.items()]\n",
    "\n",
    "            # sorts the subject_tokens_df by normalized count of the subject word, from highest to lowest normalized count\n",
    "            subject_tokens_df = subject_tokens_df.sort_values(by='SubjectNormalizedCount', ascending=False)\n",
    "            \n",
    "            if search_word_string == \"ain\\'t\":\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                subject_tokens_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_subjectTokens.csv\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                subject_tokens_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_subjectTokens.csv\")\n",
    "\n",
    "            \n",
    "            \n",
    "            ####################   \n",
    "\n",
    "\n",
    "\n",
    "            # the following code follows the exact same process as for word tokens and types\n",
    "            #  and applies it to parts of speech. please see documentation for corresponding\n",
    "            #  lines above\n",
    "\n",
    "            subject_POS_tokens = sorted([subject_POS for subject_POS in list(patterns_df['SubjectPOS'])])\n",
    "\n",
    "            subject_POS_types = sorted(set(subject_POS_tokens))\n",
    "\n",
    "            subject_POS_tokens_count = len(subject_POS_tokens)\n",
    "\n",
    "            subject_POS_types_count = len(subject_POS_types)\n",
    "\n",
    "\n",
    "\n",
    "            subject_POS_df = pd.DataFrame(index=subject_POS_types)\n",
    "\n",
    "            subject_POS_df['SubjectPOSRawCount'] = [subject_POS_tokens.count(subject_POS_type) for subject_POS_type in subject_POS_types]\n",
    "\n",
    "            subject_POS_df['SubjectPOSNormalizedCount'] = [float(f\"{(subject_POS_tokens.count(subject_POS_type)/corpus_word_count_total)*100000:.3f}\") \n",
    "                                                           for subject_POS_type in subject_POS_types]\n",
    "\n",
    "            subject_POS_df['SubjectPOSPercentTotal'] = [float(f\"{(subject_POS_tokens.count(subject_POS_type) / subject_POS_tokens_count)*100:.2f}\") \n",
    "                                                        for subject_POS_type in subject_POS_types]\n",
    "\n",
    "\n",
    "\n",
    "            subject_POS_token_files = []\n",
    "\n",
    "            for row in patterns_df.itertuples():\n",
    "                \n",
    "                subject_POS_token_files.append((row.SubjectPOS, row.File))\n",
    "\n",
    "            subject_POS_type_files = sorted(set(subject_POS_token_files))\n",
    "\n",
    "            subject_POS_type_files_dict = {}\n",
    "\n",
    "            for POS_type_file in subject_POS_type_files: \n",
    "\n",
    "                if POS_type_file[0] in subject_POS_type_files_dict:\n",
    "                    subject_POS_type_files_dict[POS_type_file[0]].append(POS_type_file[1])\n",
    "\n",
    "                else:\n",
    "                    subject_POS_type_files_dict[POS_type_file[0]] = [POS_type_file[1]]\n",
    "\n",
    "\n",
    "\n",
    "            subject_POS_df['SubjectPOSFileCount'] = [len(filename_list) for subject_POS_type, filename_list in subject_POS_type_files_dict.items()]\n",
    "\n",
    "            subject_POS_df['SubjectPOSFilePercentTotal'] = [float(f\"{(len(filename_list)/corpus_file_count_total)*100:.2f}\") \n",
    "                                                                    for subject_POS_type, filename_list in subject_POS_type_files_dict.items()]\n",
    "\n",
    "            subject_POS_df['SubjectPOSFileList'] = [filename_list for subject_POS_type, filename_list in subject_POS_type_files_dict.items()]\n",
    "\n",
    "            subject_POS_df = subject_POS_df.sort_values(by='SubjectPOSNormalizedCount', ascending=False)\n",
    "            \n",
    "            if search_word_string == \"ain\\'t\":\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                subject_POS_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_subjectPOS.csv\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                subject_POS_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_subjectPOS.csv\")\n",
    "\n",
    "            \n",
    "            \n",
    "            ####################                                                                       \n",
    "\n",
    "\n",
    "\n",
    "            # creates a list of all predicate tokens, lowercased, stripped of white space on right and left, and punctuation\n",
    "            #  other than ' and _ removed.                                                                          \n",
    "            predicate_tokens = sorted([token.lower().strip().translate(str.maketrans(\"\", \"\", punctuation_modified)) \n",
    "                                       for token in list(patterns_df['PredicateWordToken'])])\n",
    "\n",
    "            # creates a list of unique predicate word types                                                                         \n",
    "            predicate_types = sorted(set(predicate_tokens))\n",
    "\n",
    "            # calculates the total number of predicate tokens                                                                         \n",
    "            predicate_tokens_count = len(predicate_tokens)\n",
    "\n",
    "            # calculates the total number of predicate word types                                                                         \n",
    "            predicate_types_count = len(predicate_types)\n",
    "\n",
    "            # adds the total number of predicate word types to the all_corpora dataframe                                                                       \n",
    "            all_corpora_to_append_df.loc['PredicateWordTypeCount', corpus_name] = predicate_types_count\n",
    "\n",
    "\n",
    "\n",
    "            # creates a dataframe for predicate word tokens\n",
    "            #  uses predicate word types as row index names\n",
    "            predicate_tokens_df = pd.DataFrame(index=predicate_types) \n",
    "\n",
    "            # counts the number of token occurences of a predicate word type\n",
    "            #  adds the total as a new column to the predicate_tokens dataframe\n",
    "            predicate_tokens_df['PredicateRawCount'] = [predicate_tokens.count(predicate_type) for predicate_type in predicate_types] \n",
    "\n",
    "            # calculates the normalized count of predicate word occurences in the corpus per 100,000 words\n",
    "            #  adds the normalized count to the predicate_tokens dataframe\n",
    "            predicate_tokens_df['PredicateNormalizedCount'] = [float(f\"{(predicate_tokens.count(predicate_type)/corpus_word_count_total)*100000:.3f}\") \n",
    "                                                               for predicate_type in predicate_types]\n",
    "\n",
    "            # calculates the percent of total predicate token count the predicate type occurs\n",
    "            #  adds the percentage to the predicate_tokens dataframe\n",
    "            predicate_tokens_df['PredicatePercentTotal'] = [float(f\"{(predicate_tokens.count(predicate_type) / predicate_tokens_count)*100:.2f}\") \n",
    "                                                            for predicate_type in predicate_types]\n",
    "\n",
    "\n",
    "\n",
    "            # creates an empty list to be appended to\n",
    "            predicate_token_files = []\n",
    "\n",
    "            # loops through the rows in the patterns_df\n",
    "            for row in patterns_df.itertuples():   \n",
    "\n",
    "                #creates a tuple of the cleaned predicate word token and the file it occurs within\n",
    "                # appends the tuple to the empty predicate_token_files list\n",
    "                predicate_token_files.append((row.PredicateWordToken.lower().strip().translate(str.maketrans(\"\", \"\", punctuation_modified)), row.File))\n",
    "\n",
    "            # creates a list of unique predicate token + file occurences \n",
    "            # in other words, if a predicate occurs more than once in a file, that predicate-file combo only counts once\n",
    "            predicate_type_files = sorted(set(predicate_token_files))\n",
    "\n",
    "            # creates an empty dictionary to be appended to\n",
    "            predicate_type_files_dict = {}\n",
    "\n",
    "            # loops through the list of predicate_type_files tuples\n",
    "            for type_file in predicate_type_files:  \n",
    "\n",
    "                # if the predicate word is in the dictionary as a key already,\n",
    "                #  the filename is added to the list of filenames stored in the value for that key\n",
    "                if type_file[0] in predicate_type_files_dict:\n",
    "                    \n",
    "                    predicate_type_files_dict[type_file[0]].append(type_file[1])  \n",
    "\n",
    "                # if the predicate word is not in the dictionary yet,\n",
    "                #  creates a key of that predicate word and creates a list with the filename to be the value\n",
    "                else:\n",
    "                    predicate_type_files_dict[type_file[0]] = [type_file[1]]\n",
    "\n",
    "\n",
    "            # loops through the key, value pairs in the subject_type_files_dict\n",
    "            #  and calculates the length of the list to get the number of unique files in which the predicate word occurs\n",
    "            #  adds the resulting list as a column to predicate_tokens_df\n",
    "            predicate_tokens_df['PredicateFileCount'] = [len(filename_list) for predicate_type, filename_list in predicate_type_files_dict.items()] \n",
    "\n",
    "            # loops through the key, value pairs in the predicate_type_files_dict\n",
    "            #  and calculates the percent of the total number of files in which each predicate word occurs\n",
    "            #  adds the resulting list as a column to predicate_tokens_df\n",
    "            predicate_tokens_df['PredicateFilePercentTotal'] = [float(f\"{(len(filename_list)/corpus_file_count_total)*100:.2f}\") \n",
    "                                                           for predicate_type, filename_list in predicate_type_files_dict.items()]\n",
    "\n",
    "            # loops through the key, value pairs in the predicate_type_files_dict\n",
    "            #  and stores the list of filenames for each predicate word\n",
    "            #  adds the resulting list as a column to predicate_tokens_df\n",
    "            predicate_tokens_df['PredicateFileList'] = [filename_list for predicate_type, filename_list in predicate_type_files_dict.items()]\n",
    "\n",
    "            # sorts the predicate_tokens_df by normalized count of the predicate word, from highest to lowest normalized count\n",
    "            predicate_tokens_df = predicate_tokens_df.sort_values(by='PredicateNormalizedCount', ascending=False)\n",
    "            \n",
    "            \n",
    "            if search_word_string == \"ain\\'t\":\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                predicate_tokens_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_predicateTokens.csv\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                predicate_tokens_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_predicateTokens.csv\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            ####################\n",
    "\n",
    "\n",
    "\n",
    "            # the following code follows the exact same process as for word tokens and types\n",
    "            #  and applies it to parts of speech. please see documentation for corresponding\n",
    "            #  lines above\n",
    "\n",
    "            predicate_POS_tokens = sorted([predicate_POS for predicate_POS in list(patterns_df['PredicatePOS'])])\n",
    "\n",
    "            predicate_POS_types = sorted(set(predicate_POS_tokens))\n",
    "\n",
    "            predicate_POS_tokens_count = len(predicate_POS_tokens)\n",
    "\n",
    "            predicate_POS_types_count = len(predicate_POS_types)\n",
    "\n",
    "\n",
    "\n",
    "            predicate_POS_df = pd.DataFrame(index=predicate_POS_types)\n",
    "\n",
    "            predicate_POS_df['PredicatePOSRawCount'] = [predicate_POS_tokens.count(predicate_POS_type) \n",
    "                                                        for predicate_POS_type in predicate_POS_types]\n",
    "\n",
    "            predicate_POS_df['PredicatePOSNormalizedCount'] = [float(f\"{(predicate_POS_tokens.count(predicate_POS_type)/corpus_word_count_total)*100000:.3f}\") \n",
    "                                                               for predicate_POS_type in predicate_POS_types]\n",
    "\n",
    "            predicate_POS_df['PredicatePOSPercentTotal'] = [float(f\"{(predicate_POS_tokens.count(predicate_POS_type) / predicate_POS_tokens_count)*100:.2f}\") \n",
    "                                                            for predicate_POS_type in predicate_POS_types]\n",
    "\n",
    "\n",
    "\n",
    "            predicate_POS_token_files = []\n",
    "\n",
    "            for row in patterns_df.itertuples():\n",
    "                \n",
    "                predicate_POS_token_files.append((row.PredicatePOS, row.File))\n",
    "\n",
    "            predicate_POS_type_files = sorted(set(predicate_POS_token_files))\n",
    "\n",
    "            predicate_POS_type_files_dict = {}\n",
    "\n",
    "            for POS_type_file in predicate_POS_type_files: \n",
    "\n",
    "                if POS_type_file[0] in predicate_POS_type_files_dict:\n",
    "                    \n",
    "                    predicate_POS_type_files_dict[POS_type_file[0]].append(POS_type_file[1])\n",
    "\n",
    "                else:\n",
    "                    predicate_POS_type_files_dict[POS_type_file[0]] = [POS_type_file[1]]\n",
    "\n",
    "\n",
    "\n",
    "            predicate_POS_df['PredicatePOSFileRawCount'] = [len(filename_list) for predicate_POS_type, filename_list in predicate_POS_type_files_dict.items()]\n",
    "\n",
    "            predicate_POS_df['PredicatePOSFilePercentTotal'] = [float(f\"{(len(filename_list)/corpus_file_count_total)*100:.2f}\") \n",
    "                                                                       for predicate_POS_type, filename_list in predicate_POS_type_files_dict.items()]\n",
    "\n",
    "            predicate_POS_df['PredicatePOSFileList'] = [filename_list for predicate_POS_type, filename_list in predicate_POS_type_files_dict.items()]\n",
    "\n",
    "            predicate_POS_df = predicate_POS_df.sort_values(by='PredicatePOSNormalizedCount', ascending=False)\n",
    "            \n",
    "            \n",
    "            if search_word_string == \"ain\\'t\":\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                predicate_POS_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_predicatePOS.csv\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                predicate_POS_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_predicatePOS.csv\")\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            ####################\n",
    "\n",
    "\n",
    "            # the following code follows the exact same process as for word tokens and types\n",
    "            #  and applies it to word patterns. these are the collocations of subject-predicate\n",
    "            #  word token/types that co-occur with the feature\n",
    "            #  please see documentation for corresponding lines above\n",
    "\n",
    "            word_pattern_tokens = sorted([word_pattern.lower().strip().translate(str.maketrans(\"\", \"\", punctuation_modified)) \n",
    "                                          for word_pattern in list(patterns_df['WordPattern'])])\n",
    "\n",
    "            word_pattern_types = sorted(set(word_pattern_tokens))\n",
    "\n",
    "            word_pattern_tokens_count = len(word_pattern_tokens)\n",
    "\n",
    "            word_pattern_types_count = len(word_pattern_types)\n",
    "\n",
    "\n",
    "\n",
    "            word_pattern_df = pd.DataFrame(index=word_pattern_types)\n",
    "\n",
    "            word_pattern_df['WordPatternRawCount'] = [word_pattern_tokens.count(word_pattern_type) for word_pattern_type in word_pattern_types]\n",
    "\n",
    "            word_pattern_df['WordPatternNormalizedCount'] = [float(f\"{(word_pattern_tokens.count(word_pattern_type)/corpus_word_count_total)*100000:.3f}\") \n",
    "                                                             for word_pattern_type in word_pattern_types]\n",
    "\n",
    "            word_pattern_df['WordPatternPercentTotal'] = [float(f\"{(word_pattern_tokens.count(word_pattern_type) / word_pattern_tokens_count)*100:.2f}\") \n",
    "                                                          for word_pattern_type in word_pattern_types]                                                                         \n",
    "\n",
    "\n",
    "\n",
    "            word_pattern_token_files = []\n",
    "\n",
    "            for row in patterns_df.itertuples():\n",
    "                \n",
    "                word_pattern_token_files.append((row.WordPattern.lower().strip().translate(str.maketrans(\"\", \"\", punctuation_modified)), row.File))\n",
    "\n",
    "            word_pattern_type_files = sorted(set(word_pattern_token_files))\n",
    "\n",
    "            word_pattern_type_files_dict = {}\n",
    "\n",
    "            for word_pattern_type_file in word_pattern_type_files: \n",
    "\n",
    "                if word_pattern_type_file[0] in word_pattern_type_files_dict:\n",
    "                    \n",
    "                    word_pattern_type_files_dict[word_pattern_type_file[0]].append(word_pattern_type_file[1])\n",
    "\n",
    "                else:\n",
    "                    word_pattern_type_files_dict[word_pattern_type_file[0]] = [word_pattern_type_file[1]]\n",
    "\n",
    "\n",
    "            word_pattern_df['WordPatternFileRawCount'] = [len(filename_list) for word_pattern_type, filename_list in word_pattern_type_files_dict.items()]\n",
    "\n",
    "            word_pattern_df['WordPatternFilePercentTotal'] = [float(f\"{(len(filename_list)/corpus_file_count_total)*100:.2f}\") \n",
    "                                                                    for word_pattern_type, filename_list in word_pattern_type_files_dict.items()]\n",
    "\n",
    "            word_pattern_df['WordPatternFileList'] = [filename_list for word_pattern_type, filename_list in word_pattern_type_files_dict.items()]\n",
    "\n",
    "            word_pattern_df = word_pattern_df.sort_values(by='WordPatternNormalizedCount', ascending=False)                                                                         \n",
    "            \n",
    "            \n",
    "            if search_word_string == \"ain\\'t\":\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                word_pattern_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_wordPatterns.csv\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                word_pattern_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_wordPatterns.csv\")\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            ####################\n",
    "\n",
    "\n",
    "            # the following code follows the exact same process as for word tokens and types\n",
    "            #  and applies it to part of speech patterns. these are the collocations of subject-predicate\n",
    "            #  part of speech token/types that co-occur with the feature\n",
    "            #  please see documentation for corresponding lines above\n",
    "\n",
    "\n",
    "            POS_pattern_tokens = sorted([POS_pattern for POS_pattern in list(patterns_df['POSPattern'])])\n",
    "\n",
    "            POS_pattern_types = sorted(set(POS_pattern_tokens))\n",
    "\n",
    "            POS_pattern_tokens_count = len(POS_pattern_tokens)\n",
    "\n",
    "            POS_pattern_types_count = len(POS_pattern_types)\n",
    "\n",
    "\n",
    "\n",
    "            POS_pattern_df = pd.DataFrame(index=POS_pattern_types)\n",
    "\n",
    "            POS_pattern_df['POSPatternRawCount'] = [POS_pattern_tokens.count(POS_pattern_type) for POS_pattern_type in POS_pattern_types]\n",
    "\n",
    "            POS_pattern_df['POSPatternNormalizedCount'] = [float(f\"{(POS_pattern_tokens.count(POS_pattern_type)/corpus_word_count_total)*100000:.3f}\") \n",
    "                                                           for POS_pattern_type in POS_pattern_types]\n",
    "\n",
    "            POS_pattern_df['POSPatternPercentTotal'] = [float(f\"{(POS_pattern_tokens.count(POS_pattern_type) / POS_pattern_tokens_count)*100:.2f}\") \n",
    "                                                        for POS_pattern_type in POS_pattern_types]\n",
    "\n",
    "\n",
    "\n",
    "            POS_pattern_token_files = []\n",
    "\n",
    "            for row in patterns_df.itertuples():\n",
    "                \n",
    "                POS_pattern_token_files.append((row.POSPattern, row.File))\n",
    "\n",
    "            POS_pattern_type_files = sorted(set(POS_pattern_token_files))\n",
    "\n",
    "            POS_pattern_type_files_dict = {}\n",
    "\n",
    "            for POS_type_file in POS_pattern_type_files: \n",
    "\n",
    "                if POS_type_file[0] in POS_pattern_type_files_dict:\n",
    "                    \n",
    "                    POS_pattern_type_files_dict[POS_type_file[0]].append(POS_type_file[1])\n",
    "\n",
    "                else:\n",
    "                    POS_pattern_type_files_dict[POS_type_file[0]] = [POS_type_file[1]]\n",
    "\n",
    "\n",
    "            POS_pattern_df['POSPatternFileRawCount'] = [len(filename_list) for POS_pattern_type, filename_list in POS_pattern_type_files_dict.items()]\n",
    "\n",
    "            POS_pattern_df['POSPatternFilePercentTotal'] = [float(f\"{(len(filename_list)/corpus_file_count_total)*100:.2f}\") \n",
    "                                                                       for POS_pattern_type, filename_list in POS_pattern_type_files_dict.items()]\n",
    "\n",
    "            POS_pattern_df['POSPatternFileList'] = [filename_list for POS_pattern_type, filename_list in POS_pattern_type_files_dict.items()]\n",
    "\n",
    "            POS_pattern_df = POS_pattern_df.sort_values(by='POSPatternNormalizedCount', ascending=False)\n",
    "\n",
    "            \n",
    "            if search_word_string == \"ain\\'t\":\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                POS_pattern_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_POSPatterns.csv\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # exports the dataframe to a csv file\n",
    "                POS_pattern_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_POSPatterns.csv\")\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            ####################\n",
    "\n",
    "\n",
    "\n",
    "            # creates a list of all possible part of speech patterns based on\n",
    "            #  the part of speech patterns that occur within the corpus\n",
    "            #  utilizes the subject_POS_types and predicate_POS_types lists from above                                                                        \n",
    "            possible_POS_pattern_types = [f\"{subject_POS_type}_{search_word_string}_{predicate_POS_type}\" \n",
    "                                          for subject_POS_type in subject_POS_types \n",
    "                                          for predicate_POS_type in predicate_POS_types]\n",
    "\n",
    "            # removes part of speech patterns that include disfluencies\n",
    "            remove_possible_disfluencies = [POS_pattern for POS_pattern in possible_POS_pattern_types \n",
    "                                            if 'Disfluency' not in POS_pattern]\n",
    "\n",
    "            # removes part of speech patterns that include constituent boundaries\n",
    "            remove_possible_constituent_boundaries = [POS_pattern for POS_pattern in remove_possible_disfluencies \n",
    "                                                     if 'Constituent' not in POS_pattern]\n",
    "\n",
    "            # reassigns to new variable name for user usability\n",
    "            possible_POS_pattern_types = remove_possible_constituent_boundaries\n",
    "\n",
    "            # appends items in the list of possible part of speech to the total list way back in the beginning\n",
    "            for possible_POS_pattern_type in possible_POS_pattern_types:\n",
    "                \n",
    "                total_possible_POS_patterns.append(possible_POS_pattern_type)\n",
    "                                                                                 \n",
    "           \n",
    "                                                                                     \n",
    "        ####################   \n",
    "                                                                                     \n",
    "                  \n",
    "                                                                                     \n",
    "        # creates a type list of possible part of speech from the total list                                                                            \n",
    "        total_possible_POS_pattern_types = sorted(set(total_possible_POS_patterns))\n",
    "                                        \n",
    "        # calculates the total number of possible part of speech patterns based on\n",
    "        #  all part of speech patterns that occur across all corpora\n",
    "        possible_POS_patterns_count = len(total_possible_POS_pattern_types)\n",
    "        \n",
    "        # loops through the filepath and corpus name tuples list\n",
    "        for file_path, corpus_name in filePath_corpusName:\n",
    "            \n",
    "            # creates a dataframe from the split content csv for the corpus\n",
    "            patterns_df = pd.read_csv(file_path)\n",
    "\n",
    "            # if the feature count in a corpus is zero, skips the corpus\n",
    "            if len(patterns_df) == 0:\n",
    "                \n",
    "                continue\n",
    "\n",
    "            else:\n",
    "        \n",
    "                #  adds the list of possible part of speech patterns to the dataframe\n",
    "                all_corpora_to_append_df.at['PossiblePOSPatternList', corpus_name] = total_possible_POS_pattern_types\n",
    "                \n",
    "                #  adds the total count of possible part of speech patterns to the dataframe\n",
    "                all_corpora_to_append_df.at['PossiblePOSPatternCount', corpus_name] = possible_POS_patterns_count\n",
    "                                                                                     \n",
    "                   \n",
    "                                                                                     \n",
    "        ####################   \n",
    "                                                                                     \n",
    "                  \n",
    "                                                                                     \n",
    "        # loops through the filepath and corpus name tuples list\n",
    "        for file_path, corpus_name in filePath_corpusName:\n",
    "\n",
    "            # creates a dataframe from the split content csv for the corpus\n",
    "            patterns_df = pd.read_csv(file_path)\n",
    "\n",
    "            # if the feature count in a corpus is zero, skips the corpus\n",
    "            if len(patterns_df) == 0:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "            \n",
    "                # creates a list of part of speech pattern tokens                                                                      \n",
    "                POS_pattern_tokens = sorted([POS_pattern for POS_pattern in list(patterns_df['POSPattern'])])\n",
    "                \n",
    "                # creates a list of part of speech pattern types \n",
    "                POS_pattern_types = sorted(set(POS_pattern_tokens))\n",
    "\n",
    "                # creates a list of actually occuring part of speech patterns in the corpus\n",
    "                # utilizes the POS_pattern_types list\n",
    "                # removes part of speech patterns that include disfluencies\n",
    "                remove_occuring_disfluencies = [POS_pattern for POS_pattern in POS_pattern_types \n",
    "                                                if 'Disfluency' not in POS_pattern]\n",
    "\n",
    "                # removes part of speech patterns that include constituent boundaries\n",
    "                remove_occuring_constituent_boundaries = [POS_pattern for POS_pattern in remove_occuring_disfluencies \n",
    "                                                         if 'Constituent' not in POS_pattern]\n",
    "\n",
    "                # reassigns to new variable name for user usability\n",
    "                occuring_POS_patterns = remove_occuring_constituent_boundaries\n",
    "                                                                                     \n",
    "                # calculates the total number of possible part of speech patterns\n",
    "                occuring_POS_patterns_count = len(occuring_POS_patterns)\n",
    "                                                                                     \n",
    "                # adds the total number of actually occuring part of speech patterns in the corpus to the dataframe\n",
    "                all_corpora_to_append_df.loc['OccuringPOSPatternCount', corpus_name] = occuring_POS_patterns_count\n",
    "\n",
    "                                                                                     \n",
    "                # calculates the percent of the total number of possible part of speech patterns actually occur\n",
    "                #  adds the percentage to the dataframe\n",
    "                all_corpora_to_append_df.loc['PercentTotalPOSPatternOccuringCount', corpus_name] = float(\n",
    "                    f\"{occuring_POS_patterns_count/possible_POS_patterns_count*100:.2f}\")\n",
    "               \n",
    "                # adds the list of actually occuring part of speech patterns in the corpus to the dataframe\n",
    "                all_corpora_to_append_df.loc['OccurringPOSPatternList', corpus_name] = occuring_POS_patterns                                                                     \n",
    "        \n",
    "                                                                                     \n",
    "    ####################\n",
    "                                                                                     \n",
    "    # combines the original all_corpora_info and the new all_corpora_to_append_df                                                                                \n",
    "    complete_all_corpora_df = all_corpora_info_df.append(all_corpora_to_append_df)\n",
    "    \n",
    "    if search_word_string == \"ain\\'t\":\n",
    "        \n",
    "        # exports the dataframe to a csv file\n",
    "        complete_all_corpora_df.to_csv(f\"{csv_output_path}/aint_complete_all_corpora_info.csv\")\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        # exports the dataframe to a csv file\n",
    "        complete_all_corpora_df.to_csv(f\"{csv_output_path}/{search_word_string}_complete_all_corpora_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbb138-16c5-4bad-a6ad-f0ec2e055273",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating Quantitative  Dataframes and Exporting Dataframes to CSV Files\n",
    "\n",
    "This will execute the code and create the dataframes and then export them as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e2ae5-d92e-4e79-82fb-7dc85cf7653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the input path where the gold standard CSVs are stored\n",
    "csv_input_path = \"path\"\n",
    "\n",
    "# Designate the output path where the gold standard CSVs are stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33576a-bae7-4657-8742-eb0257a60356",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e6259-1812-4288-8d1e-054568c3945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"ain\\'t\"\n",
    "\n",
    "# execute code\n",
    "get_corpora_stats_csvs(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1162a8-d9de-492d-9a12-022716aca883",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596c701-bbe7-4f9e-8ca3-b7d912d74d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"be\"\n",
    "\n",
    "# execute code\n",
    "get_corpora_stats_csvs(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280668e0-81e0-42a9-986d-b2f35fc0f044",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e660b0d-27a7-4364-9d93-37acfc4335f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"done\"\n",
    "\n",
    "# execute code\n",
    "get_corpora_stats_csvs(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b7c27-06f4-4143-8789-438214d1608b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
