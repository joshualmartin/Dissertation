{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e3259c-5bf5-416d-bf60-042ca11345fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 1.1: Creating Dataframes from Corpora\n",
    "\n",
    "This code will produce the following pandas dataframes:\n",
    "<ol>\n",
    "<li>Dataframes for each corpus that contain transcript lines that contain the search word</li>\n",
    "<li>Dataframes that provide quantitative information from each corpus</li>\n",
    "<li>A dataframe that combines quantitative information from all corpora into one dataframe</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79389727-e640-4969-8246-ba7a20a587aa",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code:\n",
    "string, os, re, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/), [nltk](https://pypi.org/project/nltk/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d79f0-9e5e-48ea-a647-8f547409dff5",
   "metadata": {},
   "source": [
    "## Designate Corpora Filepaths\n",
    "\n",
    "Ensure each path folder contains all the text files for each corpus. Be sure to end file paths with the appropriate slash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e446f5-bbd9-4aea-a204-c420f1575976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the file path where you keep all the corpora sub-folders\n",
    "corpora_path = \"path\"\n",
    "\n",
    "#This is the file path extension for the corpus specific sub-folders\n",
    "coraal_extension = \"CORAAL/\"\n",
    "fisher_extension = \"Fisher/\"\n",
    "librispeech_extension = \"LibriSpeech/\"\n",
    "switchboard_extension = \"SwitchboardHub5/Switchboard/\"\n",
    "hub5_extension = \"SwitchboardHub5/hub5_noHeader/\"\n",
    "timit_extension = \"TIMIT/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7780ca6-6a0d-45b7-a553-d979c614e152",
   "metadata": {},
   "source": [
    "## Define the Dataframe Creating Function\n",
    "\n",
    "This function takes the following arguments:\n",
    "\n",
    "<ol>\n",
    "<li>The filepath to set of txt files from a corpus</li>\n",
    "<li>The word being searched for</li>\n",
    "<li>The name of the corpus taken from the following names: CORAAL, Switchboard, Hub5, Fisher, LibriSpeech, and TIMIT </li>\n",
    "</ol>\n",
    "\n",
    "It will return two pandas dataframes. One will contain all transcript lines in the corpus which contain the search word. The second will contain information about the corpus, including:\n",
    "\n",
    "<ol>\n",
    "<li>The total word count</li>\n",
    "<li>The total number of instances of the search word</li>\n",
    "<li>The normalized number of search word instances per 100,000 words </li>\n",
    "<li>The total number of files in the corpus </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5805957-3d67-4157-bc0b-373e79233ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instances_info_dataframes(txt_import_path, search_word_string, corpus_name):\n",
    "    \"\"\"\n",
    "    Takes a set of txt files from a corpus, a search word, and a corpus name.\n",
    "    Returns two Pandas dataframes. One will contain all transcript lines in the corpus\n",
    "    which contain the search word. The second will contain information about the corpus, including:\n",
    "    total word count, total number of instances of the search word, the normalized number of\n",
    "    search word instances per 100,000 words, and the total number of files in the corpus.\n",
    "    There are six options for corpora: CORAAL, Switchboard, Hub5, Fisher, LibriSpeech, and TIMIT.\n",
    "    One of these must be given in the corpus_name argument or the code will ask you to correct it.\n",
    "    \"\"\"\n",
    "\n",
    "#####################################################################################################\n",
    "############################### SECTION 1: PRELIMINARY ACTIONS ######################################\n",
    "#####################################################################################################\n",
    "\n",
    "    if corpus_name not in ['CORAAL', 'Switchboard', 'Hub5', 'Fisher', 'LibriSpeech', 'Librispeech', 'TIMIT',\n",
    "                           'corral', 'switchboard', 'hub5', 'fisher', 'librispeech', 'timit']:\n",
    "        \n",
    "        raise Exception(\"\"\"The corpus name you gave is not valid. Please use one of the following: CORAAL, Switchboard, Hub5, Fisher, LibriSpeech, or TIMIT.\"\"\")\n",
    "\n",
    "    import pandas as pd\n",
    "    import string\n",
    "    import os\n",
    "    import re\n",
    "    from nltk import word_tokenize\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "############################### SECTION 2: INTERNAL FUNCTIONS #######################################\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "    def create_coraal_df(transcript_filepath, transcript_filename):\n",
    "        \"\"\"Takes a txt file transcript from the CORAAL corpus and creates a cleaned Pandas dataframe.\"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        # reads in csv and creates Pandas dataframe\n",
    "        coraal_df = pd.read_csv(\n",
    "            f\"{transcript_filepath}{transcript_filename}\", sep=\"\\t\")\n",
    "\n",
    "        # creates a 'File' column\n",
    "        coraal_df['File'] = transcript_filename[:-4]\n",
    "\n",
    "        # renames column names for consistency with other corpora dataframes\n",
    "        coraal_df = coraal_df.rename(\n",
    "            columns={\"Spkr\": \"Speaker\", \"StTime\": \"UttStartTime\", \"EnTime\": \"UttEndTime\"})\n",
    "        \n",
    "        # creates a column with the utterance audio length in seconds\n",
    "        #  this will be used later to calculate speech rate\n",
    "        coraal_df['UttLength'] = coraal_df['UttEndTime'] - coraal_df['UttStartTime']\n",
    "        \n",
    "        # drops the 'all_info' column from the dataframe\n",
    "        coraal_df = coraal_df[['File', 'Line', 'Speaker', 'UttStartTime', \n",
    "                               'UttEndTime', 'UttLength', 'Content']]\n",
    "\n",
    "        # removes all metalinguistic and paralinguistic information except for pauses\n",
    "        coraal_df = coraal_df.replace(\n",
    "            to_replace='\\/(\\?+|[Uu]nintelligible)\\/|\\/|\\[|\\]|\\<.*?\\>|\\((?!pause).*?\\)', value='', regex=True)\n",
    "        \n",
    "        #this will remove all White interviewers from the dataframe\n",
    "        white_interviewer_ids = ['DCA_int_01', 'DCA_int_02', 'DCA_int_04', 'DCA_int_07','DCA_int_08','LES_int_01','PRV_int_01','PRV_int_02']\n",
    "        \n",
    "        coraal_df = coraal_df[~coraal_df['Speaker'].isin(white_interviewer_ids)]\n",
    "        \n",
    "        # returns the dataframe\n",
    "        return coraal_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_switchboard_df(transcript_filepath, transcript_filename):\n",
    "        \"\"\"Takes a txt file transcript from the Switchboard corpus and creates a cleaned Pandas dataframe.\"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        # reads in csv and creates Pandas dataframe\n",
    "        swb_df = pd.read_csv(f\"{transcript_filepath}{transcript_filename}\",\n",
    "                             sep=\"\\t\", header=None, names=['all_info'])\n",
    "\n",
    "        # replaces the first three spaces in each transcript line with tab characters for separation purposes\n",
    "        swb_df = pd.DataFrame([row.all_info.replace(\" \", \"\\t\", 3)\n",
    "                               for row in swb_df.itertuples()], columns=['all_info'])\n",
    "\n",
    "        # splits each row into multiple columns, breaking on tab characters\n",
    "        swb_df = swb_df.join(swb_df['all_info'].str.split('\\t', expand=True).rename(\n",
    "            columns={0: 'File-Line', 1: 'UttStartTime', 2: 'UttEndTime', 3: 'Content'}))\n",
    "        \n",
    "        # converts the start and end times from a string format to a float format\n",
    "        #  python can't perform mathematical functions on strings\n",
    "        swb_df['UttStartTime'] = swb_df['UttStartTime'].astype(float)\n",
    "        swb_df['UttEndTime'] = swb_df['UttEndTime'].astype(float)\n",
    "\n",
    "        # splits the file-line combination that is built into Switchboard transcript lines\n",
    "        # creates a 'File' column\n",
    "        swb_df['File'] = swb_df['File-Line'].str.slice(0, 14)\n",
    "\n",
    "        # creates a 'Line column'\n",
    "        swb_df['Line'] = swb_df['File-Line'].str.slice(-4,)\n",
    "\n",
    "        # gets the speaker id from the filename\n",
    "        swb_df['Speaker'] = transcript_filename[6]\n",
    "\n",
    "        # creates a column with the utterance audio length in seconds\n",
    "        #  this will be used later to calculate speech rate\n",
    "        swb_df['UttLength'] = swb_df['UttEndTime'] - swb_df['UttStartTime']\n",
    "        \n",
    "        # drops the 'all_info' column from the dataframe\n",
    "        swb_df = swb_df[['File', 'Line', 'Speaker', 'UttStartTime', \n",
    "                         'UttEndTime', 'UttLength', 'Content']]\n",
    "\n",
    "        # removes all metalinguistic and paralinguistic information, except for silence markers since those compose half the lines in Switchboard\n",
    "        #  Switchboard is a phone conversation corpus and they split the transcript into each caller's audio instead of combining each side\n",
    "        #  that means, when the speaker in the transcript is not talking, the line will just be silence\n",
    "        swb_df = swb_df.replace(to_replace='\\[(?!silence).*?\\]|\\<.*?\\>', value='', regex=True)\n",
    "\n",
    "        # returns the dataframe\n",
    "        return swb_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_hub5_df(transcript_filepath, transcript_filename):\n",
    "        \"\"\"Takes a txt file transcript from the Hub5 corpus and creates a cleaned Pandas dataframe.\"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        # reads in csv and creates Pandas dataframe\n",
    "        hub5_df = pd.read_csv(f\"{transcript_filepath}{transcript_filename}\",\n",
    "                              sep=\"\\t\", header=None, names=['all_info'])\n",
    "\n",
    "        # replaces the first three spaces in each transcript line with tab characters for separation purposes\n",
    "        hub5_df = pd.DataFrame([row.all_info.replace(\" \", \"\\t\", 3)\n",
    "                                for row in hub5_df.itertuples()], columns=['all_info'])\n",
    "\n",
    "        # splits each row into multiple columns, breaking on tab characters\n",
    "        hub5_df = hub5_df.join(hub5_df['all_info'].str.split('\\t', expand=True).rename(\n",
    "            columns={0: 'UttStartTime', 1: 'UttEndTime', 2: 'Speaker', 3: 'Content'}))\n",
    "        \n",
    "        # converts the start and end times from a string format to a float format\n",
    "        #  python can't perform mathematical functions on strings\n",
    "        hub5_df['UttStartTime'] = hub5_df['UttStartTime'].astype(float)\n",
    "        hub5_df['UttEndTime'] = hub5_df['UttEndTime'].astype(float)\n",
    "        \n",
    "        # creates a 'File' column\n",
    "        hub5_df['File'] = hub5_df['File'] = transcript_filename[:-4]\n",
    "\n",
    "        # creates a 'Line column'\n",
    "        # NOTE: The Hub5 transcripts DO NOT have line numbers. The numbers added here are based on Pandas row numbers\n",
    "        hub5_df['Line'] = np.arange(hub5_df.shape[0])\n",
    "        hub5_df['Line'] = hub5_df['Line'] + 1\n",
    "\n",
    "        #removes the colon from the speaker id\n",
    "        hub5_df['Speaker'] = hub5_df['Speaker'].apply(lambda speaker_id: speaker_id.strip(string.punctuation))\n",
    "        \n",
    "        # creates a column with the utterance audio length in seconds\n",
    "        #  this will be used later to calculate speech rate\n",
    "        hub5_df['UttLength'] = hub5_df['UttEndTime'] - hub5_df['UttStartTime']\n",
    "        \n",
    "        # drops the 'all_info' column from the dataframe\n",
    "        hub5_df = hub5_df[['File', 'Line', 'Speaker', 'UttStartTime', \n",
    "                           'UttEndTime', 'UttLength', 'Content']]\n",
    "\n",
    "        # removes all metalinguistic and paralinguistic information\n",
    "        hub5_df = hub5_df.replace(\n",
    "            to_replace='\\<.*?\\[.*?\\]\\[.*?\\].*?\\>|\\{.*?\\}|\\%|\\&|\\*{2}.*?\\*{2}|\\[{2}.*?\\]{2}|\\({2}|\\){2}', value='', regex=True)\n",
    "        hub5_df = hub5_df.replace(\n",
    "            to_replace='\\<.*?\\>|\\[.*?\\]', value='', regex=True)\n",
    "\n",
    "        # returns the dataframe\n",
    "        return hub5_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_fisher_df(transcript_filepath, transcript_filename):\n",
    "        \"\"\"Takes a txt file transcript from the Fisher corpus and creates a cleaned Pandas dataframe.\"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        # reads in csv and creates Pandas dataframe\n",
    "        fisher_df = pd.read_csv(f\"{transcript_filepath}{transcript_filename}\", header=None, names=[\n",
    "                                'all_info'], sep=\"\\t\", skiprows=2)\n",
    "\n",
    "        # replaces the first three spaces in each transcript line with tab characters for separation purposes\n",
    "        fisher_df = pd.DataFrame([row.all_info.replace(\" \", \"\\t\", 3)\n",
    "                                  for row in fisher_df.itertuples()], columns=['all_info'])\n",
    "\n",
    "        # splits each row into multiple columns, breaking on tab characters\n",
    "        fisher_df = fisher_df.join(fisher_df['all_info'].str.split('\\t', expand=True).rename(\n",
    "            columns={0: 'UttStartTime', 1: 'UttEndTime', 2: 'Speaker', 3: 'Content'}))\n",
    "        \n",
    "        # converts the start and end times from a string format to a float format\n",
    "        #  python can't perform mathematical functions on strings\n",
    "        fisher_df['UttStartTime'] = fisher_df['UttStartTime'].astype(float)\n",
    "        fisher_df['UttEndTime'] = fisher_df['UttEndTime'].astype(float)\n",
    "        \n",
    "        # creates a 'File' column\n",
    "        fisher_df['File'] = fisher_df['File'] = transcript_filename[:-4]\n",
    "\n",
    "        # creates a 'Line column'\n",
    "        # NOTE: The Fisher transcripts DO NOT have line numbers. The numbers added here are based on Pandas row numbers\n",
    "        fisher_df['Line'] = np.arange(fisher_df.shape[0])\n",
    "        fisher_df['Line'] = fisher_df['Line'] + 1\n",
    "\n",
    "        #removes the colon from the speaker id\n",
    "        fisher_df['Speaker'] = fisher_df['Speaker'].apply(lambda speaker_id: speaker_id.strip(string.punctuation))\n",
    "\n",
    "        # creates a column with the utterance audio length in seconds\n",
    "        #  this will be used later to calculate speech rate\n",
    "        fisher_df['UttLength'] = fisher_df['UttEndTime'] - fisher_df['UttStartTime']\n",
    "        \n",
    "        # drops the 'all_info' column from the dataframe\n",
    "        fisher_df = fisher_df[['File', 'Line', 'Speaker', 'UttStartTime', \n",
    "                               'UttEndTime', 'UttLength', 'Content']]\n",
    "\n",
    "        # removes all metalinguistic and paralinguistic information\n",
    "        fisher_df = fisher_df.replace(\n",
    "            to_replace='\\_|\\({2}|\\){2}|\\s{2}|\\[.*?\\]', value='', regex=True)\n",
    "\n",
    "        # returns the dataframe\n",
    "        return fisher_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_librispeech_df(transcript_filepath, transcript_filename):\n",
    "        \"\"\"Takes a txt file transcript from the LibriSpeech corpus and creates a cleaned Pandas dataframe.\"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        # reads in csv and creates Pandas dataframe\n",
    "        librispeech_df = pd.read_csv(\n",
    "            f\"{transcript_filepath}{transcript_filename}\", sep=\"\\t\", header=None, names=['all_info'])\n",
    "\n",
    "        # replaces the first space in each transcript line with tab characters for separation purposes\n",
    "        librispeech_df = pd.DataFrame([row.all_info.replace(\n",
    "            \" \", \"\\t\", 1) for row in librispeech_df.itertuples()], columns=['all_info'])\n",
    "\n",
    "        # splits each row into multiple columns, breaking on tab characters\n",
    "        librispeech_df = librispeech_df.join(librispeech_df['all_info'].str.split(\n",
    "            '\\t', expand=True).rename(columns={0: 'File-Line', 1: 'Content'}))\n",
    "\n",
    "        # splits the file-line combination that is built into Switchboard transcript lines\n",
    "        # creates a 'File' column\n",
    "        librispeech_df['File'] = librispeech_df['File-Line'].str.slice(0, 16)\n",
    "\n",
    "        # creates a 'Line column'\n",
    "        librispeech_df['Line'] = librispeech_df['File-Line'].str.slice(-4,)\n",
    "\n",
    "        # drops the 'all_info' column from the dataframe\n",
    "        librispeech_df = librispeech_df[['File', 'Line', 'Content']]\n",
    "\n",
    "        # lowercases all letters in Content. LibriSpeech transcripts are all uppercase with no punctuation\n",
    "        librispeech_df['Content'] = librispeech_df['Content'].str.lower()\n",
    "\n",
    "        # returns the dataframe\n",
    "        return librispeech_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_timit_df(transcript_filepath, transcript_filename):\n",
    "        \"\"\"Takes a txt file transcript from the TIMIT corpus and creates a cleaned Pandas dataframe.\"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        # reads in csv and creates Pandas dataframe\n",
    "        timit_df = pd.read_csv(f\"{transcript_filepath}{transcript_filename}\",\n",
    "                               header=None, sep=\"\\t\", names=['all_info'])\n",
    "\n",
    "        # replaces the first two spaces in each transcript line with tab characters for separation purposes\n",
    "        timit_df = pd.DataFrame([row.all_info.replace(\" \", \"\\t\", 2)\n",
    "                                 for row in timit_df.itertuples()], columns=['all_info'])\n",
    "\n",
    "        # splits each row into multiple columns, breaking on tab characters\n",
    "        timit_df = timit_df.join(timit_df['all_info'].str.split('\\t', expand=True).rename(\n",
    "            columns={0: 'BeginningIntegerSampleNumber', 1: 'EndIntegerSampleNumber', 2: 'Content'}))\n",
    "\n",
    "        # splits the file-line combination that is built into Switchboard transcript lines\n",
    "        # creates a 'File' column\n",
    "        timit_df['File'] = transcript_filename[:-4]\n",
    "\n",
    "        # drops the 'all_info' column from the dataframe\n",
    "        timit_df = timit_df[['File', 'BeginningIntegerSampleNumber',\n",
    "                             'EndIntegerSampleNumber', 'Content']]\n",
    "\n",
    "        # returns the dataframe\n",
    "        return timit_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def filter_df_by_word(dataframe, search_word_string):\n",
    "        \"\"\"\n",
    "        Takes a word and filters a Pandas dataframe and leaves only rows \n",
    "        that contain that search word in its Content.\n",
    "        \"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        # takes the search word input and transforms it into a regular expression that will search for only whole words\n",
    "        #  if the sequence of strings submitted is contained within a larger word, this will filter those instances out\n",
    "        #  and leave only whole matches\n",
    "        search_word_regex = f\"\\\\b[{search_word_string[0].upper()}|{search_word_string[0].lower()}]{search_word_string[1:]}\\\\b\"\n",
    "\n",
    "        # filters the dataframe by rows whose 'Content' contains the word\n",
    "        word_df = dataframe[dataframe['Content'].str.contains(\n",
    "            search_word_regex, case=False, flags=re.IGNORECASE, regex=True)]\n",
    "\n",
    "        #creates an empty list for number of instances of search word to be appended to\n",
    "        instances_count_per_line = []\n",
    "        \n",
    "        #loops through rows\n",
    "        for row in word_df.itertuples():\n",
    "            \n",
    "            #appends number of search word instances per row\n",
    "            instances_count_per_line.append(len(re.findall(search_word_regex, row.Content)))\n",
    "        \n",
    "        #adds a column to the dataframe that has the number of instances per row\n",
    "        word_df[\"InstancesCountPerLine\"] = instances_count_per_line\n",
    "\n",
    "        # returns the dataframe\n",
    "        return word_df\n",
    "\n",
    "#####################################################################################################\n",
    "############################### SECTION 3: EXECUTION OF CODE ######################################\n",
    "#####################################################################################################\n",
    "\n",
    "    # gathers all the filenames for the txt files\n",
    "    txt_filenames = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(txt_import_path):\n",
    "        \n",
    "        for file in files:\n",
    "            \n",
    "            #this is because the 2021 CORAAL files have duplicate versions\n",
    "            #  of files that start with ._ and contain nothing\n",
    "            if file.startswith(\"._\"):\n",
    "                pass\n",
    "            \n",
    "            elif file.endswith(\".txt\") or file.endswith(\".TXT\") or file.endswith(\".text\"):\n",
    "                txt_filenames.append(file)\n",
    "\n",
    "    # creates an empty list for each file's word totals to be appended to\n",
    "    total_word_count_list = []\n",
    "\n",
    "    # creates an empty list for each dataframe of lines containing instances of the search word\n",
    "    #  to be appended to\n",
    "    instances_list = []\n",
    "\n",
    "    # creates a dataframe for the transcript\n",
    "    #  uses the correct corpus based on the corpus_name given by the user\n",
    "    for txt_filename in txt_filenames:\n",
    "        \n",
    "        if corpus_name.lower() == \"coraal\":\n",
    "            \n",
    "            file_df = create_coraal_df(txt_import_path, txt_filename)\n",
    "            \n",
    "        elif corpus_name.lower() == \"switchboard\":\n",
    "            \n",
    "            file_df = create_switchboard_df(txt_import_path, txt_filename)\n",
    "            \n",
    "        elif corpus_name.lower() == \"hub5\":\n",
    "            \n",
    "            file_df = create_hub5_df(txt_import_path, txt_filename)\n",
    "            \n",
    "        elif corpus_name.lower() == \"fisher\":\n",
    "            \n",
    "            file_df = create_fisher_df(txt_import_path, txt_filename)\n",
    "            \n",
    "        elif corpus_name.lower() == \"librispeech\":\n",
    "            \n",
    "            file_df = create_librispeech_df(txt_import_path, txt_filename)\n",
    "            \n",
    "        elif corpus_name.lower() == \"timit\":\n",
    "            \n",
    "            file_df = create_timit_df(txt_import_path, txt_filename)\n",
    "            \n",
    "\n",
    "        # creates an empty list for each line's total word count to be appended to\n",
    "        file_total_word_count = []\n",
    "\n",
    "        # calculates the total number of words in each line's Content\n",
    "        for file_row in file_df.itertuples():\n",
    "            \n",
    "            # ensures CORAAL rows composed only of pauses won't be counted\n",
    "            if corpus_name.lower() == \"coraal\":\n",
    "                \n",
    "                if file_row.Content.startswith(\"(pause\"):\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    #tokenizes words in Content using nltk's word tokenizer\n",
    "                    # important to note that this will divide contracted words into two\n",
    "                    content_words_tokenized = word_tokenize(file_row.Content)\n",
    "                    \n",
    "                    #cleans words by: (1) lowercasing, stripping punctuation to the right\n",
    "                    # so that the apostrophe in contacted words can be preserved\n",
    "                    # (2) eliminates tokens which are only punctuation markers\n",
    "                    # since nltk separates those out\n",
    "                    content_words_tokenized_cleaned = [word.lower().rstrip(string.punctuation) for word in content_words_tokenized if word not in string.punctuation]\n",
    "                    \n",
    "                    # appends the len of the Content word list to the file total count list\n",
    "                    file_total_word_count.append(len(content_words_tokenized_cleaned))\n",
    "\n",
    "            # ensures Switchboard rows composed only of silence won't be counted\n",
    "            elif corpus_name.lower() == \"switchboard\":\n",
    "                \n",
    "                if file_row.Content.startswith(\"[silence\"):\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    #tokenizes words in Content using nltk's word tokenizer\n",
    "                    # important to note that this will divide contracted words into two\n",
    "                    content_words_tokenized = word_tokenize(file_row.Content)\n",
    "                   \n",
    "                    #cleans words by: (1) lowercasing, stripping punctuation to the right\n",
    "                    # so that the apostrophe in contacted words can be preserved\n",
    "                    # (2) eliminates tokens which are only punctuation markers\n",
    "                    # since nltk separates those out\n",
    "                    content_words_tokenized_cleaned = [word.lower().rstrip(string.punctuation) for word in content_words_tokenized if word not in string.punctuation]\n",
    "                    \n",
    "                    # appends the len of the Content word list to the file total count list\n",
    "                    file_total_word_count.append(len(content_words_tokenized_cleaned))\n",
    "\n",
    "\n",
    "            # for all other corpora\n",
    "            else:\n",
    "                #tokenizes words in Content using nltk's word tokenizer\n",
    "                #important to note that this will divide contracted words into two\n",
    "                content_words_tokenized = word_tokenize(file_row.Content)\n",
    "                \n",
    "                #cleans words by: (1) lowercasing, stripping punctuation to the right\n",
    "                # so that the apostrophe in contacted words can be preserved\n",
    "                # (2) eliminates tokens which are only punctuation markers\n",
    "                # since nltk separates those out\n",
    "                content_words_tokenized_cleaned = [word.lower().rstrip(string.punctuation) for word in content_words_tokenized if word not in string.punctuation]\n",
    "                \n",
    "                # appends the len of the Content word list to the file total count list\n",
    "                file_total_word_count.append(len(content_words_tokenized_cleaned))\n",
    "\n",
    "\n",
    "        # appends the sum of the file's lines' Content count\n",
    "        total_word_count_list.append(sum(file_total_word_count))\n",
    "\n",
    "        # appends the dataframe of instances found within the file\n",
    "        instances_list.append(filter_df_by_word(file_df, search_word_string))\n",
    "\n",
    "    # calculates the total word count for the corpus by summing the word count totals for every file\n",
    "    total_word_count = sum(total_word_count_list)\n",
    "\n",
    "    # calculates the total number of files by the length of the filenames list\n",
    "    total_file_count = len(txt_filenames)\n",
    "\n",
    "    # concatenates the list of instance dataframes for each file into one dataframe\n",
    "    instances_df = pd.concat(instances_list).reset_index(drop=True)\n",
    "\n",
    "    # calculates the total number of instances of the search word occur in the corpus\n",
    "    #  this may be a different number than the row count in the instances_df\n",
    "    #  that is because there may be more than once instance of the search word\n",
    "    #  in a given Content line\n",
    "    total_instances_count = instances_df[\"InstancesCountPerLine\"].sum()\n",
    "\n",
    "    # calculates the normalized amount of instances of the search word per 100,000\n",
    "    #  the formula is total number of instances / total word count in the corpus * 100,000\n",
    "    normalized_instances_count = total_instances_count / total_word_count * 100000\n",
    "\n",
    "    # creates a dataframe that provides the corpus' total word count, total number of instances of the search word,\n",
    "    #   the normalized number of instances per 100,000, and the total file count\n",
    "    info_df = pd.DataFrame({f'{corpus_name}': [total_word_count,\n",
    "                                               total_instances_count,\n",
    "                                               normalized_instances_count,\n",
    "                                               total_file_count]}, index=['TotalCorpusWordCount',\n",
    "                                                                          'TotalWordInstancesCount',\n",
    "                                                                          'NormalizedWordInstancesCount',\n",
    "                                                                          'TotalFileCount'])\n",
    "    \n",
    "    #Pandas defaults to scientific notation. This will correct that.\n",
    "    info_df = info_df.round(2)\n",
    "\n",
    "    # returns (1) the instances dataframe which will contain full information for lines that contain the search word\n",
    "    #       (2) the informational dataframe created one step before this\n",
    "    #  to make this work, write the code like this:\n",
    "    #  example1, example2 = get_instances_info_dataframes('/dir/ec/tory', 'search_word', 'corpus_name')\n",
    "    #  example1 will contain the instances_df, example 2 will contain the info_df\n",
    "    return instances_df, info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04b619-e015-4952-8f24-8bcf15c2e21f",
   "metadata": {},
   "source": [
    "## Creating Instances and Info Dataframes\n",
    "\n",
    "This will create instances and info dataframes for each corpus. Because the function returns two variables, two variables must be assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ecbe82-a735-4a68-9e52-2341a33338e1",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ddb88-68af-4289-a42d-fda5f9cb3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_coraal_instances_df, aint_coraal_info_df = get_instances_info_dataframes(f\"{corpora_path}{coraal_extension}\", \"ain\\'t\", \"CORAAL\")\n",
    "\n",
    "aint_fisher_instances_df, aint_fisher_info_df = get_instances_info_dataframes(f\"{corpora_path}{fisher_extension}\", \"ain\\'t\", \"Fisher\")\n",
    "\n",
    "aint_librispeech_instances_df, aint_librispeech_info_df = get_instances_info_dataframes(f\"{corpora_path}{librispeech_extension}\", \"ain\\'t\", \"LibriSpeech\")\n",
    "\n",
    "aint_switchboard_instances_df, aint_switchboard_info_df = get_instances_info_dataframes(f\"{corpora_path}{switchboard_extension}\", \"ain\\'t\", \"Switchboard\")\n",
    "\n",
    "aint_hub5_instances_df, aint_hub5_info_df = get_instances_info_dataframes(f\"{corpora_path}{hub5_extension}\", \"ain\\'t\", \"Hub5\")\n",
    "\n",
    "aint_timit_instances_df, aint_timit_info_df = get_instances_info_dataframes(f\"{corpora_path}{timit_extension}\", \"ain\\'t\", \"TIMIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf4b94-9aa1-45af-b906-97e17f4cd74e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669bef4-883f-4d99-bc54-113372927dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_coraal_instances_df, be_coraal_info_df = get_instances_info_dataframes(f\"{corpora_path}{coraal_extension}\", \"be\", \"CORAAL\")\n",
    "\n",
    "be_fisher_instances_df, be_fisher_info_df = get_instances_info_dataframes(f\"{corpora_path}{fisher_extension}\", \"be\", \"Fisher\")\n",
    "\n",
    "be_librispeech_instances_df, be_librispeech_info_df = get_instances_info_dataframes(f\"{corpora_path}{librispeech_extension}\", \"be\", \"LibriSpeech\")\n",
    "\n",
    "be_switchboard_instances_df, be_switchboard_info_df = get_instances_info_dataframes(f\"{corpora_path}{switchboard_extension}\", \"be\", \"Switchboard\")\n",
    "\n",
    "be_hub5_instances_df, be_hub5_info_df = get_instances_info_dataframes(f\"{corpora_path}{hub5_extension}\", \"be\", \"Hub5\")\n",
    "\n",
    "be_timit_instances_df, be_timit_info_df = get_instances_info_dataframes(f\"{corpora_path}{timit_extension}\", \"be\", \"TIMIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e94c64-e526-4061-ace2-28de5966336c",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af49ac2-4caf-4d3b-bc4c-8758b3de2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_coraal_instances_df, done_coraal_info_df = get_instances_info_dataframes(f\"{corpora_path}{coraal_extension}\", \"done\", \"CORAAL\")\n",
    "\n",
    "done_fisher_instances_df, done_fisher_info_df = get_instances_info_dataframes(f\"{corpora_path}{fisher_extension}\", \"done\", \"Fisher\")\n",
    "\n",
    "done_librispeech_instances_df, done_librispeech_info_df = get_instances_info_dataframes(f\"{corpora_path}{librispeech_extension}\", \"done\", \"LibriSpeech\")\n",
    "\n",
    "done_switchboard_instances_df, done_switchboard_info_df = get_instances_info_dataframes(f\"{corpora_path}{switchboard_extension}\", \"done\", \"Switchboard\")\n",
    "\n",
    "done_hub5_instances_df, done_hub5_info_df = get_instances_info_dataframes(f\"{corpora_path}{hub5_extension}\", \"done\", \"Hub5\")\n",
    "\n",
    "done_timit_instances_df, done_timit_info_df = get_instances_info_dataframes(f\"{corpora_path}{timit_extension}\", \"done\", \"TIMIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf2a12-f324-4e39-9598-cc1398e4fb06",
   "metadata": {},
   "source": [
    "## Combining Switchboard and Hub5 Dataframes\n",
    "\n",
    "The Switchboard and Hub5 corpora are often used together in ASR development and evaluation. I analyze them together, so the following code will create combined instances and info dataframes for Switchboard and Hub5. If you would like to analyze them separately, simply use the variables created for each in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea857adf-e875-4a5d-acd9-b9f138192bdc",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6f92a-b9bf-48c7-84b4-0c2798bb7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aint_switchboardHub5_instances_df = pd.concat([aint_switchboard_instances_df, aint_hub5_instances_df])\n",
    "\n",
    "#adds the switchboard and hub5 info dataframes together\n",
    "aint_switchboardHub5_info_df = aint_switchboard_info_df.add(aint_hub5_info_df, fill_value=0)\n",
    "\n",
    "#sums the values in each dataframe and inserts the info in a new column\n",
    "aint_switchboardHub5_info_df[\"SwitchboardHub5\"] = aint_switchboardHub5_info_df.sum(axis=1)\n",
    "\n",
    "#recalculates the normalized instance count based on the new combined figures\n",
    "# if you don't do this, the process will simply sum the two normalized counts\n",
    "# which would be highly inaccurate\n",
    "aint_switchboardHub5_info_df.iloc[2,2] = round(aint_switchboardHub5_info_df.iloc[1,2]/\n",
    "                                          aint_switchboardHub5_info_df.iloc[0,2]\n",
    "                                          *100000, 2)\n",
    "\n",
    "#drops the separate switchboard and hub5 columns and leaves the combined column\n",
    "aint_switchboardHub5_info_df = aint_switchboardHub5_info_df.drop(['Hub5', 'Switchboard'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cec98-8618-459c-a672-e7771983b4b2",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9e3ea-4bee-478b-ae99-f578b69e8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_switchboardHub5_instances_df = pd.concat([be_switchboard_instances_df, be_hub5_instances_df])\n",
    "\n",
    "#adds the switchboard and hub5 info dataframes together\n",
    "be_switchboardHub5_info_df = be_switchboard_info_df.add(be_hub5_info_df, fill_value=0)\n",
    "\n",
    "#sums the values in each dataframe and inserts the info in a new column\n",
    "be_switchboardHub5_info_df[\"SwitchboardHub5\"] = be_switchboardHub5_info_df.sum(axis=1)\n",
    "\n",
    "#recalculates the normalized instance count based on the new combined figures\n",
    "# if you don't do this, the process will simply sum the two normalized counts\n",
    "# which would be highly inaccurate\n",
    "be_switchboardHub5_info_df.iloc[2,2] = round(be_switchboardHub5_info_df.iloc[1,2]/\n",
    "                                          be_switchboardHub5_info_df.iloc[0,2]\n",
    "                                          *100000, 2)\n",
    "\n",
    "#drops the separate switchboard and hub5 columns and leaves the combined column\n",
    "be_switchboardHub5_info_df = be_switchboardHub5_info_df.drop(['Hub5', 'Switchboard'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba41aa9-920f-4456-8586-edfc58e0f073",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ce058-c63f-48db-a745-ddb1d39e42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_switchboardHub5_instances_df = pd.concat([done_switchboard_instances_df, done_hub5_instances_df])\n",
    "\n",
    "#adds the switchboard and hub5 info dataframes together\n",
    "done_switchboardHub5_info_df = done_switchboard_info_df.add(done_hub5_info_df, fill_value=0)\n",
    "\n",
    "#sums the values in each dataframe and inserts the info in a new column\n",
    "done_switchboardHub5_info_df[\"SwitchboardHub5\"] = done_switchboardHub5_info_df.sum(axis=1)\n",
    "\n",
    "#recalculates the normalized instance count based on the new combined figures\n",
    "# if you don't do this, the process will simply sum the two normalized counts\n",
    "# which would be highly inaccurate\n",
    "done_switchboardHub5_info_df.iloc[2,2] = round(done_switchboardHub5_info_df.iloc[1,2]/\n",
    "                                          done_switchboardHub5_info_df.iloc[0,2]\n",
    "                                          *100000, 2)\n",
    "\n",
    "#drops the separate switchboard and hub5 columns and leaves the combined column\n",
    "done_switchboardHub5_info_df = done_switchboardHub5_info_df.drop(['Hub5', 'Switchboard'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd635491-2f45-48d5-9aea-a97920f4d37c",
   "metadata": {},
   "source": [
    "## Sorting the Dataframes by File and Line\n",
    "\n",
    "This will sort the dataframes first by filename and then by line number. Doing this each step will ensure consistency across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47195ad-6d05-461b-a871-2a59147014fb",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3ab31-536c-4d26-bc15-5a8325dc0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_coraal_instances_df = aint_coraal_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "aint_fisher_instances_df = aint_fisher_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "aint_librispeech_instances_df = aint_librispeech_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "aint_switchboardHub5_instances_df = aint_switchboardHub5_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "aint_timit_instances_df = aint_timit_instances_df.sort_values(by=['File'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c43b6-d8fe-42b7-92d8-42a3f150da97",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a174b-f7fa-40a2-a9dc-ae5f4fbae3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_coraal_instances_df = be_coraal_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "be_fisher_instances_df = be_fisher_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "be_librispeech_instances_df = be_librispeech_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "be_switchboardHub5_instances_df = be_switchboardHub5_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "be_timit_instances_df = be_timit_instances_df.sort_values(by=['File'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef37ec6-a4d6-4faa-8801-f0ecda36b893",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708334f3-2a8c-4a33-947a-9943b6bd05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_coraal_instances_df = done_coraal_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "done_fisher_instances_df = done_fisher_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "done_librispeech_instances_df = done_librispeech_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "done_switchboardHub5_instances_df = done_switchboardHub5_instances_df.sort_values(by=['File', 'Line'])\n",
    "\n",
    "done_timit_instances_df = done_timit_instances_df.sort_values(by=['File'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50b3ab-c45f-4c72-b7bc-dd2a42b853c7",
   "metadata": {},
   "source": [
    "## Creating a Summary Dataframe of All the Quantitative Information from Each Corpus\n",
    "\n",
    "This will combine the quantitative information from each corpus into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4eb22-a12a-413a-9da7-e209cdd8518d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b669e-a9f1-4c65-a3c7-638413be7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a summary info dataframe where SWITCHBOARD AND HUB5 ARE COMBINED\n",
    "aint_all_corpora_info_df = aint_coraal_info_df.join(aint_fisher_info_df).join(aint_librispeech_info_df).join(aint_switchboardHub5_info_df).join(aint_timit_info_df)\n",
    "\n",
    "##Creates a summary info dataframe where SWITCHBOARD AND HUB5 ARE SEPARATE\n",
    "#aint_all_corpora_info_df = aint_coraal_info_df.join(aint_fisher_info_df).join(aint_librispeech_info_df).join(aint_switchboard_info_df).join(aint_hub5_info_df).join(aint_timit_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a9f7c-fe33-45a4-b4be-90abf02aec3e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0bf4d-0b7e-47c4-9367-6fd62f1362ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a summary info dataframe where SWITCHBOARD AND HUB5 ARE COMBINED\n",
    "be_all_corpora_info_df = be_coraal_info_df.join(be_fisher_info_df).join(be_librispeech_info_df).join(be_switchboardHub5_info_df).join(be_timit_info_df)\n",
    "\n",
    "##Creates a summary info dataframe where SWITCHBOARD AND HUB5 ARE SEPARATE\n",
    "#be_all_corpora_info_df = be_coraal_info_df.join(be_fisher_info_df).join(be_librispeech_info_df).join(be_switchboard_info_df).join(be_hub5_info_df).join(be_timit_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325df9e-ff48-4d7f-852f-a7141cd536ad",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735283d-6b51-4548-8ef1-7ba28383d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a summary info dataframe where SWITCHBOARD AND HUB5 ARE COMBINED\n",
    "done_all_corpora_info_df = done_coraal_info_df.join(done_fisher_info_df).join(done_librispeech_info_df).join(done_switchboardHub5_info_df).join(done_timit_info_df)\n",
    "\n",
    "##Creates a summary info dataframe where SWITCHBOARD AND HUB5 ARE SEPARATE\n",
    "#done_all_corpora_info_df = done_coraal_info_df.join(done_fisher_info_df).join(done_librispeech_info_df).join(done_switchboard_info_df).join(done_hub5_info_df).join(done_timit_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183eda68-6c56-44ff-823b-0d31a5b33c03",
   "metadata": {},
   "source": [
    "## Exporting Dataframes to CSV Files\n",
    "\n",
    "This will export the dataframes to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba1ad1-9471-40a8-bb8f-2cf84f815b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output filepath\n",
    "output_filepath = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e5904-f224-46c1-ba1b-4a773c02ef64",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcf810-2183-4442-aece-037febeb615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_coraal_instances_df.to_csv(f\"{output_filepath}aint_coraal_instances.csv\", index=False)\n",
    "\n",
    "aint_fisher_instances_df.to_csv(f\"{output_filepath}aint_fisher_instances.csv\", index=False)\n",
    "\n",
    "aint_librispeech_instances_df.to_csv(f\"{output_filepath}aint_librispeech_instances.csv\", index=False)\n",
    "\n",
    "aint_switchboardHub5_instances_df.to_csv(f\"{output_filepath}aint_switchboardHub5_instances.csv\", index=False)\n",
    "\n",
    "aint_timit_instances_df.to_csv(f\"{output_filepath}aint_timit_instances.csv\", index=False)\n",
    "\n",
    "\n",
    "aint_all_corpora_info_df.to_csv(f\"{output_filepath}aint_all_corpora_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84bce8-aa26-4297-b5a8-bf5976246780",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fd7b5-a519-4c5a-b83b-4c2106ee041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_coraal_instances_df.to_csv(f\"{output_filepath}be_coraal_instances.csv\", index=False)\n",
    "\n",
    "be_fisher_instances_df.to_csv(f\"{output_filepath}be_fisher_instances.csv\", index=False)\n",
    "\n",
    "be_librispeech_instances_df.to_csv(f\"{output_filepath}be_librispeech_instances.csv\", index=False)\n",
    "\n",
    "be_switchboardHub5_instances_df.to_csv(f\"{output_filepath}be_switchboardHub5_instances.csv\", index=False)\n",
    "\n",
    "be_timit_instances_df.to_csv(f\"{output_filepath}be_timit_instances.csv\", index=False)\n",
    "\n",
    "\n",
    "be_all_corpora_info_df.to_csv(f\"{output_filepath}be_all_corpora_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40c726-16bc-4d72-93c4-7dfb846b6e92",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74b516-4f71-4555-abc8-bf04439ac780",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_coraal_instances_df.to_csv(f\"{output_filepath}done_coraal_instances.csv\", index=False)\n",
    "\n",
    "done_fisher_instances_df.to_csv(f\"{output_filepath}done_fisher_instances.csv\", index=False)\n",
    "\n",
    "done_librispeech_instances_df.to_csv(f\"{output_filepath}done_librispeech_instances.csv\", index=False)\n",
    "\n",
    "done_switchboardHub5_instances_df.to_csv(f\"{output_filepath}done_switchboardHub5_instances.csv\", index=False)\n",
    "\n",
    "done_timit_instances_df.to_csv(f\"{output_filepath}done_timit_instances.csv\", index=False)\n",
    "\n",
    "\n",
    "done_all_corpora_info_df.to_csv(f\"{output_filepath}done_all_corpora_info.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
