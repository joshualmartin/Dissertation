{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af98568-db11-4b00-9546-6cd0c9e4a154",
   "metadata": {},
   "source": [
    "# Step 2.6: Getting Word Counts\n",
    "\n",
    "This code will perform a number of tasks. First, it will duplicate lines that have more than one instance of the AAL morphosyntactic feature in question, in order for the each instance within the line to be analyzed in its own line rather than all instances within an utterance being analyzed in the same line. \n",
    "\n",
    "It will also return the following:\n",
    "1. Word count for the full CORAAL utterance content\n",
    "2. Word count for CORAAL pre-feature utterance content\n",
    "3. Word count for CORAAL post-feature utterance content\n",
    "4. Columns containing the text of pre- and post- feature for CORAAL\n",
    "5. Word count for full ASR output content\n",
    "\n",
    "It will then write the results into a CSV file and export it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27e6b-6798-4684-9d4b-b505d5bbc52f",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code: re, os, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f58c-3a5c-4109-91ae-bf6083dbf0da",
   "metadata": {},
   "source": [
    "## Intitial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9910a3-9f2c-450d-b5e8-70c68efaef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9bcf7-7295-44d6-8ef7-dabff40708fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath for the csv produced in Step 2.5\n",
    "aint_file_path = \"path\"\n",
    "\n",
    "be_file_path = \"path\"\n",
    "\n",
    "done_file_path = \"path\"\n",
    "\n",
    "#reads in the gold standard dataframe    \n",
    "aint_gs_df = pd.read_csv(aint_file_path)\n",
    "\n",
    "be_gs_df = pd.read_csv(be_file_path)\n",
    "\n",
    "done_gs_df = pd.read_csv(done_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575a508-d9e1-45d9-8664-a098566d539a",
   "metadata": {},
   "source": [
    "## Defining the Row Duplicating Function\n",
    "\n",
    "This function takes one argument:\n",
    "1. The dataframe created in step 2.5\n",
    "2. The word being searched for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e5c5f-ed40-46f8-9e5c-18e1582fa685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rows(gs_df, search_word_string):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes a gold standard csv produced by previous steps in the pipeline.\n",
    "    Performs the following task:\n",
    "    (1) Duplicates lines which have more than one instance of the\n",
    "    morphosyntactic feature in question in order for each instance to be\n",
    "    examined in its own separate line rather than all instances being\n",
    "    examined in the same line.\n",
    "\n",
    "    One issue remains with this function. If there are multiple instances\n",
    "    of a search word in an utterance, and there is a difference between\n",
    "    the InstancesCountPerLine and the FeatureCountPerLine, then the code\n",
    "    which replicates lines may replicate as many lines as there are\n",
    "    instances rather than actual feature counts. This shouldn't be a\n",
    "    large issue since occurences like this are rare. However, extra\n",
    "    care should be taken in the manually annotation of the .csv files\n",
    "    that result from this code. If the contents of ContentFeature\n",
    "    are not a feature, then the row can be deleted.\n",
    "    \"\"\"\n",
    "\n",
    "################################################################################\n",
    "################## SECTION 1: PRELIMINARY ACTIONS ############################\n",
    "################################################################################\n",
    "\n",
    "    import re\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "################################################################################\n",
    "###################### SECTION 2: EXECUTION OF CODE ##########################\n",
    "################################################################################\n",
    "\n",
    "    # takes the search word input and transforms it into a regular expression \n",
    "    # that will search for only whole words if the sequence of strings submitted \n",
    "    # is contained within a larger word, this will filter those instances out\n",
    "    # and leave only whole matches\n",
    "    search_word_regex = f\"\\\\b[{search_word_string[0].upper()}|{search_word_string[0].lower()}]{search_word_string[1:]}\\\\b\"\n",
    "\n",
    "    # the following will create columns to be changed later\n",
    "    # IterationNumber will be used to help duplicate lines that contain more\n",
    "    #  than one instance of the feature in question\n",
    "    gs_df['IterationNumber'] = 1\n",
    "\n",
    "    # creates an empty column to store the Content that occurs before the feature\n",
    "    gs_df['Content_PreFeature'] = np.nan\n",
    "\n",
    "    # creates an empty column to store the Content that occurs after the feature\n",
    "    gs_df['Content_PostFeature'] = np.nan\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # creates a list of columns from the gold standard dataframe\n",
    "    gs_df_column_names = list(gs_df)\n",
    "                \n",
    "    # creates an empty list for replicated lines to be appended to. A line will \n",
    "    # be replicated if it contains more than one instance of a feature. The\n",
    "    # result will be as many total occurences of the line as there are number\n",
    "    # of features in the line. This will allow each instance of the feature in \n",
    "    # the line to be examined separately, rather than needing to examine each\n",
    "    # instance within the same line.\n",
    "    replicated_lines = []\n",
    "\n",
    "    #iterates through the rows of the gold standard dataframe\n",
    "    for row in gs_df.itertuples():\n",
    "        \n",
    "        #if there is only one instance of the feature in the line, this will \n",
    "        # separate the Content into columns based on what comes before and after\n",
    "        # the feature in the text and a separate column for the feature. Doing\n",
    "        # this will replicate concordance lines which can center the feature\n",
    "        # in question.\n",
    "        #re.findall is used here to find all the instances of the search word \n",
    "        # (i.e., feature). There are other ways to search strings with re but this\n",
    "        # one will find them all.\n",
    "        if row.InstancesCountPerLine == 1:\n",
    "\n",
    "          #here, re.finditer is used because finditer will iterate through the\n",
    "          # matches of the search word and return a match object. match objects\n",
    "          # provide more information than just the match. for example, .start()\n",
    "          # returns the start position of the match within the text.\n",
    "          # Check here for more info: \n",
    "          #  https://docs.python.org/3/library/re.html#match-objects\n",
    "          #Loops through the matches of the search word in the row's Content\n",
    "            for match in re.finditer(search_word_regex, str(row.Content)):\n",
    "\n",
    "                # replaces the \"ContentBeforeFeature\" cell with all text that occurs\n",
    "                #  before the search word (i.e., feature in question)\n",
    "                gs_df.loc[row.Index, \"Content_PreFeature\"] = str(row.Content)[:match.start()]\n",
    "\n",
    "                # replaces the \"ContentAfterFeature\" cell with all text that occurs\n",
    "                #  after the search word (i.e., feature in question)\n",
    "                gs_df.loc[row.Index, \"Content_PostFeature\"] = str(row.Content)[match.end()+1:]\n",
    "\n",
    "        elif row.InstancesCountPerLine > 1:\n",
    "\n",
    "            # sets the iteration number to 0. This number will be changed within\n",
    "            # the loops below. It will be used to determine which instance of the\n",
    "            # feature should be centered on that line. It will also be used later\n",
    "            # to sort lines along with File and Line.\n",
    "            iteration_number = 0\n",
    "            \n",
    "            if search_word_string in [\"ain't\", \"isn't\", \"aren't\", \"I'm not\", \"didn't\", \"hasn't\", \"haven't\"]:\n",
    "\n",
    "                #here, re.finditer is used because finditer will iterate through the\n",
    "                # matches of the search word and return a match object. match objects\n",
    "                # provide more information than just the match. for example, .start()\n",
    "                # returns the start position of the match within the text.\n",
    "                # Check here for more info: \n",
    "                #  https://docs.python.org/3/library/re.html#match-objects\n",
    "                #Loops through the matches of the search word in the row's Content\n",
    "                for match in re.finditer(search_word_regex, str(row.Content)):\n",
    "                                        \n",
    "                    # creates a variable with all text that occurs\n",
    "                    #  before the search word (i.e., feature in question)\n",
    "                    current_before = str(row.Content)[:match.start()]\n",
    "\n",
    "                    # creates a variable with all text that occurs\n",
    "                    #  after the search word (i.e., feature in question)\n",
    "                    current_after = str(row.Content)[match.end()+1:]\n",
    "\n",
    "                    # creates a list of the values in cells of the row. Importantly,\n",
    "                    # if the variable begins with \"row.\" that means, it's simply\n",
    "                    # taking what's already there. But the last four variables are taken\n",
    "                    # from the variables created in the immediately previous steps.\n",
    "                    # iteration number is added to by 1 to reflect which iteration of \n",
    "                    # the feature matches it is.\n",
    "                    \n",
    "                    cell_values = [row.File, row.Line, row.Speaker, row.UttStartTime,\n",
    "                                   row.UttEndTime, row.UttLength,\n",
    "                                   row.Content, row.Content_cleaned, row.AintVariation,\n",
    "                                   row.InstancesCountPerLine, row.FeatureCountPerLine,\n",
    "                                   row.WadaSNRMeade, row.WadaSNRRigal, row.NistSNRRigal,\n",
    "                                   row.SyllableCount, row.SpeechRate,\n",
    "                                   row.amazon_transcription,\n",
    "                                   row.amazon_transcription_cleaned,\n",
    "                                   row.deepspeech_transcription,\n",
    "                                   row.deepspeech_transcription_cleaned, row.deepspeech_ConfidenceLevel,\n",
    "                                   row.google_transcription, row.google_transcription_cleaned,\n",
    "                                   row.google_ConfidenceLevel, row.IBMWatson_transcription,\n",
    "                                   row.IBMWatson_transcription_cleaned, row.IBMWatson_ConfidenceLevel,\n",
    "                                   row.microsoft_transcription, row.microsoft_transcription_cleaned,\n",
    "                                   iteration_number+1, current_before, current_after]\n",
    "                                        \n",
    "                    # zips together the column names with the cell values from the\n",
    "                    # current row. Zipping creates tuples\n",
    "                    zipped = zip(gs_df_column_names, cell_values)\n",
    "                    \n",
    "                    # creates a python dictionary based on the zipped variable just\n",
    "                    #  created. This is necessary to be able to append the row\n",
    "                    #  back into the larger dataframe later.\n",
    "                    line_dict = dict(zipped)\n",
    "                    \n",
    "                    # appends the dictionary to the replicated lines empty list above\n",
    "                    #  to be used for appending to the dataframe later\n",
    "                    replicated_lines.append(line_dict)\n",
    "\n",
    "                    # increases the iteration number by one so the next iteration number\n",
    "                    #  will be correct\n",
    "                    iteration_number += 1\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                #here, re.finditer is used because finditer will iterate through the\n",
    "                # matches of the search word and return a match object. match objects\n",
    "                # provide more information than just the match. for example, .start()\n",
    "                # returns the start position of the match within the text.\n",
    "                # Check here for more info: \n",
    "                #  https://docs.python.org/3/library/re.html#match-objects\n",
    "                #Loops through the matches of the search word in the row's Content\n",
    "                for match in re.finditer(search_word_regex, str(row.Content)):\n",
    "\n",
    "                    # creates a variable with all text that occurs\n",
    "                    #  before the search word (i.e., feature in question)\n",
    "                    current_before = str(row.Content)[:match.start()]\n",
    "\n",
    "                    # creates a variable with all text that occurs\n",
    "                    #  after the search word (i.e., feature in question)\n",
    "                    current_after = str(row.Content)[match.end()+1:]\n",
    "\n",
    "                    # creates a list of the values in cells of the row. Importantly,\n",
    "                    # if the variable begins with \"row.\" that means, it's simply\n",
    "                    # taking what's already there. But the last four variables are taken\n",
    "                    # from the variables created in the immediately previous steps.\n",
    "                    # iteration number is added to by 1 to reflect which iteration of \n",
    "                    # the feature matches it is.\n",
    "                    cell_values = [row.File, row.Line, row.Speaker, row.UttStartTime,\n",
    "                                   row.UttEndTime, row.UttLength,\n",
    "                                   row.Content, row.Content_cleaned,\n",
    "                                   row.InstancesCountPerLine, row.FeatureCountPerLine,\n",
    "                                   row.WadaSNRMeade, row.WadaSNRRigal, row.NistSNRRigal,\n",
    "                                   row.SyllableCount, row.SpeechRate,\n",
    "                                   row.amazon_transcription,\n",
    "                                   row.amazon_transcription_cleaned,\n",
    "                                   row.deepspeech_transcription,\n",
    "                                   row.deepspeech_transcription_cleaned, row.deepspeech_ConfidenceLevel,\n",
    "                                   row.google_transcription, row.google_transcription_cleaned,\n",
    "                                   row.google_ConfidenceLevel, row.IBMWatson_transcription,\n",
    "                                   row.IBMWatson_transcription_cleaned, row.IBMWatson_ConfidenceLevel,\n",
    "                                   row.microsoft_transcription, row.microsoft_transcription_cleaned,\n",
    "                                   iteration_number+1, current_before, current_after]\n",
    "\n",
    "                    # zips together the column names with the cell values from the\n",
    "                    # current row. Zipping creates tuples\n",
    "                    zipped = zip(gs_df_column_names, cell_values)\n",
    "\n",
    "                    # creates a python dictionary based on the zipped variable just\n",
    "                    #  created. This is necessary to be able to append the row\n",
    "                    #  back into the larger dataframe later.\n",
    "                    line_dict = dict(zipped)\n",
    "\n",
    "                    # appends the dictionary to the replicated lines empty list above\n",
    "                    #  to be used for appending to the dataframe later\n",
    "                    replicated_lines.append(line_dict)\n",
    "\n",
    "                    # increases the iteration number by one so the next iteration number\n",
    "                    #  will be correct\n",
    "                    iteration_number += 1\n",
    "\n",
    "    # appends replicated lines to the gold standard dataframe              \n",
    "    gs_df = gs_df.append(replicated_lines)\n",
    "    \n",
    "    gs_df_1instance = gs_df[gs_df['InstancesCountPerLine'] == 1]\n",
    "    \n",
    "    gs_df_replicated = gs_df[(gs_df['InstancesCountPerLine'].astype(int)>1) & (gs_df['Content_PreFeature'].notna())]\n",
    "    \n",
    "    gs_df = pd.concat([gs_df_1instance, gs_df_replicated], ignore_index=True).sort_values(by=['File', 'Line'])\n",
    "    \n",
    "    return gs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d2172-5414-4620-9a13-16f20ae13391",
   "metadata": {},
   "source": [
    "## Defining the Word Counting Function\n",
    "\n",
    "TThis function takes one argument:\n",
    "1. The dataframe created in step 2.5\n",
    "2. The word being searched for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1b98a-bc31-4dff-921c-40613885645d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_word_counts(gs_df, search_word_string):\n",
    "\n",
    "    \"\"\"\n",
    "    This function will get you:\n",
    "\n",
    "    (1) word count for the full CORAAL utterance content\n",
    "    (2) word count for CORAAL pre-feature utterance content\n",
    "    (3) word count for CORAAL post-feature utterance content\n",
    "    (4) columns containing the text of pre- and post- feature for CORAAL\n",
    "    (5) word count for full ASR output content\n",
    "\n",
    "    It will also rearrange the columns to a better reading order\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    gs_df[\"Content_cleaned_PreFeature\"] = np.nan\n",
    "\n",
    "    gs_df[\"Content_cleaned_PostFeature\"] = np.nan\n",
    "\n",
    "\n",
    "    # takes the search word input and transforms it into a regular expression \n",
    "    # that will search for only whole words if the sequence of strings submitted \n",
    "    # is contained within a larger word, this will filter those instances out\n",
    "    # and leave only whole matches\n",
    "    search_word_regex = f\"\\\\b[{search_word_string[0].upper()}|{search_word_string[0].lower()}]{search_word_string[1:]}\\\\b\"\n",
    "\n",
    "\n",
    "    for row in gs_df.itertuples():\n",
    "        \n",
    "        if search_word_regex == \"\\\\b[A|a]in't\\\\b\":\n",
    "            \n",
    "            search_word_regex = \"\\\\b[A|a]i n't\\\\b\"\n",
    "            \n",
    "        elif search_word_regex == \"\\\\b[I|i]sn't\\\\b\":\n",
    "            \n",
    "            search_word_regex = \"\\\\b[I|i]s n't\\\\b\"\n",
    "            \n",
    "        elif search_word_regex == \"\\\\b[A|a]ren't\\\\b\":\n",
    "            \n",
    "            search_word_regex = \"\\\\b[A|a]re n't\\\\b\"\n",
    "        \n",
    "        elif search_word_regex == \"\\\\b[I|i]'m not\\\\b\":\n",
    "            \n",
    "            search_word_regex = \"\\\\b[I|i] 'm not\\\\b\"\n",
    "            \n",
    "        elif search_word_regex == \"\\\\b[D|d]idn't\\\\b\":\n",
    "            \n",
    "            search_word_regex = \"\\\\b[D|d]id n't\\\\b\"\n",
    "            \n",
    "        elif search_word_regex == \"\\\\b[H|h]aven't\\\\b\":\n",
    "            \n",
    "            search_word_regex = \"\\\\b[H|h]ave n't\\\\b\"\n",
    "            \n",
    "        elif search_word_regex == \"\\\\b[H|h]asn't\\\\b\":\n",
    "            \n",
    "            search_word_regex = \"\\\\b[H|h]as n't\\\\b\"\n",
    "            \n",
    "        \n",
    "        if row.InstancesCountPerLine == 1:\n",
    "\n",
    "            for match in re.finditer(search_word_regex, str(row.Content_cleaned)):\n",
    "\n",
    "                # creates a variable with all text that occurs\n",
    "                #  before the search word (i.e., feature in question)\n",
    "                gs_df.loc[row.Index, \"Content_cleaned_PreFeature\"] = str(row.Content_cleaned)[:match.start()]\n",
    "\n",
    "                # creates a variable with all text that occurs\n",
    "                #  after the search word (i.e., feature in question)\n",
    "                gs_df.loc[row.Index, \"Content_cleaned_PostFeature\"] = str(row.Content_cleaned)[match.end()+1:]\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            #if there is more than one instance, the original code would only append the \n",
    "            #  pre- and post- of the first iteration. This should fix that\n",
    "                        \n",
    "            matches = []\n",
    "            \n",
    "            for match in re.finditer(search_word_regex, str(row.Content_cleaned)):\n",
    "                \n",
    "                matches.append(match)\n",
    "                            \n",
    "            if len(matches) > 0:\n",
    "            \n",
    "                #gets the iteration number and subtracts one to get the\n",
    "                #  correct list index to draw the correlated match from\n",
    "                iteration_number = row.IterationNumber - 1\n",
    "\n",
    "                # creates a variable with all text that occurs\n",
    "                #  before the search word (i.e., feature in question)\n",
    "                gs_df.loc[row.Index, \"Content_cleaned_PreFeature\"] = str(row.Content_cleaned)[:matches[iteration_number].start()]\n",
    "\n",
    "                # creates a variable with all text that occurs\n",
    "                #  after the search word (i.e., feature in question)\n",
    "                gs_df.loc[row.Index, \"Content_cleaned_PostFeature\"] = str(row.Content_cleaned)[matches[iteration_number].end()+1:]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # creates a variable with all text that occurs\n",
    "                #  before the search word (i.e., feature in question)\n",
    "                gs_df.loc[row.Index, \"Content_cleaned_PreFeature\"] = np.nan\n",
    "\n",
    "                # creates a variable with all text that occurs\n",
    "                #  after the search word (i.e., feature in question)\n",
    "                gs_df.loc[row.Index, \"Content_cleaned_PostFeature\"] = np.nan\n",
    "                \n",
    "            \n",
    "            \n",
    "    #a list of new column names to be created\n",
    "    new_column_names = [\"Content_WordCount\", \"Content_PreFeature_WordCount\", \"Content_PostFeature_WordCount\",\n",
    "                    \"Content_cleaned_WordCount\", \"Content_cleaned_PreFeature_WordCount\",\n",
    "                    \"Content_cleaned_PostFeature_WordCount\"]\n",
    "\n",
    "\n",
    "    #loops through column names\n",
    "    for column in new_column_names:\n",
    "        \n",
    "        #creates an empty column with that name\n",
    "        gs_df[column] = np.nan\n",
    "        \n",
    "    #loops through the rows in the dataframe    \n",
    "    for row in gs_df.itertuples():\n",
    "        \n",
    "        #the rest of this code gets the various counts or content\n",
    "        \n",
    "        #original content\n",
    "        if type(row.Content) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_WordCount\"] = len(row.Content.split())\n",
    "        \n",
    "        \n",
    "        #cleaned content\n",
    "        if type(row.Content_cleaned) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_cleaned_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_cleaned_WordCount\"] = len(row.Content_cleaned.split())\n",
    "        \n",
    "        \n",
    "        #original content pre-feature\n",
    "        if type(row.Content_PreFeature) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_PreFeature_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_PreFeature_WordCount\"] = len(row.Content_PreFeature.split())\n",
    "        \n",
    "        \n",
    "        #original content pre-feature cleaned\n",
    "        if type(row.Content_cleaned_PreFeature) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_cleaned_PreFeatureWord_Count\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_cleaned_PreFeature_WordCount\"] = len(row.Content_cleaned_PreFeature.split())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #cleaned content post-feature\n",
    "        if type(row.Content_PostFeature) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_PostFeature_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_PostFeature_WordCount\"] = len(row.Content_PostFeature.split())\n",
    "        \n",
    "        \n",
    "        #cleaned content post-feature\n",
    "        if type(row.Content_cleaned_PostFeature) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_cleaned_PostFeature_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"Content_cleaned_PostFeature_WordCount\"] = len(row.Content_cleaned_PostFeature.split())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #### Amazon\n",
    "        \n",
    "        if type(row.amazon_transcription) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"amazon_transcription_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"amazon_transcription_WordCount\"] = len(row.amazon_transcription.split())\n",
    "        \n",
    "        \n",
    "        if type(row.amazon_transcription_cleaned) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"amazon_transcription_cleaned_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"amazon_transcription_cleaned_WordCount\"] = len(row.amazon_transcription_cleaned.split())\n",
    "        \n",
    "        \n",
    "        #### Deepspeech\n",
    "        \n",
    "        if type(row.deepspeech_transcription) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"deepspeech_transcription_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"deepspeech_transcription_WordCount\"] = len(row.deepspeech_transcription.split())\n",
    "        \n",
    "        \n",
    "        if type(row.deepspeech_transcription_cleaned) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"deepspeech_transcription_cleaned_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"deepspeech_transcription_cleaned_WordCount\"] = len(row.deepspeech_transcription_cleaned.split())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         #### Google\n",
    "        \n",
    "        if type(row.google_transcription) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"google_transcription_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"google_transcription_WordCount\"] = len(row.google_transcription.split())\n",
    "        \n",
    "        \n",
    "        if type(row.google_transcription_cleaned) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"google_transcription_cleaned_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"google_transcription_cleaned_WordCount\"] = len(row.google_transcription_cleaned.split())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         #### IBMWatson\n",
    "        \n",
    "        if type(row.IBMWatson_transcription) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"IBMWatson_transcription_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"IBMWatson_transcription_WordCount\"] = len(row.IBMWatson_transcription.split())\n",
    "        \n",
    "        \n",
    "        if type(row.IBMWatson_transcription_cleaned) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"IBMWatson_transcription_cleaned_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"IBMWatson_transcription_cleaned_WordCount\"] = len(row.IBMWatson_transcription_cleaned.split())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         #### Microsoft\n",
    "        \n",
    "        if type(row.microsoft_transcription) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"microsoft_transcription_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"microsoft_transcription_WordCount\"] = len(row.microsoft_transcription.split())\n",
    "        \n",
    "        \n",
    "        if type(row.microsoft_transcription_cleaned) != str:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"microsoft_transcription_cleaned_WordCount\"] = np.nan\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            gs_df.loc[row.Index, \"microsoft_transcription_cleaned_WordCount\"] = len(row.microsoft_transcription_cleaned.split())\n",
    "        \n",
    "        \n",
    "    if search_word_string in [\"ain't\", \"isn't\", \"aren't\", \"I'm not\", \"didn't\", \"hasn't\", \"haven't\"]:\n",
    "        \n",
    "        gs_df = gs_df[['File', 'Line', 'Speaker', \n",
    "                           'UttStartTime', 'UttEndTime', \n",
    "                           'UttLength', 'AintVariation', 'InstancesCountPerLine', \n",
    "                           'FeatureCountPerLine','IterationNumber', 'Content', \n",
    "                           'SyllableCount', 'SpeechRate', 'WadaSNRMeade', \n",
    "                           'WadaSNRRigal', 'NistSNRRigal',\n",
    "                           'Content_WordCount', 'Content_PreFeature', 'Content_PreFeature_WordCount',\n",
    "                           'Content_PostFeature', 'Content_PostFeature_WordCount', \n",
    "                           'Content_cleaned', 'Content_cleaned_WordCount',\n",
    "                           'Content_cleaned_PreFeature', 'Content_cleaned_PreFeature_WordCount',\n",
    "                           'Content_cleaned_PostFeature', 'Content_cleaned_PostFeature_WordCount',\n",
    "                           'amazon_transcription', 'amazon_transcription_WordCount',\n",
    "                           'amazon_transcription_cleaned', 'amazon_transcription_cleaned_WordCount',\n",
    "                           'deepspeech_transcription', 'deepspeech_transcription_WordCount', \n",
    "                           'deepspeech_transcription_cleaned', 'deepspeech_transcription_cleaned_WordCount',\n",
    "                           'deepspeech_ConfidenceLevel', \n",
    "                           'google_transcription', 'google_transcription_WordCount',\n",
    "                           'google_transcription_cleaned', 'google_transcription_cleaned_WordCount', \n",
    "                           'google_ConfidenceLevel', 'IBMWatson_transcription', \n",
    "                           'IBMWatson_transcription_WordCount', \n",
    "                           'IBMWatson_transcription_cleaned', 'IBMWatson_transcription_cleaned_WordCount', \n",
    "                           'IBMWatson_ConfidenceLevel', \n",
    "                           'microsoft_transcription', 'microsoft_transcription_WordCount', \n",
    "                           'microsoft_transcription_cleaned', 'microsoft_transcription_cleaned_WordCount']]\n",
    "    \n",
    "    else:        \n",
    "        \n",
    "        gs_df = gs_df[['File', 'Line', 'Speaker', \n",
    "                           'UttStartTime', 'UttEndTime', \n",
    "                           'UttLength', 'InstancesCountPerLine', \n",
    "                           'FeatureCountPerLine','IterationNumber', 'Content', \n",
    "                           'SyllableCount', 'SpeechRate', 'WadaSNRMeade', \n",
    "                           'WadaSNRRigal', 'NistSNRRigal',\n",
    "                           'Content_WordCount', 'Content_PreFeature', 'Content_PreFeature_WordCount',\n",
    "                           'Content_PostFeature', 'Content_PostFeature_WordCount', \n",
    "                           'Content_cleaned', 'Content_cleaned_WordCount',\n",
    "                           'Content_cleaned_PreFeature', 'Content_cleaned_PreFeature_WordCount',\n",
    "                           'Content_cleaned_PostFeature', 'Content_cleaned_PostFeature_WordCount',\n",
    "                           'amazon_transcription', 'amazon_transcription_WordCount',\n",
    "                           'amazon_transcription_cleaned', 'amazon_transcription_cleaned_WordCount',\n",
    "                           'deepspeech_transcription', 'deepspeech_transcription_WordCount', \n",
    "                           'deepspeech_transcription_cleaned', 'deepspeech_transcription_cleaned_WordCount',\n",
    "                           'deepspeech_ConfidenceLevel', \n",
    "                           'google_transcription', 'google_transcription_WordCount',\n",
    "                           'google_transcription_cleaned', 'google_transcription_cleaned_WordCount', \n",
    "                           'google_ConfidenceLevel', 'IBMWatson_transcription', \n",
    "                           'IBMWatson_transcription_WordCount', \n",
    "                           'IBMWatson_transcription_cleaned', 'IBMWatson_transcription_cleaned_WordCount', \n",
    "                           'IBMWatson_ConfidenceLevel', \n",
    "                           'microsoft_transcription', 'microsoft_transcription_WordCount', \n",
    "                           'microsoft_transcription_cleaned', 'microsoft_transcription_cleaned_WordCount']]\n",
    "    \n",
    "    gs_df = gs_df.reset_index(drop=True)\n",
    "    \n",
    "    return gs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21f853-1ba1-4f40-8a25-51d17bbfe75d",
   "metadata": {},
   "source": [
    "## Executing the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63203f6b-2d23-492b-a8c3-6bfd15f0125d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4b3f6-cc76-4b14-ba43-e41a74011e7e",
   "metadata": {},
   "source": [
    "Before running the code for the *ain't* variations, the variations will be split into separate dataframes to be processed. These will be concatenated again in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b8373-d31e-4082-84c2-4c89e22bbe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"ain't\"]\n",
    "isnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"isn't\"]\n",
    "arent_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"aren't\"]\n",
    "imnot_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"I'm not\"]\n",
    "didnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"didn't\"]\n",
    "havent_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"haven't\"]\n",
    "hasnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"hasn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b236a-5269-40f7-8fc9-067769d57d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "aint_df = duplicate_rows(aint_df, \"ain't\")\n",
    "\n",
    "# create the word count dataframe\n",
    "aint_df = get_word_counts(aint_df, \"ain't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28334414-9d5a-4848-8581-b15c46bf2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "isnt_df = duplicate_rows(isnt_df, \"isn't\")\n",
    "\n",
    "# create the word count dataframe\n",
    "isnt_df = get_word_counts(isnt_df, \"isn't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df1819-9c9a-445b-b670-39028ee68377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "arent_df = duplicate_rows(arent_df, \"aren't\")\n",
    "\n",
    "# create the word count dataframe\n",
    "arent_df = get_word_counts(arent_df, \"aren't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122e22d-8506-4920-8a25-f11c81fc7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "imnot_df = duplicate_rows(imnot_df, \"I'm not\")\n",
    "\n",
    "# create the word count dataframe\n",
    "imnot_df = get_word_counts(imnot_df, \"I'm not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908d23c-64c9-4ca5-836d-eb983aa4a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "didnt_df = duplicate_rows(didnt_df, \"didn't\")\n",
    "\n",
    "# create the word count dataframe\n",
    "didnt_df = get_word_counts(didnt_df, \"didn't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948fb2ec-5443-46d1-ac64-122447a9b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "havent_df = duplicate_rows(havent_df, \"haven't\")\n",
    "\n",
    "# create the word count dataframe\n",
    "havent_df = get_word_counts(havent_df, \"haven't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c9df9-f93d-47e3-a574-c6b927b3b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "hasnt_df = duplicate_rows(hasnt_df, \"hasn't\")\n",
    "\n",
    "# create the word count dataframe\n",
    "hasnt_df = get_word_counts(hasnt_df, \"hasn't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133cab13-9d2b-4c1a-b5c9-7b95ae8832ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = pd.concat([aint_df, isnt_df, arent_df, imnot_df, didnt_df, havent_df, hasnt_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f839-307e-4e80-a648-1842e9f01eb5",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa61f4-9499-4a75-afcd-b44d3509ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "be_gs_df = duplicate_rows(be_gs_df, \"be\")\n",
    "\n",
    "# create the word count dataframe\n",
    "be_gs_df = get_word_counts(be_gs_df, \"be\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1093-19ed-4e4b-be56-0aec20f9002c",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb105bf-9f1c-470e-ba6c-9db49b30ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the duplicated rows dataframe\n",
    "done_gs_df = duplicate_rows(done_gs_df, \"done\")\n",
    "\n",
    "# create the word count dataframe\n",
    "done_gs_df = get_word_counts(done_gs_df, \"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c303e-b650-4e9d-bc79-e279751611fe",
   "metadata": {},
   "source": [
    "## Sorting the Dataframes by File and Line\n",
    "\n",
    "This will sort the dataframes first by filename and then by line number. Doing this each step will ensure consistency across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57205-3bfd-4933-bdbe-5f8e6e75ea9d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f32913-4e1f-44ea-bd82-f0fdbfd167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = aint_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a08157-c6a8-4cbb-8978-e6af4b931a7e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12356d-7ffb-4af6-9c58-21e00fae9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df = be_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cd837-4bb3-4129-b4db-583aeb11ea90",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc53ff-73c4-484b-ad31-2025fba38270",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df = done_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983788-c9a5-4b73-8a2e-1d18a6ae24b9",
   "metadata": {},
   "source": [
    "## Exporting Dataframes to CSV Files\n",
    "\n",
    "This will export the dataframes to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff3dee-e5ab-47f3-a75d-14de88610b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the output path where the CSVs will be stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331a54-e11e-4b58-813a-717796e37e73",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e3561-7148-4ace-aa8f-2e2ca60a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df.to_csv(f\"{csv_output_path}aint_variations_wordCounts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf982d6-41cb-46d8-9917-8d4a6a5bdd4d",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6042b-fa7e-4017-aac3-7cfca0b8fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df.to_csv(f\"{csv_output_path}be_wordCounts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101802dd-7fd7-4676-a52b-8b9d76e1c990",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cab56-e32a-467e-aa8e-12a6640b5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df.to_csv(f\"{csv_output_path}done_wordCounts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396c79b-7f3f-4379-bca7-965543e3bf17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
