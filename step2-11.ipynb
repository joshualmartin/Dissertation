{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af98568-db11-4b00-9546-6cd0c9e4a154",
   "metadata": {},
   "source": [
    "# Step 2.11: Checking Adjacent Tokens\n",
    "\n",
    "This code will check adjacent tokens to the left and right of the instance of the word in the original utterance and the ASR output and see if they match. It will also check just one word to the left of each to see if they match (Further, for *be*, it will check more specific cases.). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27e6b-6798-4684-9d4b-b505d5bbc52f",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code: os, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f58c-3a5c-4109-91ae-bf6083dbf0da",
   "metadata": {},
   "source": [
    "## Intitial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9910a3-9f2c-450d-b5e8-70c68efaef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9bcf7-7295-44d6-8ef7-dabff40708fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath for the csv produced in Step 2.10\n",
    "aint_file_path = \"path\"\n",
    "\n",
    "be_file_path = \"path\"\n",
    "\n",
    "done_file_path = \"path\"\n",
    "\n",
    "#reads in the gold standard dataframe    \n",
    "aint_gs_df = pd.read_csv(aint_file_path)\n",
    "\n",
    "be_gs_df = pd.read_csv(be_file_path)\n",
    "\n",
    "done_gs_df = pd.read_csv(done_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108b2f4-a3a4-4b60-a39e-2d0ed35a1606",
   "metadata": {},
   "source": [
    "# Defining the Checking Adjacent Tokens Function\n",
    "\n",
    "This function takes the following arguments:\n",
    "1. The feature\n",
    "2. The number of instances of the word (taken from the dataframe)\n",
    "3. The number of instances of the AAL feature (taken from the dataframe)\n",
    "4. The iteration number (taken from the dataframe)\n",
    "5. The cleaned original utterance content as a string\n",
    "6. The cleaned ASR output as a string\n",
    "\n",
    "The function will return different numbers for *be* inputs based on the case. These are meant to be informative and will be converted for data use in the next step. For now, here's a key:\n",
    "\n",
    "- 13 (multiple instances): The first two words to the left of the original and the first word to the left of the ASR output are in the modals list (composed of single and multi-word compound modals).\n",
    "- 12 (multiple instances): The first word to the left of the original and the first two words to the left of the ASR output are in the modals list (composed of single and multi-word compound modals).\n",
    "- 11 (multiple instances): The first two words to the left of the original and ASR output are in the modals list (composed of single and multi-word compound modals).\n",
    "- 10 (multiple instances): The first word to the left of the original and ASR output are both modals.\n",
    "- 9 (multiple instances): The first word to the left of the original and ASR output match.\n",
    "- 8 (multiple instances): The first word to the left and right of *be* in both the original and ASR output match.\n",
    "- 7 (one instance): The first two words to the left of the original and the first word to the left of the ASR output are in the modals list (composed of single and multi-word compound modals).\n",
    "- 6 (one instance): The first word to the left of the original and the first two words to the left of the ASR output are in the modals list (composed of single and multi-word compound modals).\n",
    "- 5 (one instance): The first two words to the left of the original and ASR output are in the modals list (composed of single and multi-word compound modals).\n",
    "- 4 (one instance): The first word to the left of the original and ASR output are both modals.\n",
    "- 3 (one instance): The first word to the left of the original and ASR output match.\n",
    "- 2 (one instance): The first word to the left and right of *be* in both the original and ASR output match.\n",
    "- 1 (one instance): The first word in the ASR output is *be* but the first word in the original is also *be*.\n",
    "- -1 (one instance): The first word in the ASR output is *be* but the first word in the original is not *be*.\n",
    "- -2 (one instance): There is only one instance of the feature in the original utterance, but the ASR output is preceded by a modal.\n",
    "- -3 (one instance): The else condition for one instance of the word.\n",
    "- -4 (multiple instances): The number of word instances does not match between the original and ASR output.\n",
    "- -5 (multiple instances): The else condition for multiple instances of the word.\n",
    "- -6: The general else condition\n",
    "- -7: The except condition for if the code tries but does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d874c9-fba0-47a5-8298-44b8c4ea2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkAdjacentTokens(feature, instances_count, feature_count, iteration_number, cleaned_input, cleaned_output):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check adjacent tokens to the left and right of the instance of the word\n",
    "    in the original utterance and the ASR output and see if they match.\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_input = cleaned_input.replace(\"gon na\", \"going to\").replace(\"got ta\", \"got to\").replace(\"wan na\", \"want to\").replace(\"i 'm a be\", \"i 'm going to be\")\n",
    "    \n",
    "    \n",
    "    if feature == \"ain't\":\n",
    "        \n",
    "        cleaned_input = cleaned_input.replace(\"ai n't\", \"ain't\")\n",
    "        \n",
    "    if feature == \"ain't\":\n",
    "        \n",
    "        cleaned_input = cleaned_input.replace(\"ai n't\", \"ain't\")\n",
    "            \n",
    "    elif feature == \"isn't\":\n",
    "\n",
    "        cleaned_input = cleaned_input.replace(\"is n't\", \"isn't\")\n",
    "\n",
    "    elif feature == \"aren't\":\n",
    "\n",
    "        cleaned_input = cleaned_input.replace(\"are n't\", \"aren't\")\n",
    "\n",
    "    elif feature == \"I'm not\":\n",
    "        \n",
    "        feature = \"i'mnot\"\n",
    "\n",
    "        cleaned_input = cleaned_input.replace(\"i 'm not\", \"i'mnot\")\n",
    "\n",
    "    elif feature == \"didn't\":\n",
    "\n",
    "        cleaned_input = cleaned_input.replace(\"did n't\", \"didn't\")\n",
    "\n",
    "    elif feature == \"haven't\":\n",
    "\n",
    "        cleaned_input = cleaned_input.replace(\"have n't\", \"haven't\")\n",
    "\n",
    "    elif feature == \"hasn't\":\n",
    "\n",
    "        cleaned_input = cleaned_input.replace(\"has n't\", \"hasn't\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    split_input = cleaned_input.split()\n",
    "    \n",
    "    input_index = split_input.index(feature)\n",
    "    \n",
    "    L2_L1_input = split_input[input_index-2:input_index]\n",
    "    \n",
    "    L1_R1_input = split_input[input_index-1:input_index+2]\n",
    "    \n",
    "    L1_input = split_input[input_index-1]\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        cleaned_output = cleaned_output.replace(\"gon na\", \"going to\").replace(\"got ta\", \"got to\").replace(\"wan na\", \"want to\").replace(\"i 'm a be\", \"i 'm going to be\")\n",
    "        \n",
    "        \n",
    "        if feature == \"ain't\":\n",
    "            \n",
    "            cleaned_output = cleaned_output.replace(\"ai n't\", \"ain't\")\n",
    "            \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "\n",
    "                L1_R1_output = cleaned_output.split()[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = cleaned_output.split()[output_index-1]\n",
    "\n",
    "\n",
    "                if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                elif L1_input == L1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                return np.nan\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif feature == \"isn't\":\n",
    "            \n",
    "            cleaned_output = cleaned_output.replace(\"is n't\", \"isn't\")\n",
    "            \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "\n",
    "                L1_R1_output = cleaned_output.split()[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = cleaned_output.split()[output_index-1]\n",
    "\n",
    "\n",
    "                if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                elif L1_input == L1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                return np.nan\n",
    "            \n",
    "        \n",
    "        elif feature == \"aren't\":\n",
    "            \n",
    "            cleaned_output = cleaned_output.replace(\"are n't\", \"aren't\")\n",
    "            \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "\n",
    "                L1_R1_output = cleaned_output.split()[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = cleaned_output.split()[output_index-1]\n",
    "\n",
    "\n",
    "                if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                elif L1_input == L1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                return np.nan\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif feature == \"I'm not\":\n",
    "            \n",
    "            cleaned_output = cleaned_output.replace(\"i 'm not\", \"i'mnot\")\n",
    "            \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "\n",
    "                L1_R1_output = cleaned_output.split()[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = cleaned_output.split()[output_index-1]\n",
    "\n",
    "\n",
    "                if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                elif L1_input == L1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                return np.nan\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif feature == \"didn't\":\n",
    "            \n",
    "            cleaned_output = cleaned_output.replace(\"did n't\", \"didn't\")\n",
    "            \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "\n",
    "                L1_R1_output = cleaned_output.split()[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = cleaned_output.split()[output_index-1]\n",
    "\n",
    "\n",
    "                if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                elif L1_input == L1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                return np.nan\n",
    "            \n",
    "            \n",
    "        elif feature == \"haven't\":\n",
    "            \n",
    "            cleaned_output = cleaned_output.replace(\"have n't\", \"haven't\")\n",
    "            \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "\n",
    "                L1_R1_output = cleaned_output.split()[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = cleaned_output.split()[output_index-1]\n",
    "\n",
    "\n",
    "                if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                elif L1_input == L1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                return np.nan\n",
    "            \n",
    "        elif feature == \"hasn't\":\n",
    "            \n",
    "            cleaned_output = cleaned_output.replace(\"has n't\", \"hasn't\")\n",
    "            \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "\n",
    "                L1_R1_output = cleaned_output.split()[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = cleaned_output.split()[output_index-1]\n",
    "\n",
    "\n",
    "                if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                elif L1_input == L1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                return np.nan    \n",
    "            \n",
    "            \n",
    "            \n",
    "        elif feature == \"done\":\n",
    "                        \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "\n",
    "                L1_R1_output = cleaned_output.split()[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = cleaned_output.split()[output_index-1]\n",
    "\n",
    "\n",
    "                if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                return np.nan\n",
    "        \n",
    "            \n",
    "        elif feature == \"be\":\n",
    "            \n",
    "            #this doesn't include modals which are compounds that contain \"to\"\n",
    "            #  such as \"going to\" and \"used to\" because if the code checks for\n",
    "            #  L1s being the same and if both are \"to\" then it will deem it correct\n",
    "            modals = [\"can\", \"could\", \"will\", \"would\", \"'ll\", \"may\",\n",
    "                      \"might\", \"must\", \"shall\", \"should\", \"'d\"]\n",
    "            \n",
    "            #compound modals\n",
    "            multi_modals = [[\"going\", \"to\"], [\"got\", \"to\"], [\"has\", \"to\"],\n",
    "                            [\"have\", \"to\"], [\"supposed\", \"to\"], [\"used\", \"to\"],\n",
    "                            [\"ought\", \"to\"]]\n",
    "            \n",
    "            #negated modals\n",
    "            neg_modals = [[\"ca\", \"n't\"], [\"could\", \"n't\"], [\"wo\", \"n't\"],\n",
    "                          [\"would\", \"n't\"], [\"must\", \"n't\"], [\"should\", \"n't\"]]\n",
    "            \n",
    "            #combined list\n",
    "            mega_modals = modals + multi_modals + neg_modals\n",
    "\n",
    "            \n",
    "            if instances_count == 1:\n",
    "\n",
    "                split_output = cleaned_output.split()\n",
    "\n",
    "                output_index = split_output.index(feature)\n",
    "                \n",
    "                L2_L1_output = split_output[output_index-2:output_index]\n",
    "\n",
    "                L1_R1_output = split_output[output_index-1:output_index+2]\n",
    "\n",
    "                L1_output = split_output[output_index-1]\n",
    "\n",
    "\n",
    "\n",
    "                if split_output[0] == \"be\" and split_input[0] == \"be\":\n",
    "\n",
    "                    return 1\n",
    "\n",
    "                #the returned 2 here will be converted to a 0 in the automated correctness step\n",
    "                elif split_output[0] == \"be\" and split_input[0] != \"be\":\n",
    "\n",
    "                    return -1\n",
    "\n",
    "                elif L1_R1_input == L1_R1_output:\n",
    "\n",
    "                    return 2\n",
    "\n",
    "                elif L1_input == L1_output:\n",
    "\n",
    "                    return 3\n",
    "\n",
    "                elif L1_input in mega_modals and L1_output in mega_modals:\n",
    "\n",
    "                    return 4\n",
    "                \n",
    "                elif L2_L1_input in mega_modals and L2_L1_output in mega_modals:\n",
    "\n",
    "                    return 5\n",
    "                \n",
    "                elif L1_input in mega_modals and L2_L1_output in mega_modals:\n",
    "\n",
    "                    return 6\n",
    "                \n",
    "                elif L2_L1_input in mega_modals and L1_output in mega_modals:\n",
    "\n",
    "                    return 7\n",
    "\n",
    "                elif feature_count >= 1 and L1_output in modals:\n",
    "\n",
    "                    return -2\n",
    "\n",
    "                else:\n",
    "\n",
    "                    return -3\n",
    "                \n",
    "                \n",
    "            \n",
    "            elif instances_count > 1:\n",
    "                \n",
    "                \n",
    "                split_output = cleaned_output.split()\n",
    "                \n",
    "                \n",
    "                input_indexes = [] \n",
    "                \n",
    "                output_indexes = []\n",
    "                \n",
    "                \n",
    "                for input_index in range(len(split_input)):\n",
    "                    \n",
    "                    if split_input[input_index] == \"be\":\n",
    "                        \n",
    "                        input_indexes.append(input_index)\n",
    "                        \n",
    "                for output_index in range(len(split_output)):\n",
    "                    \n",
    "                    if split_output[output_index] == \"be\":\n",
    "                        \n",
    "                        output_indexes.append(output_index)\n",
    "                        \n",
    "                \n",
    "                if len(input_indexes) != len(output_indexes):\n",
    "                    \n",
    "                    return -4\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    input_iteration_index = input_indexes[iteration_number-1]\n",
    "                    \n",
    "                    output_iteration_index = output_indexes[iteration_number-1]\n",
    "                    \n",
    "                   \n",
    "                    L2_L1_input = split_input[input_iteration_index-2:input_iteration_index]\n",
    "\n",
    "                    L1_R1_input = split_input[input_iteration_index-1:input_iteration_index+2]\n",
    "\n",
    "                    L1_input = split_input[input_iteration_index-1]\n",
    "                    \n",
    "                    \n",
    "                    L2_L1_output = split_output[output_iteration_index-2:output_iteration_index]\n",
    "                    \n",
    "                    L1_R1_output = split_output[output_iteration_index-1:output_iteration_index+2]\n",
    "\n",
    "                    L1_output = split_output[output_iteration_index-1]\n",
    "                    \n",
    "                    \n",
    "                    if L1_R1_input == L1_R1_output:\n",
    "\n",
    "                        return 8\n",
    "\n",
    "                    elif L1_input == L1_output:\n",
    "\n",
    "                        return 9\n",
    "\n",
    "                    elif L1_input in mega_modals and L1_output in mega_modals:\n",
    "\n",
    "                        return 10\n",
    "\n",
    "                    elif L2_L1_input in mega_modals and L2_L1_output in mega_modals:\n",
    "\n",
    "                        return 11\n",
    "\n",
    "                    elif L1_input in mega_modals and L2_L1_output in mega_modals:\n",
    "\n",
    "                        return 12\n",
    "\n",
    "                    elif L2_L1_input in mega_modals and L1_output in mega_modals:\n",
    "\n",
    "                        return 13\n",
    "                    \n",
    "                    else:\n",
    "                        \n",
    "                        return -5\n",
    "                \n",
    "            \n",
    "            else:\n",
    "\n",
    "                return -6\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        return -7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21f853-1ba1-4f40-8a25-51d17bbfe75d",
   "metadata": {},
   "source": [
    "## Executing the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd65c1-d80a-4370-b8a0-10a0a1851ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of column names to be appended next to\n",
    "column_names = [\"amazon_transcription_cleaned\", \n",
    "                \"deepspeech_transcription_cleaned\", \"google_transcription_cleaned\", \n",
    "                \"IBMWatson_transcription_cleaned\", \"microsoft_transcription_cleaned\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63203f6b-2d23-492b-a8c3-6bfd15f0125d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d620f-7f43-4b74-b64b-eaaff450daee",
   "metadata": {},
   "source": [
    "Before running the code for the *ain't* variations, the variations will be split into separate dataframes to be processed. These will be concatenated again in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbccc87-e221-4cdc-b578-1d508e48e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"ain't\"]\n",
    "isnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"isn't\"]\n",
    "arent_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"aren't\"]\n",
    "imnot_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"I'm not\"]\n",
    "didnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"didn't\"]\n",
    "havent_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"haven't\"]\n",
    "hasnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"hasn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fe7a4-fbb7-454c-8320-1853dfa98a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"ain't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = aint_df.columns.get_loc(column_name)\n",
    "    \n",
    "    aint_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in aint_df.itertuples():\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3a7a1-32b5-4af0-bb44-d22c32f7bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"isn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = isnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    isnt_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in isnt_df.itertuples():\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7866f-ac02-40ca-97a1-0c8417f6dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"aren't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = arent_df.columns.get_loc(column_name)\n",
    "    \n",
    "    arent_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in arent_df.itertuples():\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a18fd7-33c2-405a-be0e-33723bae517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"I'm not\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = imnot_df.columns.get_loc(column_name)\n",
    "    \n",
    "    imnot_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in imnot_df.itertuples():\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3421be2-ceb2-4e60-9dff-3aa1d217a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"didn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = didnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    didnt_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in didnt_df.itertuples():\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59fdba-aa44-403e-8f64-e92e29d60f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"haven't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = havent_df.columns.get_loc(column_name)\n",
    "    \n",
    "    havent_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in havent_df.itertuples():\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a80dd-972a-4fd9-ab5e-834ca047e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"hasn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = hasnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    hasnt_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in hasnt_df.itertuples():\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e39724-5ead-4dd9-93df-21d12d914e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = pd.concat([aint_df, isnt_df, arent_df, imnot_df, didnt_df, havent_df, hasnt_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f839-307e-4e80-a648-1842e9f01eb5",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa61f4-9499-4a75-afcd-b44d3509ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"be\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = be_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    be_gs_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in be_gs_df.itertuples():\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1093-19ed-4e4b-be56-0aec20f9002c",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb105bf-9f1c-470e-ba6c-9db49b30ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"done\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = done_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    done_gs_df.insert(col_index+2, f\"{column_name}_adjacentTokens\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through the rows and executes the function\n",
    "for file_row in done_gs_df.itertuples():\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"google_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_adjacentTokens\"] = checkAdjacentTokens(feature, file_row.InstancesCountPerLine, file_row.FeatureCountPerLine, file_row.IterationNumber, file_row.Content_cleaned, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c303e-b650-4e9d-bc79-e279751611fe",
   "metadata": {},
   "source": [
    "## Sorting the Dataframes by File and Line\n",
    "\n",
    "This will sort the dataframes first by filename and then by line number. Doing this each step will ensure consistency across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57205-3bfd-4933-bdbe-5f8e6e75ea9d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f32913-4e1f-44ea-bd82-f0fdbfd167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = aint_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a08157-c6a8-4cbb-8978-e6af4b931a7e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12356d-7ffb-4af6-9c58-21e00fae9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df = be_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cd837-4bb3-4129-b4db-583aeb11ea90",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc53ff-73c4-484b-ad31-2025fba38270",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df = done_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983788-c9a5-4b73-8a2e-1d18a6ae24b9",
   "metadata": {},
   "source": [
    "## Exporting Dataframes to CSV Files\n",
    "\n",
    "This will export the dataframes to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff3dee-e5ab-47f3-a75d-14de88610b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the output path where the CSVs will be stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331a54-e11e-4b58-813a-717796e37e73",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e3561-7148-4ace-aa8f-2e2ca60a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df.to_csv(f\"{csv_output_path}aint_variations_checkAdjacentTokens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf982d6-41cb-46d8-9917-8d4a6a5bdd4d",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6042b-fa7e-4017-aac3-7cfca0b8fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df.to_csv(f\"{csv_output_path}be_checkAdjacentTokens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101802dd-7fd7-4676-a52b-8b9d76e1c990",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cab56-e32a-467e-aa8e-12a6640b5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df.to_csv(f\"{csv_output_path}done_checkAdjacentTokens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df4499-4d1b-4953-939e-b1179bebf0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
