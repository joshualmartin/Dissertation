{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af98568-db11-4b00-9546-6cd0c9e4a154",
   "metadata": {},
   "source": [
    "# Step 2.12: Creating the Correctness Column\n",
    "\n",
    "This code will create a column for the annotation of correctness or incorrectness of ASR outputs. First, if the *ContainsFeature* column is 0 (meaning the word does not appear in the ASR output), then a 0 is appended to this column to indicate incorrectness. Next, if the *AdjacentTokens* column is any number equal to or greater than 1, then a 1 is appended to this column to indicate correctness. If the *AdjacentTokens* column is a -1 or -2, then a 0 is appended to this column to indicate incorrectness (see previous step for number keys/meanings). Otherwise, a NaN is appended so that the cell will be left blank. These blank cells will then be manually annotated for correctness in the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27e6b-6798-4684-9d4b-b505d5bbc52f",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code: os, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f58c-3a5c-4109-91ae-bf6083dbf0da",
   "metadata": {},
   "source": [
    "## Intitial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9910a3-9f2c-450d-b5e8-70c68efaef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f9bcf7-7295-44d6-8ef7-dabff40708fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath for the csv produced in Step 2.11\n",
    "aint_file_path = \"path\"\n",
    "\n",
    "be_file_path = \"path\"\n",
    "\n",
    "done_file_path = \"path\"\n",
    "\n",
    "#reads in the gold standard dataframe    \n",
    "aint_gs_df = pd.read_csv(aint_file_path)\n",
    "\n",
    "be_gs_df = pd.read_csv(be_file_path)\n",
    "\n",
    "done_gs_df = pd.read_csv(done_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108b2f4-a3a4-4b60-a39e-2d0ed35a1606",
   "metadata": {},
   "source": [
    "# Defining the Copying Feature Columns Function\n",
    "\n",
    "This function takes the following arguments:\n",
    "1. The content of the *ContainsFeature* column\n",
    "2. The content of the *AdjacentTokens* column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d874c9-fba0-47a5-8298-44b8c4ea2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyFeatureColumns(feature_present, adjacent_tokens):\n",
    "    \n",
    "    \"\"\"\n",
    "    Copies the content of the \"...containsFeature\" and \"...adjacentTokens\" columns\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    # if the feature is not present (0), it is not correct\n",
    "    if feature_present == 0:\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    # if the adjacent tokens match between the original utterance and the ASR output\n",
    "    #  it is correct\n",
    "    elif adjacent_tokens >= 1:\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    #converts -1s and -2s from the previous step into 0s as incorrect\n",
    "    elif adjacent_tokens in [-1,-2]:\n",
    "        \n",
    "        return 0\n",
    "   \n",
    "   \n",
    "    # else, returns a NaN for manual analysis\n",
    "    else:\n",
    "    \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21f853-1ba1-4f40-8a25-51d17bbfe75d",
   "metadata": {},
   "source": [
    "## Executing the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccd65c1-d80a-4370-b8a0-10a0a1851ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of column names to be appended next to\n",
    "column_names = [\"amazon_transcription_cleaned\", \n",
    "                \"deepspeech_transcription_cleaned\", \"google_transcription_cleaned\", \n",
    "                \"IBMWatson_transcription_cleaned\", \"microsoft_transcription_cleaned\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63203f6b-2d23-492b-a8c3-6bfd15f0125d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0fe7a4-fbb7-454c-8320-1853dfa98a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = aint_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    aint_gs_df.insert(col_index+3, f\"{column_name}_correctness\", np.nan)\n",
    "            \n",
    "# Loops through rows and executes the function\n",
    "for file_row in aint_gs_df.itertuples():\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.amazon_transcription_cleaned_containsFeature, file_row.amazon_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.deepspeech_transcription_cleaned_containsFeature, file_row.deepspeech_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"google_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.google_transcription_cleaned_containsFeature, file_row.google_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.IBMWatson_transcription_cleaned_containsFeature, file_row.IBMWatson_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    aint_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.microsoft_transcription_cleaned_containsFeature, file_row.microsoft_transcription_cleaned_adjacentTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f839-307e-4e80-a648-1842e9f01eb5",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3faa61f4-9499-4a75-afcd-b44d3509ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = be_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    be_gs_df.insert(col_index+3, f\"{column_name}_correctness\", np.nan)\n",
    "            \n",
    "# Loops through rows and executes the function\n",
    "for file_row in be_gs_df.itertuples():\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.amazon_transcription_cleaned_containsFeature, file_row.amazon_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.deepspeech_transcription_cleaned_containsFeature, file_row.deepspeech_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"google_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.google_transcription_cleaned_containsFeature, file_row.google_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.IBMWatson_transcription_cleaned_containsFeature, file_row.IBMWatson_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.microsoft_transcription_cleaned_containsFeature, file_row.microsoft_transcription_cleaned_adjacentTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1093-19ed-4e4b-be56-0aec20f9002c",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb105bf-9f1c-470e-ba6c-9db49b30ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = done_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    done_gs_df.insert(col_index+3, f\"{column_name}_correctness\", np.nan)\n",
    "            \n",
    "# Loops through rows and executes the function\n",
    "for file_row in done_gs_df.itertuples():\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.amazon_transcription_cleaned_containsFeature, file_row.amazon_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.deepspeech_transcription_cleaned_containsFeature, file_row.deepspeech_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"google_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.google_transcription_cleaned_containsFeature, file_row.google_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.IBMWatson_transcription_cleaned_containsFeature, file_row.IBMWatson_transcription_cleaned_adjacentTokens)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_correctness\"] = copyFeatureColumns(file_row.microsoft_transcription_cleaned_containsFeature, file_row.microsoft_transcription_cleaned_adjacentTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c303e-b650-4e9d-bc79-e279751611fe",
   "metadata": {},
   "source": [
    "## Sorting the Dataframes by File and Line\n",
    "\n",
    "This will sort the dataframes first by filename and then by line number. Doing this each step will ensure consistency across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57205-3bfd-4933-bdbe-5f8e6e75ea9d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f32913-4e1f-44ea-bd82-f0fdbfd167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = aint_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a08157-c6a8-4cbb-8978-e6af4b931a7e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af12356d-7ffb-4af6-9c58-21e00fae9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df = be_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cd837-4bb3-4129-b4db-583aeb11ea90",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4cc53ff-73c4-484b-ad31-2025fba38270",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df = done_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983788-c9a5-4b73-8a2e-1d18a6ae24b9",
   "metadata": {},
   "source": [
    "## Exporting Dataframes to CSV Files\n",
    "\n",
    "This will export the dataframes to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ff3dee-e5ab-47f3-a75d-14de88610b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the output path where the CSVs will be stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331a54-e11e-4b58-813a-717796e37e73",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "247e3561-7148-4ace-aa8f-2e2ca60a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df.to_csv(f\"{csv_output_path}aint_variations_autoCorrectness.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf982d6-41cb-46d8-9917-8d4a6a5bdd4d",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b6042b-fa7e-4017-aac3-7cfca0b8fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df.to_csv(f\"{csv_output_path}be_autoCorrectness.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101802dd-7fd7-4676-a52b-8b9d76e1c990",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d4cab56-e32a-467e-aa8e-12a6640b5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df.to_csv(f\"{csv_output_path}done_autoCorrectness.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
