{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af98568-db11-4b00-9546-6cd0c9e4a154",
   "metadata": {},
   "source": [
    "# Step 2.10: Checking ASR Outputs for the Feature\n",
    "\n",
    "This code will check to see if the feature is present in the ASR output. If not, returns a 0. If so, returns a NaN to be analyzed manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27e6b-6798-4684-9d4b-b505d5bbc52f",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code: os, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f58c-3a5c-4109-91ae-bf6083dbf0da",
   "metadata": {},
   "source": [
    "## Intitial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9910a3-9f2c-450d-b5e8-70c68efaef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9bcf7-7295-44d6-8ef7-dabff40708fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath for the csv produced in Step 2.9\n",
    "aint_file_path = \"path\"\n",
    "\n",
    "be_file_path = \"path\"\n",
    "\n",
    "done_file_path = \"path\"\n",
    "\n",
    "#reads in the gold standard dataframe    \n",
    "aint_gs_df = pd.read_csv(aint_file_path)\n",
    "\n",
    "be_gs_df = pd.read_csv(be_file_path)\n",
    "\n",
    "done_gs_df = pd.read_csv(done_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108b2f4-a3a4-4b60-a39e-2d0ed35a1606",
   "metadata": {},
   "source": [
    "# Defining the Checking for Feature Function\n",
    "\n",
    "This function takes the following arguments:\n",
    "1. The feature\n",
    "2. The cleaned ASR output as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d874c9-fba0-47a5-8298-44b8c4ea2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForFeature(feature, cleaned_ASR_output):\n",
    "    \n",
    "    \"\"\"\n",
    "    Checks to see if the feature is present in the ASR output.\n",
    "    If not, returns a 0. If so, returns a NaN to be\n",
    "    analyzed manually.\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    if type(cleaned_ASR_output) != str:\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if feature == \"ain't\":\n",
    "            \n",
    "            cleaned_ASR_output = cleaned_ASR_output.replace(\"ai n't\", \"ain't\")\n",
    "            \n",
    "        elif feature == \"isn't\":\n",
    "        \n",
    "            cleaned_ASR_output = cleaned_ASR_output.replace(\"is n't\", \"isn't\")\n",
    "        \n",
    "        elif feature == \"aren't\":\n",
    "        \n",
    "            cleaned_ASR_output = cleaned_ASR_output.replace(\"are n't\", \"aren't\")\n",
    "            \n",
    "        elif feature == \"I'm not\":\n",
    "            \n",
    "            feature = \"i'mnot\"\n",
    "        \n",
    "            cleaned_ASR_output = cleaned_ASR_output.replace(\"i 'm not\", \"i'mnot\")\n",
    "            \n",
    "        elif feature == \"didn't\":\n",
    "        \n",
    "            cleaned_ASR_output = cleaned_ASR_output.replace(\"did n't\", \"didn't\")\n",
    "            \n",
    "        elif feature == \"haven't\":\n",
    "        \n",
    "            cleaned_ASR_output = cleaned_ASR_output.replace(\"have n't\", \"haven't\")\n",
    "            \n",
    "        elif feature == \"hasn't\":\n",
    "        \n",
    "            cleaned_ASR_output = cleaned_ASR_output.replace(\"has n't\", \"hasn't\")\n",
    "            \n",
    "    \n",
    "        if feature in cleaned_ASR_output.split():\n",
    "\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21f853-1ba1-4f40-8a25-51d17bbfe75d",
   "metadata": {},
   "source": [
    "## Executing the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd65c1-d80a-4370-b8a0-10a0a1851ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of column names to be appended next to\n",
    "column_names = [\"amazon_transcription_cleaned\", \n",
    "                \"deepspeech_transcription_cleaned\", \"google_transcription_cleaned\", \n",
    "                \"IBMWatson_transcription_cleaned\", \"microsoft_transcription_cleaned\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63203f6b-2d23-492b-a8c3-6bfd15f0125d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a09c11-41a4-4c83-a9d5-67bc1b2fabea",
   "metadata": {},
   "source": [
    "Before running the code for the *ain't* variations, the variations will be split into separate dataframes to be processed. These will be concatenated again in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad44df6-dad9-43c4-80dd-2858aeeb2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"ain't\"]\n",
    "isnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"isn't\"]\n",
    "arent_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"aren't\"]\n",
    "imnot_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"I'm not\"]\n",
    "didnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"didn't\"]\n",
    "havent_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"haven't\"]\n",
    "hasnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"hasn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fe7a4-fbb7-454c-8320-1853dfa98a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"ain't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = aint_df.columns.get_loc(column_name)\n",
    "    \n",
    "    aint_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in aint_df.itertuples():\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    aint_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8df06-82cd-410d-8a5e-82e45f34c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"isn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = isnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    isnt_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in isnt_df.itertuples():\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    isnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fca1c-01be-4b76-b09e-0b70bf391850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"aren't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = arent_df.columns.get_loc(column_name)\n",
    "    \n",
    "    arent_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in arent_df.itertuples():\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    arent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d43b7-3010-4bf9-b59b-d3f0a7e70f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"I'm not\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = imnot_df.columns.get_loc(column_name)\n",
    "    \n",
    "    imnot_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in imnot_df.itertuples():\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    imnot_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bac02-3715-4bcc-8bf5-c555f4e37771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"didn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = didnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    didnt_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in didnt_df.itertuples():\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    didnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857bf0a-1d4b-4a04-9152-ce4d2eab8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"haven't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = havent_df.columns.get_loc(column_name)\n",
    "    \n",
    "    havent_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in havent_df.itertuples():\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    havent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90697d-17e7-4f0e-aef0-a88403bf58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"hasn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = hasnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    hasnt_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in hasnt_df.itertuples():\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    hasnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9ea83-c986-4929-a369-236de7d72dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = pd.concat([aint_df, isnt_df, arent_df, imnot_df, didnt_df, havent_df, hasnt_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f839-307e-4e80-a648-1842e9f01eb5",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa61f4-9499-4a75-afcd-b44d3509ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"be\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = be_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    be_gs_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in be_gs_df.itertuples():\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    be_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1093-19ed-4e4b-be56-0aec20f9002c",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb105bf-9f1c-470e-ba6c-9db49b30ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the feature\n",
    "feature = \"done\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = done_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    done_gs_df.insert(col_index+1, f\"{column_name}_containsFeature\", np.nan)\n",
    "            \n",
    "\n",
    "# Loops through rows and executes function\n",
    "for file_row in done_gs_df.itertuples():\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.amazon_transcription_cleaned)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.deepspeech_transcription_cleaned)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"google_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.google_transcription_cleaned)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.IBMWatson_transcription_cleaned)\n",
    "    \n",
    "    done_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_containsFeature\"] = checkForFeature(feature, file_row.microsoft_transcription_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c303e-b650-4e9d-bc79-e279751611fe",
   "metadata": {},
   "source": [
    "## Sorting the Dataframes by File and Line\n",
    "\n",
    "This will sort the dataframes first by filename and then by line number. Doing this each step will ensure consistency across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57205-3bfd-4933-bdbe-5f8e6e75ea9d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f32913-4e1f-44ea-bd82-f0fdbfd167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = aint_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a08157-c6a8-4cbb-8978-e6af4b931a7e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12356d-7ffb-4af6-9c58-21e00fae9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df = be_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cd837-4bb3-4129-b4db-583aeb11ea90",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc53ff-73c4-484b-ad31-2025fba38270",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df = done_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983788-c9a5-4b73-8a2e-1d18a6ae24b9",
   "metadata": {},
   "source": [
    "## Exporting Dataframes to CSV Files\n",
    "\n",
    "This will export the dataframes to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff3dee-e5ab-47f3-a75d-14de88610b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the output path where the CSVs will be stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331a54-e11e-4b58-813a-717796e37e73",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e3561-7148-4ace-aa8f-2e2ca60a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df.to_csv(f\"{csv_output_path}aint_variations_checkForFeature.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf982d6-41cb-46d8-9917-8d4a6a5bdd4d",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6042b-fa7e-4017-aac3-7cfca0b8fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df.to_csv(f\"{csv_output_path}be_checkForFeature.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101802dd-7fd7-4676-a52b-8b9d76e1c990",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cab56-e32a-467e-aa8e-12a6640b5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df.to_csv(f\"{csv_output_path}done_checkForFeature.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb1855-d15a-4a9d-a672-a43845db51a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
