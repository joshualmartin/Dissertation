{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf78ff63-ae1b-4a6c-a959-6a71d2d83a76",
   "metadata": {},
   "source": [
    "# Step 1.7: Getting Word Types per Structural Patterns\n",
    "\n",
    "This code will produce CSV files which have the raw and normalized frequency of each subject and predicate word type in each structural pattern in each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e01ee8-e091-4f3a-9c45-9c7766929bce",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code:\n",
    "string, os, re, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30c5cd-2dd2-4813-b263-e98e6bf04fb6",
   "metadata": {},
   "source": [
    "## Define the Dataframe Creating Function\n",
    "\n",
    "This function takes the following arguments:\n",
    "\n",
    "<ol>\n",
    "<li>The filepath to the split content CSV produced in Step 1.5</li>\n",
    "<li>The filepath to the folder where the newly created CSVs will be stored</li>\n",
    "<li>The word being searched for</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de141f4d-969b-4508-bea3-788e5ed81645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subj_pred_structural_pattern(csv_input_path, csv_output_path, search_word_string):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Reads in split content csvs produced in step 1-5 and all copora info csv \n",
    "    produced in step 1-3 and creates the following csvs for each corpus:\n",
    "    (1) A dictionary of the subject word type token counts in each structual pattern\n",
    "    (2) A dictionary of the predicate word type token counts in each structual pattern\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from string import punctuation\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    if search_word_string == \"ain\\'t\":\n",
    "        \n",
    "        #creates a list of the split content .csv's which should be stored together in \n",
    "        # the same folder (csv_input_path)\n",
    "        csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                         if file.endswith(\".csv\") and\n",
    "                         file.startswith(\"aint\") and \"info\" not in file]\n",
    "        \n",
    "        # filename for the all_corpora_info csv from Step1-3\n",
    "        #  creates a list of the one filename and then uses [0] to get the filename\n",
    "        #  out of the list format\n",
    "        all_corpora_info_csv_path = [f\"{csv_input_path}{filename}\" \n",
    "                                     for filename in os.listdir(csv_input_path) \n",
    "                                     if \"all_corpora_info\" in filename \n",
    "                                     and filename.startswith(\"aint\")][0]\n",
    "    \n",
    "    else:\n",
    "                                     \n",
    "        #creates a list of the split content .csv's which should be stored together in \n",
    "        # the same folder (csv_input_path)\n",
    "        csv_filenames = [file for file in os.listdir(csv_input_path) \n",
    "                         if file.endswith(\".csv\") and\n",
    "                         file.startswith(search_word_string) and \"info\" not in file]\n",
    "\n",
    "        # filename for the all_corpora_info csv from Step1-3\n",
    "        #  creates a list of the one filename and then uses [0] to get the filename\n",
    "        #  out of the list format\n",
    "        all_corpora_info_csv_path = [f\"{csv_input_path}{filename}\" \n",
    "                                     for filename in os.listdir(csv_input_path) \n",
    "                                     if \"all_corpora_info\" in filename \n",
    "                                     and filename.startswith(search_word_string)][0]\n",
    "\n",
    "    #creates a list of tuples with the csv input full paths and the corpus name\n",
    "    filePath_corpusName = [(f\"{csv_input_path}{filename}\",\n",
    "                            re.search(r\"_(.*?)_\", filename).group(1).lower())\n",
    "                           for filename in csv_filenames if \"info\" not in filename]\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    # creates a dataframe from the all_corpora_info_df csv\n",
    "    all_corpora_info_df = pd.read_csv(f\"{all_corpora_info_csv_path}\", index_col=0)\n",
    "    \n",
    "    # lowercases the column names of this dataframe, which are corpus names\n",
    "    #  this is to ensure the next immediate lines of code will fuction correctly\n",
    "    all_corpora_info_df.columns = map(str.lower, all_corpora_info_df.columns)\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    \n",
    "    # loops through the filepath and corpus name tuples list\n",
    "    for file_path, corpus_name in filePath_corpusName:\n",
    "    \n",
    "        # creates a variable of the corpus' total word count\n",
    "        corpus_word_count_total = all_corpora_info_df.loc['TotalCorpusWordCount', corpus_name]\n",
    "\n",
    "        \n",
    "        ###################\n",
    "        \n",
    "        # creates a dataframe from the split content csv for the corpus\n",
    "        patterns_df = pd.read_csv(file_path)\n",
    "        \n",
    "        \n",
    "        # if the feature count in a corpus is zero, skips the corpus\n",
    "        if len(patterns_df) == 0:\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            ####################\n",
    "\n",
    "            #creates a list of Part of Speech structural patterns present in the corpus\n",
    "            # the key=lambda part ensures that capitalized types will be alphabetized\n",
    "            #  just like the rest. Python sorts things by capitals before alphabet\n",
    "            POS_patterns = sorted(set([row.POSPattern for row in patterns_df.itertuples()]), key=lambda v: v.upper())\n",
    "            \n",
    "            #creates an empty list for the column names\n",
    "            column_tuples = []\n",
    "            \n",
    "            #creates a list of tuples for the multilevel column headers\n",
    "            for POS_pattern in POS_patterns:\n",
    "                column_tuples.append((POS_pattern, \"Raw\"))\n",
    "                \n",
    "                column_tuples.append((POS_pattern, \"Normalized\"))\n",
    "                \n",
    "            #creates column headers from tuples\n",
    "            column_names = pd.MultiIndex.from_tuples(column_tuples)\n",
    "            \n",
    "            #creates a list of subject word types present in the corpus\n",
    "            subject_word_types = sorted(set([row.SubjectWordToken for row in patterns_df.itertuples()]), key=lambda v: v.upper())\n",
    "            \n",
    "            #creates a list of predicate word types present in the corpus\n",
    "            predicate_word_types = sorted(set([row.PredicateWordToken for row in patterns_df.itertuples()]), key=lambda v: v.upper())\n",
    "            \n",
    "            \n",
    "            # creates an empty dataframe with POS_patterns list as the columns\n",
    "            # and subject word types as the indexes\n",
    "\n",
    "            subject_POS_patterns_df = pd.DataFrame(columns=column_names, index=subject_word_types)\n",
    "            \n",
    "            # creates an empty dataframe with POS_patterns list as the columns\n",
    "            # and predicate word types as the indexes\n",
    "\n",
    "            predicate_POS_patterns_df = pd.DataFrame(columns=column_names, index=predicate_word_types)\n",
    "            \n",
    "            \n",
    "            #loops through the Part of Speech patterns list\n",
    "            for POS_pattern in POS_patterns:\n",
    "                \n",
    "                #creates an empty dictionary for subject and predicate type-token counts                                    \n",
    "                subject_count_dict = {}\n",
    "                \n",
    "                predicate_count_dict = {}\n",
    "                \n",
    "                #loops through the rows and looks at the POS patterns\n",
    "                for row in patterns_df.itertuples():\n",
    "                    \n",
    "                    #if the POS Pattern from the top level for loop matches\n",
    "                    #  the one in the row, continue\n",
    "                    if row.POSPattern == POS_pattern:\n",
    "                        \n",
    "                        # if the subject word is in the dictionary, add one\n",
    "                        if row.SubjectWordToken in subject_count_dict:\n",
    "                            \n",
    "                            subject_count_dict[row.SubjectWordToken] += 1\n",
    "                            \n",
    "                        # if the subject word is not in the dictionary, create a key with a value of 1\n",
    "                        elif row.SubjectWordToken not in subject_count_dict:\n",
    "                            \n",
    "                            subject_count_dict[row.SubjectWordToken] = 1\n",
    "                        \n",
    "                        # if the predicate word is in the dictionary, add one\n",
    "                        if row.PredicateWordToken in predicate_count_dict:\n",
    "                            \n",
    "                            predicate_count_dict[row.PredicateWordToken] += 1\n",
    "                            \n",
    "                        # if the predicate word is not in the dictionary, create a key with a value of 1\n",
    "                        elif row.PredicateWordToken not in predicate_count_dict:\n",
    "                            \n",
    "                            predicate_count_dict[row.PredicateWordToken] = 1\n",
    "            \n",
    "            \n",
    "                # loops through the rows of the empty subject dataframe and appends the \n",
    "                #  token count for each type\n",
    "                for row in subject_POS_patterns_df.itertuples():\n",
    "                    \n",
    "                    #checks to see if the subject word type is in the subject\n",
    "                    #  count dictionary for this part of speech structural pattern\n",
    "                    if row.Index in subject_count_dict:\n",
    "                        \n",
    "                        #if it is, appends the count to the dataframe\n",
    "                        subject_POS_patterns_df.loc[row.Index, (POS_pattern, \"Raw\")] = subject_count_dict[row.Index]\n",
    "                    \n",
    "                    # if it is not, appends a zero to the dataframe\n",
    "                    else:\n",
    "                        subject_POS_patterns_df.loc[row.Index, (POS_pattern, \"Raw\")] = 0\n",
    "                \n",
    "                \n",
    "                #calculates the normalized frequency count and appends to the dataframe\n",
    "                subject_POS_patterns_df[(POS_pattern, \"Normalized\")] = subject_POS_patterns_df[(POS_pattern, \"Raw\")]/corpus_word_count_total*100000\n",
    "                                \n",
    "                # loops through the rows of the empty subject dataframe and appends the \n",
    "                #  token count for each type\n",
    "                for row in predicate_POS_patterns_df.itertuples():\n",
    "                    \n",
    "                    #checks to see if the subject word type is in the subject\n",
    "                    #  count dictionary for this part of speech structural pattern\n",
    "                    if row.Index in predicate_count_dict:\n",
    "                        \n",
    "                        #if it is, appends the count to the dataframe\n",
    "                        predicate_POS_patterns_df.loc[row.Index, (POS_pattern, \"Raw\")] = predicate_count_dict[row.Index]\n",
    "                    \n",
    "                    # if it is not, appends a zero to the dataframe\n",
    "                    else:\n",
    "                        predicate_POS_patterns_df.loc[row.Index, (POS_pattern, \"Raw\")] = 0\n",
    "                        \n",
    "                #calculates the normalized frequency count and appends to the dataframe\n",
    "                predicate_POS_patterns_df[(POS_pattern, \"Normalized\")] = predicate_POS_patterns_df[(POS_pattern, \"Raw\")]/corpus_word_count_total*100000\n",
    "                    \n",
    "            \n",
    "            if search_word_string == \"ain\\'t\":\n",
    "            \n",
    "                #exports the subject_POS_patterns_df to a csv\n",
    "                subject_POS_patterns_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_subjectPOSPatternTypeToken.csv\")\n",
    "\n",
    "                #exports the predicate_POS_patterns_df to a csv\n",
    "                predicate_POS_patterns_df.to_csv(f\"{csv_output_path}/aint_{corpus_name}_predicatePOSPatternTypeToken.csv\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                #exports the subject_POS_patterns_df to a csv\n",
    "                subject_POS_patterns_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_subjectPOSPatternTypeToken.csv\")\n",
    "\n",
    "                #exports the predicate_POS_patterns_df to a csv\n",
    "                predicate_POS_patterns_df.to_csv(f\"{csv_output_path}/{search_word_string}_{corpus_name}_predicatePOSPatternTypeToken.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbb138-16c5-4bad-a6ad-f0ec2e055273",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating Quantitative  Dataframes and Exporting Dataframes to CSV Files\n",
    "\n",
    "This will execute the code and create the dataframes and then export them as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e2ae5-d92e-4e79-82fb-7dc85cf7653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the input path where the gold standard CSVs are stored\n",
    "csv_input_path = \"path\"\n",
    "\n",
    "# Designate the output path where the gold standard CSVs are stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33576a-bae7-4657-8742-eb0257a60356",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e6259-1812-4288-8d1e-054568c3945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"ain\\'t\"\n",
    "\n",
    "# execute code\n",
    "get_subj_pred_structural_pattern(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1162a8-d9de-492d-9a12-022716aca883",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596c701-bbe7-4f9e-8ca3-b7d912d74d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"be\"\n",
    "\n",
    "# execute code\n",
    "get_subj_pred_structural_pattern(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280668e0-81e0-42a9-986d-b2f35fc0f044",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e660b0d-27a7-4364-9d93-37acfc4335f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the search word\n",
    "search_word_string = \"done\"\n",
    "\n",
    "# execute code\n",
    "get_subj_pred_structural_pattern(csv_input_path, csv_output_path, search_word_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b7c27-06f4-4143-8789-438214d1608b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
