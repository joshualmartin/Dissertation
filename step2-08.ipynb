{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af98568-db11-4b00-9546-6cd0c9e4a154",
   "metadata": {},
   "source": [
    "# Step 2.8: Getting Error Counts\n",
    "\n",
    "This code will get the error counts for the content of ASR output before and after the occurrence of the feature in question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27e6b-6798-4684-9d4b-b505d5bbc52f",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "The following packages are necessary to run this code: re, os, [pandas](https://pypi.org/project/pandas/), [numpy](https://pypi.org/project/numpy/), [wagnerfischerpp](https://gist.github.com/kylebgorman/8034009), [interruptingcow](https://pypi.org/project/interruptingcow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f58c-3a5c-4109-91ae-bf6083dbf0da",
   "metadata": {},
   "source": [
    "## Intitial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9910a3-9f2c-450d-b5e8-70c68efaef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9bcf7-7295-44d6-8ef7-dabff40708fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath for the csv produced in Step 2.7\n",
    "aint_file_path = \"path\"\n",
    "\n",
    "be_file_path = \"path\"\n",
    "\n",
    "done_file_path = \"path\"\n",
    "\n",
    "#reads in the gold standard dataframe    \n",
    "aint_gs_df = pd.read_csv(aint_file_path)\n",
    "\n",
    "be_gs_df = pd.read_csv(be_file_path)\n",
    "\n",
    "done_gs_df = pd.read_csv(done_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108b2f4-a3a4-4b60-a39e-2d0ed35a1606",
   "metadata": {},
   "source": [
    "# Defining the Aligning Function\n",
    "\n",
    "This function is an internal function defined here and used in the function below.\n",
    "\n",
    "This function takes the following arguments:\n",
    "1. The original utterance content\n",
    "2. The ASR output content\n",
    "3. The list of edits produced by the wagnerfischerpp function below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c06c9f-951f-45ff-bbe1-b0f046285c50",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This code was provided by [Kevin Tang](https://github.com/tang-kevin) through personal correspondence. See Kevin's website [here](http://www.kevintang.org/) for more work and research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d874c9-fba0-47a5-8298-44b8c4ea2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_WF(s1, s2, edits):\n",
    "    \n",
    "    s1 = list(s1) # in case the s1 and s2 are strings\n",
    "    \n",
    "    s2 = list(s2)\n",
    "    \n",
    "    alignment_temp = []\n",
    "    \n",
    "    for match_type in edits:\n",
    "        \n",
    "    #match_type = edit\n",
    "        if match_type == 'D':\n",
    "            \n",
    "            oseg = '-'\n",
    "            \n",
    "            iseg = s1[0]\n",
    "            \n",
    "            s1.pop(0)\n",
    "            \n",
    "        elif match_type == 'I':\n",
    "            \n",
    "            iseg = '-'    \n",
    "            \n",
    "            oseg = s2[0]\n",
    "            \n",
    "            s2.pop(0)\n",
    "            \n",
    "        else: # match or sub\n",
    "            \n",
    "            iseg = s1[0]\n",
    "            \n",
    "            oseg = s2[0]\n",
    "            \n",
    "            s1.pop(0)\n",
    "            \n",
    "            s2.pop(0)\n",
    "            \n",
    "        alignment_temp.append((iseg,oseg))\n",
    "\n",
    "    if (len(s1) + len(s2)) == 0:\n",
    "        \n",
    "        return alignment_temp\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return 'BUG'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575a508-d9e1-45d9-8664-a098566d539a",
   "metadata": {},
   "source": [
    "## Defining the Word Alignment Getting Function\n",
    "\n",
    "The function defined here is a modified version of the function used in the previous step.\n",
    "\n",
    "This function depends on the importation of the wagnerfischerpp python script. To do so, follow these steps:\n",
    "1. Go to https://gist.github.com/kylebgorman/8034009.\n",
    "2. Download the wagnerfischerpp.py script.\n",
    "3. Move the script into the current working directory you are working in with this code.\n",
    "4. For a test, run *from wagnerfishcerpp import \\** to make sure it works.\n",
    "\n",
    "This function takes two arguments:\n",
    "1. The original utterance content as a string\n",
    "2. The ASR output content as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fc275-43b8-468a-b4c4-f604bd721a70",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The *getWordAlignments* function is directly adapted from the work of [Kyle Gorman](https://gist.github.com/kylebgorman) (see code [here](https://gist.github.com/kylebgorman/8034009))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e5c5f-ed40-46f8-9e5c-18e1582fa685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordAlignments(original_utterance, ASR_output):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gets alignments between intended and ASR inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    from wagnerfischerpp import WagnerFischer\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    \n",
    "    #returns NaNs if the content is not a string\n",
    "    if type(original_utterance) != str or type(ASR_output) != str:\n",
    "        \n",
    "        #this was originally return np.nan to be more precise\n",
    "        #  however, since the resulting list from this function\n",
    "        #  has to be converted to a string in order to be added\n",
    "        #  to the pandas dataframe, i use this because if \n",
    "        #  str(np.nan) is run, it returns the string \"nan\"\n",
    "        #  rather than the empty cell I need\n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #the cleaning process separates ain't into \"ai\" and \"'nt\"\n",
    "        #  this will make it back into one word for this process\n",
    "        #  without changing it in the actual csv\n",
    "        if feature == \"ain't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"ai n't\", \"ain't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"ai n't\", \"ain't\")\n",
    "            \n",
    "        elif feature == \"isn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"is n't\", \"isn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"is n't\", \"isn't\")\n",
    "        \n",
    "        elif feature == \"aren't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"are n't\", \"aren't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"are n't\", \"aren't\")\n",
    "            \n",
    "        elif feature == \"I'm not\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"i 'm not\", \"i'm not\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"i 'm not\", \"i'm not\")\n",
    "            \n",
    "        elif feature == \"didn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"did n't\", \"didn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"did n't\", \"didn't\")\n",
    "            \n",
    "        elif feature == \"haven't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"have n't\", \"haven't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"have n't\", \"haven't\")\n",
    "            \n",
    "        elif feature == \"hasn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"has n't\", \"hasn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"has n't\", \"hasn't\")\n",
    "            \n",
    "        \n",
    "        #splits the strings into a list\n",
    "        original_list = original_utterance.split()\n",
    "        \n",
    "        ASR_list = ASR_output.split()\n",
    "        \n",
    "        \n",
    "        #performs the alignments\n",
    "        aligns = [list(WagnerFischer(original_list, ASR_list).alignments())[0]]\n",
    "        \n",
    "        alignment_all = [align_WF(original_list, ASR_list, iii) for iii in aligns]\n",
    "        \n",
    "    return alignment_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e01b89-be8a-442f-b818-e55ca9d6d534",
   "metadata": {},
   "source": [
    "## Defining the Pre-Feature Error Count Getting Function\n",
    "\n",
    "This function will get alignments between the original utterance and ASR output. It will then calculate how many errors there are before the feature provided. \n",
    "\n",
    "This function takes the following arguments:\n",
    "1. The original utterance content as a string\n",
    "2. The ASR output content as a string\n",
    "3. The feature\n",
    "4. The iteration number (taken from the dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899ced9-618d-411f-be54-55ad42ea5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPreFeatureErr(original_utterance, ASR_output, feature, iteration_number):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gets alignments between intended and ASR inference.\n",
    "    Then calculates how many errors before the feature provided\n",
    "    Depends on wagnerfisherpp. Have the wagnerfischerpp.py file in the\n",
    "    same directory and then run from wagnerfischerpp import * before using this.\n",
    "    it also depends on align_WF() from Kevin Tang, defined above\n",
    "    intended_speech and perceived_inference should be strings.\n",
    "    The iteration number is for lines that have more than one\n",
    "    instance of the feature in the line.\n",
    "    this function will break them into lists\n",
    "    \"\"\"\n",
    "    \n",
    "    from wagnerfischerpp import WagnerFischer\n",
    "    import numpy as np\n",
    "\n",
    "    \n",
    "    \n",
    "    #returns NaNs if the content is not a string\n",
    "    if type(original_utterance) != str or type(ASR_output) != str:\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #the cleaning process separates ain't into \"ai\" and \"'nt\"\n",
    "        #  this will make it back into one word for this process\n",
    "        #  without changing it in the actual csv\n",
    "        if feature == \"ain't\":\n",
    "\n",
    "            original_utterance = original_utterance.replace(\"ai n't\", \"ain't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"ai n't\", \"ain't\")\n",
    "            \n",
    "        elif feature == \"isn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"is n't\", \"isn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"is n't\", \"isn't\")\n",
    "        \n",
    "        elif feature == \"aren't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"are n't\", \"aren't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"are n't\", \"aren't\")\n",
    "            \n",
    "        elif feature == \"I'm not\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"i 'm not\", \"i'm not\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"i 'm not\", \"i'm not\")\n",
    "            \n",
    "        elif feature == \"didn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"did n't\", \"didn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"did n't\", \"didn't\")\n",
    "            \n",
    "        elif feature == \"haven't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"have n't\", \"haven't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"have n't\", \"haven't\")\n",
    "            \n",
    "        elif feature == \"hasn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"has n't\", \"hasn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"has n't\", \"hasn't\")\n",
    "            \n",
    "        \n",
    "        #splits the strings into a list\n",
    "        original_list = original_utterance.split()\n",
    "        \n",
    "        ASR_list = ASR_output.split()\n",
    "        \n",
    "        \n",
    "        #performs the alignments\n",
    "        aligns = [list(WagnerFischer(original_list, ASR_list).alignments())[0]]\n",
    "        \n",
    "        alignment_all = [align_WF(original_list, ASR_list, iii) for iii in aligns]\n",
    "          \n",
    "        \n",
    "        #creates empty list to append the indexes of all the instances of features occurring in the line\n",
    "        feature_indexes = []  \n",
    "        \n",
    "        \n",
    "        # special case for I'm not since it's two words\n",
    "        if feature == \"I'm not\" or feature == \"i'm not\":\n",
    "            \n",
    "            #creates a list of the two words\n",
    "            feature = [\"i'm\", \"not\"]\n",
    "            \n",
    "            # the following will loop through pairs of words made into two set lists\n",
    "            # to see if the two set list matches the feature. the try/except structure\n",
    "            # is here so that when the loop hits the last word in the alignment list\n",
    "            # it won't throw an out of index range error\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                #alignment_all is a list within a list, so the [0] gets the list within the list\n",
    "                for x in range(len(alignment_all[0])):\n",
    "                    \n",
    "                    # the structure here is the alignment list[the first alignment][the tuple at index at the number of the loop][the first item in the tuple]\n",
    "                    if [alignment_all[0][x][0], alignment_all[0][x+1][0]] == feature:\n",
    "                                                \n",
    "                        feature_indexes.append(x) \n",
    "\n",
    "            except:\n",
    "                                \n",
    "                pass\n",
    "\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            #cycles through the tuples, which are composed of the intended (original)\n",
    "            #  utterance token paired with its aligned token in the perceived (ASR)\n",
    "            #  text. if the first item of the tuple matches the feature, the index\n",
    "            #  of the feature is appended to the feature_indexes list\n",
    "            for x in range(len(alignment_all[0])):\n",
    "\n",
    "                if alignment_all[0][x][0] == feature:\n",
    "\n",
    "                    feature_indexes.append(x) \n",
    "\n",
    "                \n",
    "                \n",
    "        #if there is only one index in the list, performs the process for the\n",
    "        #  content before the one instance of the feature\n",
    "        if len(feature_indexes) == 1:\n",
    "\n",
    "\n",
    "            #creates an empty list. this block will count how many errors there are\n",
    "            #  by cycling through the tuples and comparing the content in the tuples.\n",
    "            #  if the first and second items are the same, that means that the ASR\n",
    "            #  got it right. if they are different, then that means it got it wrong.\n",
    "            #  if it matches, a 0 will be appended to the list. if it doesn't match,\n",
    "            #  a 1 will be appended to the list. in the end, this list will be summed\n",
    "            #  and that sum number is the number of errors the ASR made before the feature\n",
    "            pre_feature_errors = []\n",
    "\n",
    "\n",
    "\n",
    "            #cycles through the tuples which occur before the feature index\n",
    "            for y in alignment_all[0][:feature_indexes[0]]:\n",
    "\n",
    "                #checks if the first item in the tuple matches the second or not\n",
    "                if y[0] == y[1]:\n",
    "\n",
    "                    pre_feature_errors.append(0)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    pre_feature_errors.append(1)\n",
    "\n",
    "\n",
    "            #gets the sum of the feature error list\n",
    "            num_pre_feature_errors = sum(pre_feature_errors)\n",
    "\n",
    "\n",
    "        # if there are no instances, this will return a nan\n",
    "        elif len(feature_indexes) == 0:\n",
    "\n",
    "            num_pre_feature_errors = np.nan\n",
    "\n",
    "\n",
    "\n",
    "        #if there are more than one instance in the line, this will\n",
    "        # take the iteration number and use that to get the correct index\n",
    "        #  of the feature in the line. it does this by collecting\n",
    "        #  the indexes of all the features in the alignment list,\n",
    "        #  storing them in a list, then, uses the iteration number to determine\n",
    "        #  which of the indexes is the correct to use for that particular instance\n",
    "        else:\n",
    "\n",
    "            pre_feature_errors = []\n",
    "\n",
    "\n",
    "            #cycles through the tuples which occur before the feature index\n",
    "            #  takes the iteration number and subtracts one. so if the iteration number\n",
    "            #  is 1, meaning the first iteration or occurrence of the feature in the line\n",
    "            #  and subtracts 1 making 0, then that will be used to get the first\n",
    "            #  index in the list. that index will be used to determine the boundary of where\n",
    "            #  the content before the feature ends\n",
    "\n",
    "\n",
    "            for y in alignment_all[0][:feature_indexes[iteration_number-1]]:\n",
    "\n",
    "                if y[0] == y[1]:\n",
    "\n",
    "                    pre_feature_errors.append(0)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    pre_feature_errors.append(1)\n",
    "\n",
    "            #gets the sum of the feature error list\n",
    "            num_pre_feature_errors = sum(pre_feature_errors)\n",
    "            \n",
    "            \n",
    "        return num_pre_feature_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563bede-4f28-45e5-8744-236d0174707a",
   "metadata": {},
   "source": [
    "## Defining the Post-Feature Error Count Getting Function\n",
    "\n",
    "This function will get alignments between the original utterance and ASR output. It will then calculate how many errors there are after the feature provided. \n",
    "\n",
    "This function takes the following arguments:\n",
    "1. The original utterance content as a string\n",
    "2. The ASR output content as a string\n",
    "3. The feature\n",
    "4. The iteration number (taken from the dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1db8b4-ebf9-4097-8118-c95ed4e7d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostFeatureErr(original_utterance, ASR_output, feature, iteration_number):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gets alignments between intended and ASR inference.\n",
    "    Then calculates how many errors after the feature provided\n",
    "    Depends on wagnerfisherpp. Have the wagnerfischerpp.py file in the\n",
    "    same directory and then run from wagnerfischerpp import * before using this.\n",
    "    it also depends on align_WF() from Kevin Tang, defined above\n",
    "    intended_speech and perceived_inference should be strings.\n",
    "    The iteration number is for lines that have more than one\n",
    "    instance of the feature in the line.\n",
    "    this function will break them into lists\n",
    "    \"\"\"\n",
    "    \n",
    "    from wagnerfischerpp import WagnerFischer\n",
    "    import numpy as np\n",
    "\n",
    "        \n",
    "    #returns NaNs if the content is not a string\n",
    "    if type(original_utterance) != str or type(ASR_output) != str:\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #the cleaning process separates ain't into \"ai\" and \"'nt\"\n",
    "        #  this will make it back into one word for this process\n",
    "        #  without changing it in the actual csv\n",
    "        if feature == \"ain't\":\n",
    "\n",
    "            original_utterance = original_utterance.replace(\"ai n't\", \"ain't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"ai n't\", \"ain't\")\n",
    "        \n",
    "        elif feature == \"isn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"is n't\", \"isn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"is n't\", \"isn't\")\n",
    "        \n",
    "        elif feature == \"aren't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"are n't\", \"aren't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"are n't\", \"aren't\")\n",
    "            \n",
    "        elif feature == \"I'm not\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"i 'm not\", \"i'm not\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"i 'm not\", \"i'm not\")\n",
    "            \n",
    "        elif feature == \"didn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"did n't\", \"didn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"did n't\", \"didn't\")\n",
    "            \n",
    "        elif feature == \"haven't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"have n't\", \"haven't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"have n't\", \"haven't\")\n",
    "            \n",
    "        elif feature == \"hasn't\":\n",
    "        \n",
    "            original_utterance = original_utterance.replace(\"has n't\", \"hasn't\")\n",
    "\n",
    "            ASR_output = ASR_output.replace(\"has n't\", \"hasn't\")\n",
    "            \n",
    "        \n",
    "        #splits the strings into a list\n",
    "        original_list = original_utterance.split()\n",
    "\n",
    "        ASR_list = ASR_output.split()\n",
    "        \n",
    "        \n",
    "        #performs the alignments\n",
    "        aligns = [list(WagnerFischer(original_list, ASR_list).alignments())[0]]\n",
    "        #original --> aligns = list(WagnerFischer(intended_list, perceived_list).alignments(bfirst = True))\n",
    "        \n",
    "        alignment_all = [align_WF(original_list, ASR_list, iii) for iii in aligns]\n",
    "\n",
    "        #creates empty list to append the indexes of all the instances of features occurring in the line\n",
    "        feature_indexes = []  #####\n",
    "\n",
    "        \n",
    "         # special case for I'm not since it's two words\n",
    "        if feature == \"I'm not\" or feature == \"i'm not\":\n",
    "            \n",
    "            #creates a list of the two words\n",
    "            feature = [\"i'm\", \"not\"]\n",
    "            \n",
    "            # the following will loop through pairs of words made into two set lists\n",
    "            # to see if the two set list matches the feature. the try/except structure\n",
    "            # is here so that when the loop hits the last word in the alignment list\n",
    "            # it won't throw an out of index range error\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                #alignment_all is a list within a list, so the [0] gets the list within the list\n",
    "                for x in range(len(alignment_all[0])):\n",
    "                    \n",
    "                    # the structure here is the alignment list[the first alignment][the tuple at index at the number of the loop][the first item in the tuple]\n",
    "                    if [alignment_all[0][x][0], alignment_all[0][x+1][0]] == feature:\n",
    "                                                \n",
    "                        feature_indexes.append(x) \n",
    "\n",
    "            except:\n",
    "                                \n",
    "                pass\n",
    "\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            #cycles through the tuples, which are composed of the intended (original)\n",
    "            #  utterance token paired with its aligned token in the perceived (ASR)\n",
    "            #  text. if the first item of the tuple matches the feature, the index\n",
    "            #  of the feature is appended to the feature_indexes list\n",
    "            for x in range(len(alignment_all[0])):\n",
    "\n",
    "                if alignment_all[0][x][0] == feature:\n",
    "\n",
    "                    feature_indexes.append(x) \n",
    "        \n",
    "                \n",
    "        #if there is only one index in the list, performs the process for the\n",
    "        #  content after the one instance of the feature\n",
    "        if len(feature_indexes) == 1:\n",
    "            \n",
    "            #creates an empty list. this block will count how many errors there are\n",
    "            #  by cycling through the tuples and comparing the content in the tuples.\n",
    "            #  if the first and second items are the same, that means that the ASR\n",
    "            #  got it right. if they are different, then that means it got it wrong.\n",
    "            #  if it matches, a 0 will be appended to the list. if it doesn't match,\n",
    "            #  a 1 will be appended to the list. in the end, this list will be summed\n",
    "            #  and that sum number is the number of errors the ASR made after the feature\n",
    "            post_feature_errors = []\n",
    "            \n",
    "\n",
    "            #cycles through the tuples which occur before the feature index\n",
    "            for y in alignment_all[0][feature_indexes[0]+1:]:\n",
    "                                \n",
    "                #checks if the first item in the tuple matches the second or not\n",
    "                if y[0] == y[1]:\n",
    "\n",
    "                    post_feature_errors.append(0)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    post_feature_errors.append(1)\n",
    "\n",
    "                    \n",
    "            #gets the sum of the feature error list\n",
    "            num_post_feature_errors = sum(post_feature_errors)\n",
    "           \n",
    "        \n",
    "        # if there are no instances of I'm not, this will return a nan\n",
    "        elif len(feature_indexes) == 0:\n",
    "\n",
    "            num_post_feature_errors = np.nan\n",
    "        \n",
    "        \n",
    "        #if there are more than one instance in the line, this will\n",
    "        # take the iteration number and use that to get the correct index\n",
    "        #  of the feature in the line. it does this by collecting\n",
    "        #  the indexes of all the features in the alignment list,\n",
    "        #  storing them in a list, then, uses the iteration number to determine\n",
    "        #  which of the indexes is the correct to use for that particular instance\n",
    "        else:\n",
    "\n",
    "            post_feature_errors = []\n",
    "            \n",
    "            \n",
    "            #cycles through the tuples which occur before the feature index\n",
    "            #  takes the iteration number and subtracts one. so if the iteration number\n",
    "            #  is 1, meaning the first iteration or occurrence of the feature in the line\n",
    "            #  and subtracts 1 making 0, then that will be used to get the first\n",
    "            #  index in the list. that index will be used to determine the boundary of where\n",
    "            #  the content before the feature begins. one is added in the end because\n",
    "            #  python includes the first number in the [ : ] list splicer, so \n",
    "            #  to have the list after the index, you have to add one            \n",
    "            for y in alignment_all[0][feature_indexes[iteration_number-1]+1:]:\n",
    "\n",
    "                if y[0] == y[1]:\n",
    "\n",
    "                    post_feature_errors.append(0)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    post_feature_errors.append(1)\n",
    "            \n",
    "            \n",
    "            #gets the sum of the feature error list\n",
    "            num_post_feature_errors = sum(post_feature_errors)\n",
    "            \n",
    "        \n",
    "        return num_post_feature_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21f853-1ba1-4f40-8a25-51d17bbfe75d",
   "metadata": {},
   "source": [
    "## Executing the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f0bab-21c7-4179-9a04-172ee68fc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from interruptingcow import timeout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd65c1-d80a-4370-b8a0-10a0a1851ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of column names to be appended next to\n",
    "column_names = [\"amazon_transcription_cleaned\", \n",
    "                \"deepspeech_transcription_cleaned\", \"google_transcription_cleaned\", \n",
    "                \"IBMWatson_transcription_cleaned\", \"microsoft_transcription_cleaned\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63203f6b-2d23-492b-a8c3-6bfd15f0125d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf97e5-74e4-48fd-86ee-6d0c0c73e271",
   "metadata": {},
   "source": [
    "Before running the code for the *ain't* variations, the variations will be split into separate dataframes to be processed. These will be concatenated again in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140eb59-31d5-40b1-bb70-5b9bcbaa3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"ain't\"]\n",
    "isnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"isn't\"]\n",
    "arent_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"aren't\"]\n",
    "imnot_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"I'm not\"]\n",
    "didnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"didn't\"]\n",
    "havent_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"haven't\"]\n",
    "hasnt_df = aint_gs_df[aint_gs_df[\"AintVariation\"]==\"hasn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fe7a4-fbb7-454c-8320-1853dfa98a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"ain't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = aint_df.columns.get_loc(column_name)\n",
    "    \n",
    "    aint_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    aint_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    aint_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in aint_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                aint_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                aint_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                aint_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                aint_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                aint_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                aint_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                aint_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(aint_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/{len(aint_df)} completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cd96d-c43e-453e-a6d3-1521ec90ed5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"isn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = isnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    isnt_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    isnt_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    isnt_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in isnt_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                isnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                isnt_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                isnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                isnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                isnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                isnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(isnt_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/{len(isnt_df)} completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c0bbd-4151-40b8-a7d3-516568bdd49e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"aren't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = arent_df.columns.get_loc(column_name)\n",
    "    \n",
    "    arent_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    arent_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    arent_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in arent_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                arent_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                arent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                arent_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                arent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                arent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                arent_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                arent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(arent_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/{len(arent_df)} completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2ddc7-514a-4b44-84d0-c4238a0cee49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"I'm not\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = imnot_df.columns.get_loc(column_name)\n",
    "    \n",
    "    imnot_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    imnot_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    imnot_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in imnot_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                imnot_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                imnot_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                imnot_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                imnot_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                imnot_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                imnot_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(imnot_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/{len(imnot_df)} completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4c4e6-ac49-4644-9be6-e1a047d65984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"didn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = didnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    didnt_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    didnt_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    didnt_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in didnt_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                didnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                didnt_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                didnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                didnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                didnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                didnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(didnt_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/{len(didnt_df)} completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98acee-19d1-482b-a19e-c5e842dbfc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"haven't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = havent_df.columns.get_loc(column_name)\n",
    "    \n",
    "    havent_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    havent_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    havent_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in havent_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                havent_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                havent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                havent_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                havent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                havent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                havent_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                havent_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(havent_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/{len(havent_df)} completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e08ab-0916-4895-a98d-82a75d931390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"hasn't\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = hasnt_df.columns.get_loc(column_name)\n",
    "    \n",
    "    hasnt_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    hasnt_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    hasnt_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in hasnt_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                hasnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                hasnt_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                hasnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                hasnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                hasnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                hasnt_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(hasnt_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/{len(hasnt_df)} completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18805fd-07fb-4eba-a3e9-6870a0b87063",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = pd.concat([aint_df, isnt_df, arent_df, imnot_df, didnt_df, havent_df, hasnt_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f839-307e-4e80-a648-1842e9f01eb5",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa61f4-9499-4a75-afcd-b44d3509ed97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"be\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = be_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    be_gs_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    be_gs_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    be_gs_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in be_gs_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                be_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                be_gs_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                be_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                be_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                be_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                be_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"be_{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(be_gs_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"be_{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/len(be_gs_df) completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1093-19ed-4e4b-be56-0aec20f9002c",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb105bf-9f1c-470e-ba6c-9db49b30ec9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature\n",
    "feature = \"done\"\n",
    "\n",
    "# Appends new columns\n",
    "for column_name in column_names:\n",
    "    \n",
    "    col_index = done_gs_df.columns.get_loc(column_name)\n",
    "    \n",
    "    done_gs_df.insert(col_index+3, f\"{column_name}_Alignments\", np.nan)\n",
    "        \n",
    "    done_gs_df.insert(col_index+4, f\"{column_name}_preFeature_errorCount\", np.nan)\n",
    "        \n",
    "    done_gs_df.insert(col_index+5, f\"{column_name}_postFeature_errorCount\", np.nan)\n",
    "\n",
    "# enable this if you'd like a progress check printed\n",
    "progress_number = 1\n",
    "\n",
    "# cycles through all rows and executes all functions\n",
    "for file_row in done_gs_df.itertuples():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #sets the timeout timer to 10 seconds\n",
    "        with timeout(10, exception = RuntimeError):    \n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.amazon_transcription_cleaned))\n",
    "                \n",
    "                done_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned))\n",
    "                \n",
    "                done_gs_df.loc[file_row.Index, \"google_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.google_transcription_cleaned))\n",
    "                \n",
    "                done_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned))\n",
    "                \n",
    "                done_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_Alignments\"] = str(getWordAlignments(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned))\n",
    "\n",
    "            \n",
    "                done_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"google_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_preFeature_errorCount\"] = getPreFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"amazon_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.amazon_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"deepspeech_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.deepspeech_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"google_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.google_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"IBMWatson_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.IBMWatson_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                done_gs_df.loc[file_row.Index, \"microsoft_transcription_cleaned_postFeature_errorCount\"] = getPostFeatureErr(file_row.Content_cleaned, file_row.microsoft_transcription_cleaned, feature, file_row.IterationNumber)\n",
    "\n",
    "                # Enable this if you'd like to print for progress\n",
    "                print(f\"be_{file_row.Index}_{file_row.File}_{file_row.Line}    {progress_number}/{len(done_gs_df)} completed.\")\n",
    "\n",
    "\n",
    "    except RuntimeError: \n",
    "\n",
    "        # enable this to print a flag\n",
    "        print(f\"be_{file_row.Index}_{file_row.File}_{file_row.Line} timed out and was skipped.  {progress_number}/len(done_gs_df) completed.\")\n",
    "        \n",
    "    progress_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c303e-b650-4e9d-bc79-e279751611fe",
   "metadata": {},
   "source": [
    "## Sorting the Dataframes by File and Line\n",
    "\n",
    "This will sort the dataframes first by filename and then by line number. Doing this each step will ensure consistency across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57205-3bfd-4933-bdbe-5f8e6e75ea9d",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f32913-4e1f-44ea-bd82-f0fdbfd167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df = aint_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a08157-c6a8-4cbb-8978-e6af4b931a7e",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12356d-7ffb-4af6-9c58-21e00fae9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df = be_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cd837-4bb3-4129-b4db-583aeb11ea90",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc53ff-73c4-484b-ad31-2025fba38270",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df = done_gs_df.sort_values(by=['File', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983788-c9a5-4b73-8a2e-1d18a6ae24b9",
   "metadata": {},
   "source": [
    "## Exporting Dataframes to CSV Files\n",
    "\n",
    "This will export the dataframes to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff3dee-e5ab-47f3-a75d-14de88610b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the output path where the CSVs will be stored\n",
    "csv_output_path = \"path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331a54-e11e-4b58-813a-717796e37e73",
   "metadata": {},
   "source": [
    "### Feature: Ain't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e3561-7148-4ace-aa8f-2e2ca60a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aint_gs_df.to_csv(f\"{csv_output_path}aint_variations_errorCounts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf982d6-41cb-46d8-9917-8d4a6a5bdd4d",
   "metadata": {},
   "source": [
    "### Feature: Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6042b-fa7e-4017-aac3-7cfca0b8fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "be_gs_df.to_csv(f\"{csv_output_path}be_errorCounts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101802dd-7fd7-4676-a52b-8b9d76e1c990",
   "metadata": {},
   "source": [
    "### Feature: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cab56-e32a-467e-aa8e-12a6640b5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_gs_df.to_csv(f\"{csv_output_path}done_errorCounts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df4499-4d1b-4953-939e-b1179bebf0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
